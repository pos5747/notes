{
  "hash": "c987bbfb970c30734df58ed9b4b2f0e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Stan\"\nformat:\n  html:\n    theme: default\nexecute:\n  cache: false\n---\n\n\n\n\n\n\nPreviously, we looked at the Metropolis algorithm and saw how it could generate samples from an unnormalized posterior. Metropolis is general and powerful, but requires many iterations to explore the posterior and careful tuning of proposal distributions.\n\nIn this chapter, we look at Stan, a modern and general-purpose platform for Bayesian inference [@carpenter2017]. Stan automates the process of sampling from complex posterior distributions using a *hyper-optimized version* of Hamiltonian Monte Carlo (HMC) [@neal2011], which is itself a form of Markov Chain Monte Carlo (MCMC). @betancourt2017 provides a nice introduction to HMC.\n\nStan handles nearly all the details automatically: step sizes, trajectory lengths, tuning, and adaptation [@hoffman2014]. This allows the user to focus on the model itself.\n\n::: {.aside}\n\nBefore Stan, Bayesian modeling in R was typically done using **BUGS** or **JAGS**, which relied on Gibbs sampling and Metropolis-within-Gibbs updates. These samplers were intuitive and easy to use but often slow to converge, especially for correlated or high-dimensional parameters.\n\nStan was designed to overcome these limitations. It uses gradients of the log-posterior to navigate the parameter space more efficiently, improving both speed and accuracy.\n\n:::\n\n## Logistic Regression Model\n\nConsider our usual logistic regression model.\n\n$$\ny_i \\sim \\text{Bernoulli}(\\pi_i), \\quad \\pi_i = \\text{logit}^{-1} \\left( X_i \\beta \\right).\n$$\n\nWe'll assume weakly informative priors (i.e., variance = 25). Because the scale of the variables affects the interpretation of the priors, it's common to rescale variables to have a common scale (e.g., SD of 0.5, SD of 1, or range of 1). For example, a prior with an SD of 5 is more or less informative depending on whether income is dollars or thousands of dollars---the coefficient is 1000x larger in the latter case.\n\n$$\n\\beta \\sim \\mathcal{N}(0, 5^2 I).\n$$\n\n## Stan Model\n\nTo use Stan, we need to represent the model in Stan's syntax.^[Stan is a *declarative* modeling language: you describe the **data** you have, the **parameters** you don’t know, and the **probabilistic relationships** (priors and likelihood) that connect them. Stan then draws samples from the posterior using HMC.]\n\nStan's [documentation](https://mc-stan.org) is excellent. They have a [user's guide](https://mc-stan.org/docs/stan-users-guide/), a [reference manual](https://mc-stan.org/docs/reference-manual/), several [tutorials](https://mc-stan.org/learn-stan/tutorials.html), and many [case-studies](https://mc-stan.org/learn-stan/case-studies.html).\n\nEach Stan model has three main blocks.\n\n1. `data { ... }` Known inputs provided from R .\n1. `parameters { ... }` Unknowns to infer (e.g., `vector[K] beta;`, `real<lower=0> sigma;`). Put **constraints** that reflect the parameter's support.\n1. `model { ... }` Priors and likelihood live here. Use sampling statements (`theta ~ normal(0,1);`) \n\nThere are other blocks you can use as well.\n\nAs an example, the model below is how we can represent our logit model in Stan. We should save this text file as `logit.stan` in our project directory.\n\nThe Stan model mirrors the math closely. \n\n```stan\n# data block\ndata {\n  int<lower=0> N;  // rows in design matrix\n  int<lower=1> K;  // colummns in design matrix\n  array[N] int<lower=0, upper=1> y;  // binary outcome\n  matrix[N, K] X;  // design matrix\n}\n\n# parameters block\nparameters {\n  vector[K] beta;  // logit coefficients\n}\n\n# model block\nmodel {\n  beta ~ normal(0, 5);               // n(0, 5) prior for each beta\n  y ~ bernoulli_logit(X * beta);     // logistic regression likelihood\n}\n```\n\n\n## Data for Stan\n\nWe can use the same turnout data and formula from the Metropolis chapter.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load only the turnout data frame\nturnout <- ZeligData::turnout  |>\n  # hard code rescaled variables, because comparisons doesn't like them in the formula for some reason I don't understand\n  mutate(across(age:income, arm::rescale, .names = \"rs_{.col}\")) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,000\nColumns: 8\n$ race       <fct> white, white, white, white, white, white, white, white, whi…\n$ age        <int> 60, 51, 24, 38, 25, 67, 40, 56, 32, 75, 46, 52, 22, 60, 24,…\n$ educate    <dbl> 14, 10, 12, 8, 12, 12, 12, 10, 12, 16, 15, 12, 12, 12, 14, …\n$ income     <dbl> 3.346, 1.856, 0.630, 3.418, 2.785, 2.387, 4.286, 9.320, 3.8…\n$ vote       <int> 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ rs_age     <dbl> 0.4197, 0.1628, -0.6079, -0.2082, -0.5793, 0.6195, -0.1512,…\n$ rs_educate <dbl> 0.2868, -0.3067, -0.0099, -0.6034, -0.0099, -0.0099, -0.009…\n$ rs_income  <dbl> -0.09566, -0.35915, -0.57595, -0.08284, -0.19482, -0.26532,…\n```\n\n\n:::\n\n```{.r .cell-code}\n# build model frame\nf <- vote ~ rs_age + rs_educate + rs_income + race\nmf <- model.frame(f, data = turnout)\nX  <- model.matrix(f, data = mf)\ny  <- model.response(mf)\n\n# bundle data for Stan\nstan_data <- list(\n  N = nrow(X),\n  K = ncol(X),\n  y = as.integer(y),\n  X = X\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# make the Document Self-Contained (silently write `logit.stan`)\nstan_code <- \"\ndata {\n  int<lower=0> N;\n  int<lower=1> K;\n  array[N] int<lower=0, upper=1> y;\n  matrix[N, K] X;\n}\nparameters {\n  vector[K] beta;\n}\nmodel {\n  beta ~ normal(0, 5);               // weakly informative prior\n  y ~ bernoulli_logit(X * beta);     // logistic regression likelihood\n}\n\"\n\n# Write the Stan program so the document runs without external files.\nwriteLines(stan_code, con = 'logit.stan')\n```\n:::\n\n\n\n\n## `rstan`\n\nThe {rstan} package allows us to compile and run Stan models directly inside R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\n\nfit_rstan <- stan(\n  file = \"logit.stan\",\n  data = stan_data,\n  chains = 4,\n  cores = 4,\n  warmup = 1000,\n  iter = 3000,  # total iter, incl warmup\n  seed = 123\n)\n```\n:::\n\n\n\n\nThe output of `stan()` has a handy `print()` method.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit_rstan, pars = \"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=3000; warmup=1000; thin=1; \npost-warmup draws per chain=2000, total post-warmup draws=8000.\n\n        mean se_mean   sd  2.5%  25%  50%  75% 97.5% n_eff Rhat\nbeta[1] 1.06       0 0.14  0.78 0.96 1.06 1.16  1.34  4596    1\nbeta[2] 1.00       0 0.12  0.77 0.92 1.00 1.08  1.24  6057    1\nbeta[3] 1.19       0 0.14  0.92 1.10 1.19 1.28  1.46  5686    1\nbeta[4] 1.00       0 0.15  0.71 0.90 1.00 1.11  1.31  6289    1\nbeta[5] 0.25       0 0.15 -0.03 0.16 0.25 0.35  0.54  4763    1\n\nSamples were drawn using NUTS(diag_e) at Wed Oct  8 12:51:46 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\n## `cmdstanr`\n\nThe {cmdstanr} package interfaces with the CmdStan engine outside R. It compiles quickly and provides clear diagnostic output. This is slightly more tedious to set up and maintain, but is preferred to {rstan}.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cmdstanr)\n\nmod <- cmdstan_model(\"logit.stan\")\n\nfit_cmd <- mod$sample(\n  data = stan_data,\n  chains = 4,\n  cores = 4,\n  iter_warmup = 1000,\n  iter_sampling = 2000,  # excluding warmup\n  seed = 123\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_cmd$summary(variables = \"beta\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 10\n  variable  mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail\n  <chr>    <dbl>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 beta[1]  1.06   1.06  0.138 0.136 0.837  1.29   1.00    4575.    4750.\n2 beta[2]  0.997  0.997 0.121 0.122 0.802  1.20   1.00    5755.    4801.\n3 beta[3]  1.19   1.18  0.134 0.133 0.970  1.41   1.00    5891.    6005.\n4 beta[4]  1.01   1.01  0.155 0.154 0.755  1.26   1.00    5964.    5457.\n5 beta[5]  0.252  0.254 0.144 0.144 0.0130 0.482  1.00    4601.    4779.\n```\n\n\n:::\n:::\n\n\n\n\nBoth interfaces fit the same model and return similar results. Either is fine! I recommend {cmdstanr}, though it is *slightly* more work to get started.\n\n| Feature | `rstan` | `cmdstanr` |\n|----------|----------|-------------|\n| Compilation | Inside R | External CmdStan executable |\n| Speed | Slower | Faster (cached binaries) |\n| Debugging | Less transparent | Easier (visible console output) |\n| Recommendation for new users | ✅ Works fine | ✅ Preferred |\n\n## Diagnostics\n\nStan reports two essentials:\n\n- $\\hat{R}$: near 1 indicates convergence across chains.\n- Effective Sample Size (ESS): how much independent information is in the draws.\n\nThe `print()` methods make these easy to find, but we can also compute them directlyAs a rule of thumb, check $\\hat{R} < 1.01$ and ESS > 2,000.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(posterior)\n\ndraws_beta <- fit_cmd$draws(variables = \"beta\")  # draws array\nrhat(draws_beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.18\n```\n\n\n:::\n\n```{.r .cell-code}\ness_bulk(draws_beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.91\n```\n\n\n:::\n\n```{.r .cell-code}\ness_tail(draws_beta)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\n\n\n### {bayesplot}\n\nThe {bayesplot} packages offers many useful functions for visualizing the simulations. There are three examples below, but there are many more useful tools in {bayesplot}. Again, the [documentation](https://mc-stan.org/bayesplot/) is excellent.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(bayesplot)\n\n# densities of parameters by chain\nmcmc_dens_overlay(fit_rstan)\n```\n\n::: {.cell-output-display}\n![](02-stan_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ridges plot of densities of parameters\nmcmc_areas_ridges(fit_rstan, regex_pars = \"beta\")\n```\n\n::: {.cell-output-display}\n![](02-stan_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# r-hat\nmcmc_rhat(rhat(fit_rstan))\n```\n\n::: {.cell-output-display}\n![](02-stan_files/figure-html/unnamed-chunk-9-3.png){width=672}\n:::\n:::\n\n\n\n\n### {ShinyStan}\n\nThe **ShinyStan** app provides an interactive dashboard for convergence diagnostics, traceplots, divergences, and posterior summaries.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# launching a GUI is interactive; keep this non-evaluated by default\nlibrary(shinystan)\n\n# for rstan\nlaunch_shinystan(fit_rstan)\n\n# for cmdstanr: convert to a stanfit first, then launch\nsf <- rstan::read_stan_csv(fit_cmd$output_files())\nlaunch_shinystan(sf)\n```\n:::\n\n\n\n\nYou can inspect $\\hat{R}$, ESS, and visual diagnostics interactively without additional coding.\n\n\n## Quantities of Interest\n\nWe have simulations of $\\beta$. We can summarize these with means, medians, SDs, quantiles, etc. The {posterior} package offers many useful function for working with the simulations from {rstan} and {cmdstanr}. Again, the documentation for {posterior} is excellent. Here, we can use the `as_draws_matrix()` function to extract the MCMC samples from `fit_rstan`. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# put the simulations of the coefficients into a matrix\nbeta_tilde_raw <- posterior::as_draws_matrix(fit_rstan, regex_pars = \"beta\")\nhead(beta_tilde_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A draws_matrix: 6 iterations, 1 chains, and 6 variables\n    variable\ndraw beta[1] beta[2] beta[3] beta[4] beta[5]  lp__\n   1    1.41    1.31     1.6    0.82  -0.024 -1020\n   2    1.35    1.07     1.2    1.17  -0.066 -1015\n   3    0.91    0.94     1.1    1.01   0.429 -1013\n   4    0.96    0.96     1.1    0.87   0.281 -1013\n   5    0.92    0.93     1.0    1.16   0.450 -1014\n   6    1.12    0.90     1.1    0.96   0.024 -1016\n```\n\n\n:::\n:::\n\n\n\n\nThis has an extra column for `lp__` so let's grab *only* the first five columns. Notice that the `draw` is a rowname, not a column.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# drop the unnecessary cols\nbeta_tilde <- as.matrix(beta_tilde_raw)[, 1:5]\nhead(beta_tilde) # first \"column\" is the row names\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A draws_matrix: 6 iterations, 1 chains, and 5 variables\n    variable\ndraw beta[1] beta[2] beta[3] beta[4] beta[5]\n   1    1.41    1.31     1.6    0.82  -0.024\n   2    1.35    1.07     1.2    1.17  -0.066\n   3    0.91    0.94     1.1    1.01   0.429\n   4    0.96    0.96     1.1    0.87   0.281\n   5    0.92    0.93     1.0    1.16   0.450\n   6    1.12    0.90     1.1    0.96   0.024\n```\n\n\n:::\n:::\n\n\n\n\nTo compute a first difference, transform draws using the invariance principle.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make X_lo\nX_lo <- cbind(\n  \"constant\" = 1, # intercept\n  \"rs_age\" = -0.5, # 1 SD above avg -- see ?arm::rescale\n  \"rs_educate\" = 0,\n  \"rs_income\" = 0,\n  \"white\" = 1 # white indicator = 1 \n)\n\n# make X_hi by modifying the relevant value of X_lo\nX_hi <- X_lo\nX_hi[, \"rs_age\"] <- 0.5  # 1 SD below avg\n\n# function to compute first difference\nfd_fn <- function(beta, hi, lo) {\n  beta <- as.vector(beta)  # to prevent column/row confusion\n  plogis(hi%*%beta) - plogis(lo%*%beta)\n}\n\n# transform simulations of coefficients into simulations of first-difference\n# note 1: for clarity, just do this one simulation at a time,\n# note 2: i indexes the simulations\nfd_tilde <- numeric(nrow(beta_tilde))  # container\nfor (i in 1:nrow(beta_tilde)) {\n  fd_tilde[i] <- fd_fn(beta_tilde[i, ], hi = X_hi, lo = X_lo)\n}\n\n# posterior mean\nmean(fd_tilde)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.167\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Summary\n\nThere are three important points to remember.\n\n**Stan as modern MCMC**  \nStan automates MCMC using a hyper-optimized version of HMC. It efficiently samples from posteriors that are otherwise difficult to explore.\n\n**Interfaces**  \n\nBoth `rstan` and `cmdstanr` run the same Stan model. They differ mainly in compilation method and speed.\n\n**Diagnostics and Visualization**  \n\nAlways check $\\hat{R}$ and ESS to ensure convergence. ShinyStan makes it easy to explore diagnostics interactively. `summary()` and `print()` methods make these easily available.\n\nStan generalizes what we learned with the Metropolis algorithm: the logic is the same, but the computation is vastly more efficient.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}