{
  "hash": "7425ffb2b5fcd631d8a0ac61f40f07bb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Metropolis Algorithm\"\nformat:\n  html:\n    theme: default\nexecute:\n  cache: true\n---\n\n\n\n\n\n\n\n\n\nPreviously, we used the rejection algorithm to build intuition about sampling from an unnormalized posterior. We showed that it can be easier to *sample from* a distribution than to work with closed-form results. The rejection algorithm is simple and intuitive, but requires (1) finding a suitable envelope constant and (2) rejecting many samples in realistic problems (e.g., logistic regression). \n\nWe can improve the rejection algorithm by making *adaptive* proposals. Many algorithms---including Stan!---use addaptive proposals. The Metropolis algorithm is one example. The Metropolis algorithm belongs to a larger class of Markov chain Monte Carlo (MCMC) algorithms,^[MCMC refers to a family of algorithms for generating samples from a target probability distribution by constructing a Markov chain whose stationary distribution is that target. Over many iterations, the samples produced by the chain approximate independent draws from the posterior distribution, allowing us to estimate quantities such as means, variances, and credible intervals even when the posterior cannot be computed analytically.] which includes Metropolis-Hastings, Gibbs sampling, and the Hamiltonian Monte Carlo algorithm used by Stan.\n\n## Algorithm: Metropolis\n\n*Inputs:*\n\n- The unnormalized log-posterior $\\log f(\\theta\\mid y)$.\n- Desired number of iterations $S$. *Note: the samples are dependent so choose large $S$ (e.g., $S = 20,000$), perhaps very large (e.g., $S = 200,000$).* \n- Tuning parameter $\\tau$, which controls the width of the uniform proposal density.\n\n*Algorithm:*\n\n- **Initialize:** Choose an initial $\\theta^{(1)}$. Set $s=1$.\n- **Repeat until $s=S$:**\n  - **Propose:** Draw $z$ from a uniform distribution from $[\\theta^{(s)} - \\tau]$ to $[\\theta^{(s)} + \\tau]$.\n  - **Compute log-acceptance:** $\\Delta = \\log f(z)-\\log f \\big(\\theta^{(s)}\\big)$\n  - **Accept/reject:** Draw $u \\sim \\text{Uniform}(0,1)$. Accept iff $\\log u \\le \\Delta$. If accept, set $\\theta^{(s+1)}=z$; otherwise $\\theta^{(s+1)}=\\theta^{(s)}$.^[Importantly, this accepts with probability equal to $\\frac{f(\\text{proposal})}{f(\\text{current})$ if the ratio is between zero and one and always if the ratio is greater than one. This step is similar to the rejection algorithm, except we keep the current value in the rejected values place in Metropolis.]\n  - **Iterate:** $s\\leftarrow s+1$.\n\n*Output:* A Markov chain $\\{\\theta^{(s)}\\}_{s=1}^S$ with stationary distribution $f(\\theta\\mid y)$. Consecutive draws are not independent; in practice, discard a burn-in and tune $\\tau$ to maintain a reasonable acceptance rate (perhaps 0.2-0.5, problem-dependent).\n\nWe can write this algorithm in R. \n\n### Minimal version\n\nThe version below has only the critical pieces.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrop <- function(logf, theta_start, S = 10000, tau = 0.1, ...) {\n  # initialize matrix of samples with starting values\n  k <- length(theta_start)\n  samples <- matrix(NA_real_, nrow = S, ncol = k)\n  samples[1, ] <- theta_start\n\n  # proceed with algorithm\n  for (s in 2:S) {\n    # extract current location\n    current <- samples[s - 1, ]\n\n    # generate symmetric random-walk proposal\n    proposed_move <- runif(k, -tau, tau)\n    proposal <- current + proposed_move\n\n    # acceptance step\n    delta <- logf(proposal, ...) - logf(current, ...)\n    if (delta > 0) {\n      accept <- TRUE\n    } else {\n      accept <- (log(runif(1)) <= delta)\n    }\n\n    # update samples\n    if (accept) {\n      samples[s, ] <- proposal\n    } else {\n      samples[s, ] <- current\n    }\n  }\n\n  samples\n}\n```\n:::\n\n\n\n\n\n\n\n### Some features added\n\nThe version below adds a few features, like a helpful message and an optional progress bar.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetrop <- function(logf, theta_start, S = 10000, tau = 0.1, progress_bar = FALSE, ...) {\n  # record start time\n  start_time <- Sys.time()\n\n  # initialize matrix of samples with starting values\n  k <- length(theta_start)\n  samples <- matrix(NA_real_, nrow = S, ncol = k)\n  samples[1, ] <- theta_start\n  n_accepted <- 0\n\n  # optionally initialize progress bar\n  if (progress_bar) {\n    pb <- txtProgressBar(min = 0, max = S, style = 3)\n  }\n\n  # proceed with algorithm\n  for (s in 2:S) {\n    # extract current location\n    current <- samples[s - 1, ]\n\n    # generate symmetric random-walk proposal\n    proposed_move <- runif(k, -tau, tau)\n    proposal <- current + proposed_move \n\n    # acceptance step\n    delta  <- logf(proposal, ...) - logf(current, ...)\n    if (delta > 0) {\n      accept <- TRUE\n    } else {\n      accept <- (log(runif(1)) <= delta)\n    }\n\n    # update samples\n    if (accept) {\n      samples[s, ] <- proposal\n      n_accepted <- n_accepted + 1\n    } else {\n      samples[s, ] <- current\n    }\n\n    # update progress bar occasionally\n    if (progress_bar && (s %% (S / 100) == 0 || s == S)) {\n      setTxtProgressBar(pb, s)\n    }\n  }\n\n  # close progress bar if used\n  if (progress_bar) close(pb)\n\n  # print summary report\n  message(\n    paste0(\n      \"ðŸ’ª Successfully generated \", scales::comma(S), \" dependent samples! ðŸŽ‰\\n\\n\",\n      \"âœ… Accepted moves: \", scales::comma(n_accepted), \"\\n\",\n      \"ï¹ª Acceptance rate: \", scales::percent(n_accepted / (S - 1), accuracy = 1),\n      \" (tune tau so that acceptance rate is about 20%-50%).\\n\",\n      \"â° Total time: \", prettyunits::pretty_dt(Sys.time() - start_time),  \"\\n\",\n      \"ðŸ§  Reminder: if a proposal is not accepted, the previous sample is reused. This makes the samples depending on starting values and each other. Discard initial samples, use R-hat to assess convergence, and use ESS to assess effective sample size.\"\n    )\n  )\n\n  list(\n    prop_accepted = n_accepted / (S - 1),\n    samples = samples\n  )\n}\n```\n:::\n\n\n\n\n\n\n\n## Toothpaste Cap\n\nThis example has a known analytical solution. For $k=8$ and $N=150$, the posterior is $\\text{beta}(11,165)$.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create log-target for Beta posterior; -Inf outside (0,1)\nlogf <- function(p) {\n  ifelse(p <= 0 | p >= 1, \n         -Inf, \n         dbeta(p, 11, 165, log = TRUE))\n}\n\n# run algorithm starting in the middle of (0,1)\nm <- metrop(\n  logf,            # log-target\n  theta_start = 0.5\n)\n```\n:::\n\n\n\n\n\n\n\nWe can compare the analytical posterior mean with the simulation.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# closed-form posterior mean\n11 / (11 + 165)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0625\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulations (samples is S x k; here k = 1)\nmean(m$samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06293733\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nWe can plot the histogram and time series of the samples. The chain explores the posterior nicely overall; zooming into the first 100 iterations shows the initial transient; later windows show dependence across draws.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))  # arrange figures\nhist(m$samples, main = \"All 10,000 samples\")\nplot(m$samples, type = \"l\", main = \"All 10,000 samples\")\nplot(m$samples[1:100], type = \"l\", main = \"First 100 samples\")\nplot(m$samples[1001:1100], type = \"l\", main = \"Samples 1,001 through 1,100\")\n```\n\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-6-1.png){width=864}\n:::\n:::\n\n\n\n\n\n\n\nThere are two things worth noting.\n\n1. The samples are **not independent**. Many consecutive draws are identical (rejections) or move only a little; the chain **explores the target slowly**.\n2. It takes a few iterations for the chain to reach the high-probability region (burn-in). Starting at $\\pi=0.5$ is far from the posterior mass; it takes roughly a few dozen iterations to reach $\\approx 0.1$.\n\n### Burn-in\n\nThe MCMC samples are heavily dependent on the starting values, so discard early samples that depend strongly on the starting value (the \"burn-in\"). Discarding the first 10% to 50% of the samples is typical.\n\nRather than use all the samples (including our chosen starting value) to compute the posterior mean, let's use only the last 5,000 samples.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulations, keep only samples 5,001 through 10,000 (discard first half)\nmean(m$samples[5001:10000, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06248805\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### $\\hat{R}$ or R-hat\n\nBut how do we know how long to run our algorithms? How do we know if the samples have converged to the target distribution. Until recently, this was a difficult problem. Now we simply quickly check that R-hat is less than 1.01.\n\nTo compute R-hat, run several chains from overdispersed starting values. It is importantly that these be high and low relevative to the target distribution. If chains have converged, within- and between-chain variation align and $\\hat{R}\\approx 1$. The literature and practices on R-hat are evolving rapidly; see `?posterior::rhat` for the latest references. \n\nFor example, we can run the algorithm for 25 iterations and see that R-hat is much larger than 1.01.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run algorithm four times, making four \"chains\"\nm1 <- metrop(logf, theta_start = 0.01, S = 25)\nm2 <- metrop(logf, theta_start = 0.25, S = 25)\nm3 <- metrop(logf, theta_start = 0.75, S = 25)\nm4 <- metrop(logf, theta_start = 0.99, S = 25)\n\n# a matrix of chains for first (and only) dimension\nmatrix_of_chains <- cbind(\n  m1$samples[, 1],\n  m2$samples[, 1],\n  m3$samples[, 1],\n  m4$samples[, 1]\n)\n\n# compute r-hat using the rhat() function in {posterior}\nposterior::rhat(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.576923\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nIf we increase S to a larger value and discard the intial samples, then R-hat falls below 1.01.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# run algorithm four times, making four \"chains\"\nm1 <- metrop(logf, theta_start = 0.01, S = 25000)\nm2 <- metrop(logf, theta_start = 0.25, S = 25000)\nm3 <- metrop(logf, theta_start = 0.75, S = 25000)\nm4 <- metrop(logf, theta_start = 0.99, S = 25000)\n\n# a matrix of chains for first (and only) dimension\nmatrix_of_chains <- cbind(\n  m1$samples[10001:25000, 1],\n  m2$samples[10001:25000, 1],\n  m3$samples[10001:25000, 1],\n  m4$samples[10001:25000, 1]\n)\n\n# compute r-hat using the rhat() function in {posterior}\nposterior::rhat(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.000696\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Parallel\n\nMultiple chains are naturally parallelizable. In fact, these are called \"embarrassingly parallel,\" because the chains are run entirely independently---nothing from the first chain is needed for the second, and so on.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# count physical cores (leave some free)\nparallel::detectCores(logical = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nHere is how we might use the {foreach} package to run the chains in parallel. This isn't important here, because each chain runs very quickly. However, it is important in practice, and modern packages make it easy.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(foreach)\nlibrary(doParallel)\n\n# overdispersed starting values\nstarting_values <- seq(0.05, 0.95, length.out = 10)\n\n# register parallel backend\ncl <- makeCluster(10)\nregisterDoParallel(cl)\n\n# run algorithm 10 times in paralle., combine samples with cbind()\nmatrix_of_chains <- foreach(s = starting_values, .combine = cbind) %dopar% {\n  m <- metrop(\n    logf,\n    theta_start = s,\n    S = 25\n  )\n  m$samples\n}\n\n# stop the parallel backend\nstopCluster(cl)\n\n# columns are chains; rows are samples\nhead(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]  [,3]  [,4]  [,5]  [,6]  [,7] [,8]  [,9] [,10]\n[1,] 0.0500 0.1500 0.250 0.350 0.450 0.550 0.650 0.75 0.850 0.950\n[2,] 0.0500 0.1076 0.250 0.253 0.450 0.499 0.564 0.75 0.850 0.946\n[3,] 0.0746 0.1076 0.200 0.168 0.425 0.465 0.494 0.75 0.768 0.900\n[4,] 0.0746 0.1076 0.200 0.168 0.327 0.465 0.431 0.75 0.768 0.810\n[5,] 0.0746 0.0922 0.178 0.161 0.327 0.465 0.431 0.75 0.768 0.778\n[6,] 0.0746 0.0508 0.178 0.176 0.327 0.394 0.431 0.75 0.768 0.752\n```\n\n\n:::\n\n```{.r .cell-code}\n# compute R-hat using {posterior}\nposterior::rhat(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\nThe figure below shows the 25 samples from each of the 10 chains. Notice the strong dependence on starting values! In practice, we'd need to generate many more samples, discard the first samples, and compute the R-hat to demonstrate convergence.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncolnames(matrix_of_chains) <- paste(\"Chain\", 1:ncol(matrix_of_chains))\ngg_df <- matrix_of_chains |>\n  as_tibble() |>\n  mutate(Iteration = 1:n()) |>\n  pivot_longer(cols = `Chain 1`:`Chain 10`, names_to = \"Chain\", values_to = \"Sample\") |>\n  separate(Chain, into = c(\"tmp\", \"Chain Number\"), remove = FALSE) |>\n  mutate(`Chain Number` = as.numeric(`Chain Number`),\n         Chain = reorder(Chain, `Chain Number`, ordered = TRUE))\n\nggplot(gg_df, aes(x = Iteration, y = Sample, color = Chain)) +\n  geom_line() +\n  theme_ipsum(base_family = \"Source Sans 3\") +\n  labs(title = \"Samples generated by Metropolis algorithm\",\n       subtitle = \"10 separate chains\")\n```\n\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### ESS\n\nDependent Metropolis samples carry less information than the same number of independent samples. The Effective Sample Size (ESS) summarizes the amount of information in our dependent samples---\"our dependent samples are like ______ independent samples.\" The literature and practices on ESS are evolving rapidly; see `?posterior::ess_bulk` for the latest references. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(matrix_of_chains)  # 15,000 x 4 = 60,000 samples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25 10\n```\n\n\n:::\n\n```{.r .cell-code}\n# are like\nposterior::ess_bulk(matrix_of_chains)  # ESS for mean-like estimands\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17.60899\n```\n\n\n:::\n\n```{.r .cell-code}\nposterior::ess_tail(matrix_of_chains)  # ESS for tail estimands\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22.92971\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n### Tuning for intuition\n\nThe tuning parameter matters a lot. I've chosen a smart default for many problems, but the value of `tau` should be something like the SD of the posterior. Below, we can see how \"bad\" values of `tau` work.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# varying the tuning parameter tau\nm1 <- metrop(logf, theta_start = 0.99, S = 1000, tau = 0.1)\nm2 <- metrop(logf, theta_start = 0.99, S = 1000, tau = 0.01)\nm3 <- metrop(logf, theta_start = 0.99, S = 1000, tau = 0.001)\nm4 <- metrop(logf, theta_start = 0.99, S = 1000, tau = 3)\n\npar(mfrow = c(2, 2))\nplot(m1$samples, type = \"l\", main = \"tau = 0.1\\nNice!\", ylim = c(0, 1))\nplot(m2$samples, type = \"l\", main = \"tau = 0.01\\nToo small! (Accept too often.)\", ylim = c(0, 1))\nplot(m3$samples, type = \"l\", main = \"tau = 0.001\\nMuch too small! (Accept much too often.)\", ylim = c(0, 1))\nplot(m4$samples, type = \"l\", main = \"tau = 3\\nToo wide! (Accept too rarely.)\", ylim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-14-1.png){width=864}\n:::\n:::\n\n\n\n\n\n\n\n## Sawtooth Prior\n\nWe can use the Metropolis algorithm for our \"weird\" sawtooth posterior. We'll the log-posterior in this case.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sawtooth log-prior (unnormalized): log of ((n_teeth*p) %% 1) on (0,1)\nlog_prior_saw <- function(p, n_teeth = 5) {\n  ifelse(p <= 0 | p >= 1, \n  -Inf, \n  log((n_teeth * p) %% 1))\n}\n\n# likelihood for 10 tosses w/ one success\nlog_lik <- function(p) {\n  ifelse(p <= 0 | p >= 1, \n  -Inf, \n  dbinom(1, size = 10, prob = p, log = TRUE))\n}\n\n# log-posterior\nlog_post <- function(p) log_lik(p) + log_prior_saw(p)\n\n# run metropolis algorithm\nm <- metrop(log_post, theta_start = 0.5)\n\n# plot samples\npar(mfrow = c(1, 2))\nplot(m$samples, type = \"l\")\nhist(m$samples, breaks = 100)\n```\n\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-15-1.png){width=864}\n:::\n:::\n\n\n\n\n\n\n\n## Logistic Regression\n\n### Data\n\nRecall the logit model from a previous chapter. This time, let's rescale the numeric variables to have mean of zero and SD of 0.5 This keeps coefficients on a similar scale, which helps us with the computation. For more on this rescaling strategy and it's advantages, see @gelman2008.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load only the turnout data frame\nturnout <- ZeligData::turnout  # see ?ZeligData::turnout for details\n\n# fit logit model \nrs <- function(x) { arm::rescale(x) }  # make an alias to print nicely\n\n# bug??? for some reason this fails with comparisons()\nf <- vote ~ rs(age) + rs(educate) + rs(income) + race  # rescaling so that coefs are similar makes everything nicer\n\n# just have to hard-code the rescaled variables, which is not optimal\nturnout <- turnout |>\n  mutate(across(age:income, rs, .names = \"rs_{.col}\")) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,000\nColumns: 8\n$ race       <fct> white, white, white, white, white, white, white, white, whiâ€¦\n$ age        <int> 60, 51, 24, 38, 25, 67, 40, 56, 32, 75, 46, 52, 22, 60, 24,â€¦\n$ educate    <dbl> 14, 10, 12, 8, 12, 12, 12, 10, 12, 16, 15, 12, 12, 12, 14, â€¦\n$ income     <dbl> 3.3458, 1.8561, 0.6304, 3.4183, 2.7852, 2.3866, 4.2857, 9.3â€¦\n$ vote       <int> 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,â€¦\n$ rs_age     <dbl> 0.41969664, 0.16280941, -0.60785226, -0.20824991, -0.579309â€¦\n$ rs_educate <dbl> 0.286848797, -0.306657055, -0.009904129, -0.603409981, -0.0â€¦\n$ rs_income  <dbl> -0.095661199, -0.359152251, -0.575948238, -0.082837743, -0.â€¦\n```\n\n\n:::\n\n```{.r .cell-code}\nf <- vote ~ rs_age + rs_educate + rs_income + race\n\nfit <- glm(f, family = binomial, data = turnout)\n\n# print estimates\narm::display(fit, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nglm(formula = f, family = binomial, data = turnout)\n            coef.est coef.se\n(Intercept) 1.0578   0.1396 \nrs_age      0.9934   0.1212 \nrs_educate  1.1837   0.1370 \nrs_income   1.0013   0.1535 \nracewhite   0.2508   0.1465 \n---\n  n = 2000, k = 5\n  residual deviance = 2024.0, null deviance = 2266.7 (difference = 242.8)\n```\n\n\n:::\n:::\n\n\n\n\n\n\n### Log-Posterior\n\nNow define the unnormalized log-posterior. For this example, we imagine an improper, constant prior.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make X and y\nmf <- model.frame(f, data = turnout)\nX <- model.matrix(f, data = mf)\ny <- model.response(mf)\n\n# âŒ correct, but unstable, log unnormalized posterior \nlog_posterior <- function(beta, y, X) {\n  linpred <- X %*% beta\n  p <- plogis(linpred)\n  sum(dbinom(y, size = 1, prob = plogis(linpred), log = TRUE))  # occassionally makes NaN, b/c 0*Inf\n}\n\n# âœ… same unnormalized log posterior, but avoid 0*Inf instability\nlog_posterior <- function(beta, y, X) {\n  linpred <- drop(X %*% beta)\n  log1pexp <- ifelse(linpred > 0, linpred + log1p(exp(-linpred)), log1p(exp(linpred)))\n  sum(y * linpred - log1pexp)\n}\n```\n:::\n\n\n\n\n\n\n\n### Running the Algorithm\n\nAfter some experimenting, I found that I 100,000 iterations with a 20,000 iteration burn-in works well. \n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed\nset.seed(1234)\n\n# sample with metropolis\nS <- 100000\nm1 <- metrop(log_posterior, S = S, tau = 0.1, theta_start = rep(-2, ncol(X)), y = y, X = X)\nm2 <- metrop(log_posterior, S = S, tau = 0.1, theta_start = rep(-1, ncol(X)), y = y, X = X)\nm3 <- metrop(log_posterior, S = S, tau = 0.1, theta_start = rep(1, ncol(X)), y = y, X = X)\nm4 <- metrop(log_posterior, S = S, tau = 0.1, theta_start = rep(2, ncol(X)), y = y, X = X)\n```\n:::\n\n\n\n\n\n\n\nNow we can verify that the samplers have converged (R-hat less than about 1.01) and that we have enough samples (ESS more than about 2,000).\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first parameter (intercept) and use first 100,000 as burn-in\nstart <- .2*S + 1\nend <-   S\nmatrix_of_chains <- cbind(\n  m1$samples[start:end, 1],\n  m2$samples[start:end, 1],\n  m3$samples[start:end, 1],\n  m4$samples[start:end, 1]\n)\n\n# compute r-hat (for intercept)\nposterior::rhat(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.000428\n```\n\n\n:::\n\n```{.r .cell-code}\n# compute ess (for intercept)\nposterior::ess_bulk(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3187.765\n```\n\n\n:::\n\n```{.r .cell-code}\nposterior::ess_tail(matrix_of_chains)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7471.759\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ncolnames(matrix_of_chains) <- paste(\"Chain\", 1:ncol(matrix_of_chains))\ngg_df <- matrix_of_chains |>\n  as_tibble() |>\n  mutate(Iteration = 1:n()) |>\n  pivot_longer(cols = `Chain 1`:`Chain 4`, names_to = \"Chain\", values_to = \"Sample\") |>\n  separate(Chain, into = c(\"tmp\", \"Chain Number\"), remove = FALSE) |>\n  mutate(`Chain Number` = as.numeric(`Chain Number`),\n         Chain = reorder(Chain, `Chain Number`, ordered = TRUE))\n\ngg1 <- ggplot(gg_df, aes(x = Iteration, y = Sample, color = Chain)) +\n  geom_line() +\n  facet_wrap(vars(Chain))\n\ngg2 <- ggplot(gg_df, aes(x = Sample, color = Chain)) +\n  geom_density()\n\nlibrary(patchwork)\ngg1 + gg2\n```\n\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-20-1.png){width=1152}\n:::\n:::\n\n\n\n\n\n\n\nThe code above computes the R-hat and ESS for only the first parameter (intercept)---we should check the rest as well. The table below shows that the model has converged. \n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <script>\n\n      function styleCell_ya6idqsp3gfsxwuwmwio(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_ya6idqsp3gfsxwuwmwio\");\n          var cell = table.querySelector(`[data-row=\"${i}\"][data-col=\"${j}\"]`);\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function spanCell_ya6idqsp3gfsxwuwmwio(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_ya6idqsp3gfsxwuwmwio\");\n        const targetCell = table.querySelector(`[data-row=\"${i}\"][data-col=\"${j}\"]`);\n        if (!targetCell) {\n          console.warn(`Cell at (${i}, ${j}) not found.`);\n        }\n\n        // Get all cells that need to be removed\n        const cellsToRemove = [];\n        for (let r = 0; r < rowspan; r++) {\n          for (let c = 0; c < colspan; c++) {\n            if (r === 0 && c === 0) continue; // Skip the target cell\n            const cell = table.querySelector(`[data-row=\"${i + r}\"][data-col=\"${j + c}\"]`);\n            if (cell) {\n              cellsToRemove.push(cell);\n            }\n          }\n        }\n\n        // Remove all cells\n        cellsToRemove.forEach(cell => cell.remove());\n\n        // Set rowspan and colspan of the target cell if it exists\n        if (targetCell) {\n          targetCell.rowSpan = rowspan;\n          targetCell.colSpan = colspan;\n        }\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: '5', j: 0 }, { i: '5', j: 1 }, { i: '5', j: 2 }, { i: '5', j: 3 }, { i: '5', j: 4 },  ], css_id: 'tinytable_css_9q7k2lx57b0pl6tco399',}, \n          { positions: [ { i: '0', j: 0 }, { i: '0', j: 1 }, { i: '0', j: 2 }, { i: '0', j: 3 }, { i: '0', j: 4 },  ], css_id: 'tinytable_css_cbkwion0dltqwuxxsxx4',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_ya6idqsp3gfsxwuwmwio(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_9q7k2lx57b0pl6tco399, .table th.tinytable_css_9q7k2lx57b0pl6tco399 { border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_cbkwion0dltqwuxxsxx4, .table th.tinytable_css_cbkwion0dltqwuxxsxx4 { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_ya6idqsp3gfsxwuwmwio\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\" data-row=\"0\" data-col=\"0\">Coefficient</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"1\">R-hat</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"2\">Converged (R-hat < 1.01)</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"3\">ESS (bulk)</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"4\">ESS (tail)</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td data-row=\"1\" data-col=\"0\">(Intercept)</td>\n                  <td data-row=\"1\" data-col=\"1\">1.000</td>\n                  <td data-row=\"1\" data-col=\"2\">âœ…</td>\n                  <td data-row=\"1\" data-col=\"3\">3,188</td>\n                  <td data-row=\"1\" data-col=\"4\">7,472</td>\n                </tr>\n                <tr>\n                  <td data-row=\"2\" data-col=\"0\">rs(age)</td>\n                  <td data-row=\"2\" data-col=\"1\">1.000</td>\n                  <td data-row=\"2\" data-col=\"2\">âœ…</td>\n                  <td data-row=\"2\" data-col=\"3\">6,803</td>\n                  <td data-row=\"2\" data-col=\"4\">14,584</td>\n                </tr>\n                <tr>\n                  <td data-row=\"3\" data-col=\"0\">rs(educate)</td>\n                  <td data-row=\"3\" data-col=\"1\">1.001</td>\n                  <td data-row=\"3\" data-col=\"2\">âœ…</td>\n                  <td data-row=\"3\" data-col=\"3\">4,889</td>\n                  <td data-row=\"3\" data-col=\"4\">10,768</td>\n                </tr>\n                <tr>\n                  <td data-row=\"4\" data-col=\"0\">rs(income)</td>\n                  <td data-row=\"4\" data-col=\"1\">1.000</td>\n                  <td data-row=\"4\" data-col=\"2\">âœ…</td>\n                  <td data-row=\"4\" data-col=\"3\">4,445</td>\n                  <td data-row=\"4\" data-col=\"4\">9,648</td>\n                </tr>\n                <tr>\n                  <td data-row=\"5\" data-col=\"0\">racewhite</td>\n                  <td data-row=\"5\" data-col=\"1\">1.000</td>\n                  <td data-row=\"5\" data-col=\"2\">âœ…</td>\n                  <td data-row=\"5\" data-col=\"3\">3,157</td>\n                  <td data-row=\"5\" data-col=\"4\">7,773</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n\n\n### Comparing\n\nFinally, we can compare the posterior mean and 95% credible intervals to the ML estimate and 95% Wald confidence intervals. They are almost identical.\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01-metrop_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Quantities of Interest\n\nTo compute a quantity of interest, we need to use our Bayesian invariance property. We need to transform, then summarize, the simulations. \n\nHere's an example of the first difference for a \"typical\" case.\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make X_lo\nX_lo <- cbind(\n  \"constant\" = 1, # intercept\n  \"rs_age\" = -0.5, # 1 SD below avg -- see ?arm::rescale\n  \"rs_educate\" = 0,\n  \"rs_income\" = 0,\n  \"white\" = 1 # white indicator = 1 \n)\n\n# make X_hi by modifying the relevant value of X_lo\nX_hi <- X_lo\nX_hi[, \"rs_age\"] <- 0.5  # 1 SD above avg\n\n# function to compute first difference\nfd_fn <- function(beta, hi, lo) {\n  plogis(hi%*%beta) - plogis(lo%*%beta)\n}\n\n# put the simulations of the coefficients into a matrix\n# note 1: each row is one beta-tilde\n# note 2: we're discarding the first samples (start and end defined above)\n# note 3: we're just stacking the chains on top of each other\nbeta_tilde <- rbind(\n  m1$samples[start:end, ], # chain 1, minus burn-in\n  m2$samples[start:end, ], # chain 2, minus burn-in\n  m3$samples[start:end, ], # chain 3, minus burn-in\n  m4$samples[start:end, ]  # chain 4, minus burn-in\n)\n\n# transform simulations of coefficients into simulations of first-difference\n# note 1: for clarity, just do this one simulation at a time,\n# note 2: i indexes the simulations\nfd_tilde <- numeric(nrow(beta_tilde))  # container\nfor (i in 1:nrow(beta_tilde)) {\n  fd_tilde[i] <- fd_fn(beta_tilde[i, ], hi = X_hi, lo = X_lo)\n}\n\n# posterior mean\nmean(fd_tilde)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1668593\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n::: {.aside}\n\nAnd we can replicate this with {marginaleffect}\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(marginaleffects)\n\n# compute qi; rs(age), etc, not working---not sure why\ncomparisons(fit, variables = list(rs_age = c(-0.5, 0.5)), \n            newdata = datagrid(grid_type = \"mean_or_mode\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n    0.166       0.02 8.32   <0.001 53.4 0.127  0.205\n\nTerm: rs_age\nType: response\nComparison: 0.5 - -0.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n:::\n\n## Summary\n\nThere are three important points in this chapter.\n\n**MCMC**\n\nMCMC offers a powerful and general way to sample from an unnormalized (log-)posterior. The Metropolis algorithm is one such method. Hamiltonian Monte Carlo (HMC) via Stan is even better.\n\n**R-hat and burn-in**\n\nThe first MCMC samples highly dependent on the starting values. Because of this, you need to:\n\n1. Discard the samples from a burn-in period. \n2. Run multiple chains and check that R-hat is less than 1.01.\n\n**ESS and large samples**\n\nThe MCMC samples are dependent on the previous samples. Because of this, you need to: \n\n1. Generate more samples that you would need if they were independent. \n1. Use ESS to understand your effective sample size. ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}