{
  "hash": "e5945efb59225a46a8ccaa70b590bcf9",
  "result": {
    "engine": "knitr",
    "markdown": "# Information Criteria\n\nSuppose we are modeling Holland's (2015) enforcement operations in Santiago. We have two immediate choices.\n\n1. Poisson regression: the simplest regression model for count outcomes, but usually avoided because the Poisson distribution assumes the mean equals the variance.\n1. Negative binomial regression. Slightly more complicated model, but usually preferred as a default because it allows for *overdispersion* so that the variance is greater than the mean.\n\nHow can we choose among these models?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load package\nlibrary(tidyverse)\n\n# load data for santiago\nsant <- crdata::holland2015 |> \n  filter(city == \"santiago\")\n\n# formula corresponds to model 1 for each city in holland (2015) table 2\nf <- operations ~ lower + vendors + budget + population\n\n# poisson regression\npois_fit <- glm(f, family = poisson, data = sant)\n\n# nb regression\nnb_fit <- MASS::glm.nb(f, data = sant)\n```\n:::\n\n\n\n\n## Predictive distributions\n\nTo guide our choice between the relative strengthos of these models, predictive distributions are *excellent*. In my mind, they are one of the most useful tools for model checking.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate from predictive distribution for poisson\npois_sims <- simulate(pois_fit, nsim = 5)\nhead(pois_sims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sim_1 sim_2 sim_3 sim_4 sim_5\n1     2     5     0     3     3\n2     2     1     1     0     0\n3     0     0     0     0     1\n4     4     1     1     1     3\n5     5     2     1     0     3\n6     1     1     2     0     0\n```\n\n\n:::\n:::\n\n\n\n\nTo evaluate the fit of the model, we can compare the simulated outcomes to the observed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# observed data\nmean(sant$operations)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.705882\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sant$operations)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.939203\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulations\napply(pois_sims, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sim_1    sim_2    sim_3    sim_4    sim_5 \n2.647059 2.911765 2.205882 2.911765 2.823529 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(pois_sims, 2, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sim_1    sim_2    sim_3    sim_4    sim_5 \n2.717869 2.478607 2.396707 3.324543 2.896922 \n```\n\n\n:::\n:::\n\n\n\n\nWe can also plot the simulations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# plot\nbind_cols(sant, pois_sims) |>\n  pivot_longer(cols = c(operations, starts_with(\"sim_\"))) |>\n  separate(name, into = c(\"type\", \"sim_id\"), sep = \"_\", remove = FALSE) |>\n  ggplot(aes(x = value)) + \n  facet_wrap(vars(name)) +\n  geom_histogram(center = 0, width = 1)\n```\n\n::: {.cell-output-display}\n![](02-ic_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# simulate from predictive distribution for nb\nnb_sims <- simulate(nb_fit, nsim = 5)\nhead(nb_sims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sim_1 sim_2 sim_3 sim_4 sim_5\n1     1     1     0     0     0\n2     0     0     0     0     0\n3     0     0     1     1     0\n4    16     0     0     0     0\n5     0     2     2     0     0\n6     3     1     1     0     2\n```\n\n\n:::\n:::\n\n\n\n\nTo evaluate the fit of the model, we can compare the simulated outcomes to the observed.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# observed data\nmean(sant$operations)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.705882\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(sant$operations)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.939203\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulations\napply(nb_sims, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sim_1    sim_2    sim_3    sim_4    sim_5 \n2.911765 4.911765 3.558824 3.500000 3.411765 \n```\n\n\n:::\n\n```{.r .cell-code}\napply(nb_sims, 2, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    sim_1     sim_2     sim_3     sim_4     sim_5 \n 5.287880 19.103177 10.100079  7.770379  9.733160 \n```\n\n\n:::\n:::\n\n\n\n\nWe can also plot the simulations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# plot\nbind_cols(sant, nb_sims) |>\n  pivot_longer(cols = c(operations, starts_with(\"sim_\"))) |>\n  separate(name, into = c(\"type\", \"sim_id\"), sep = \"_\", remove = FALSE) |>\n  ggplot(aes(x = value)) + \n  facet_wrap(vars(name)) +\n  geom_histogram(center = 0, width = 1)\n```\n\n::: {.cell-output-display}\n![](02-ic_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## Information criteria\n\nWe can use **information** criteria for a similar purpose, but they are much similar.\n\nInformation criteria have the following general structure:\n\n$$\n-2 \\ell(\\hat{\\theta}) + [\\text{constant}\\times k ]\n$$\n\nHere, $\\ell(\\hat{\\theta}) = \\log L(\\hat{\\theta})$ is the maximized log-likelihood function (not the $\\hat{\\theta}$, but the value of $\\log L$ itself *at* $\\hat{\\theta}$), $k$ is the *total* number of parameters (including intercept, variance, scale, etc.), and $\\text{constant}$ is a constant term that varies across information criteria.\n\nThe two most common information criteria are:\n\n1.  **Akaike Information Criterion (AIC)** $= -2 \\log L(\\hat{\\theta}) + [2 \\times k]$\n2.  **Bayesian Information Criterion (BIC)** $= -2 \\log L(\\hat{\\theta}) + [\\log(n) \\times k]$\n\nThe AIC and BIC have a deep and detailed theoretical development---the choice of constant is not at all arbitrary. I don't reproduce the theory here, but instead mention a few practical points.\n\n-   The *magnitude* of the IC is generally not of interest. Instead, focus on the *difference* in the IC between models.\n-   Both the the AIC and the BIC work to identify the \"best\" model, but in two difference senses:\n    -   The AIC roughly compares the observed and predictive distributions are tries to identify the best match.\n    -   The BIC roughly identifies the model with the highest posterior probability---the most likely model to have generated the data.\n-   Both AIC and BIC penalize adding parameters. That is, in order to improve the IC, a more complex model must improve the fit enough to offset the additional penalty. That said, the BIC imposes a larger penalty for $n \\geq 8$.\n\nThe table below from @raftery1995 summarizes a rough interpretation of the magnitude of differences between BICs below (and the same applies for AICs as well.\n\n![](img/raftery1995-table.png)\n\nTo compute the AIC and BIC, we have the easy-to-use `AIC()` and `BIC()` functions.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(pois_fit, nb_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         df      AIC\npois_fit  5 226.7905\nnb_fit    6 133.6848\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(pois_fit, nb_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         df      BIC\npois_fit  5 234.4223\nnb_fit    6 142.8430\n```\n\n\n:::\n:::\n\n\n\n\nTo ease interpretation, we can convert these AIC and BIC to weights. Raftery argues that we can interprete these as the probability that each model is correct (assuming the correct model is in the set).\n\n![](img/raftery-1995-post-prob.png)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(pois_fit, nb_fit) |>\n  mutate(diff_min = BIC - min(BIC),\n         post_prob = exp(-0.5*diff_min)/sum(exp(-0.5*diff_min)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         df      BIC diff_min    post_prob\npois_fit  5 234.4223 91.57931 1.299588e-20\nnb_fit    6 142.8430  0.00000 1.000000e+00\n```\n\n\n:::\n:::\n\n\n\n\n\nWe can do something similar with the AIC. I refer to these as \"Akaike weights.\" See @wagenmakers2004 for more on this. As with BIC, we shouldn't take these weight too seriously, but they do give us an idea of how much the IC like each model. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(pois_fit, nb_fit) |>\n  mutate(diff_min = AIC - min(AIC),\n         akaike_weights = exp(-0.5*diff_min)/sum(exp(-0.5*diff_min)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         df      AIC diff_min akaike_weights\npois_fit  5 226.7905 93.10567    6.05844e-21\nnb_fit    5 133.6848  0.00000    1.00000e+00\n```\n\n\n:::\n:::\n\n\n\n\nYou can see that (as is common), both the BIC and AIC *strongly* prefer the negative binomial model over the Poisson model.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}