{
  "hash": "d180f458e509e9a3fd223c22cb7b3049",
  "result": {
    "engine": "knitr",
    "markdown": "# Rejection Sampling\n\nIn the previous chapter, we saw that in simple cases, especially when we use conjugate priors, we can find a closed-form posterior. But in most applied cases, we can only *sample from* the posterior distribution. It might be counter-intuitive that it can be *easy* to sample from a distribution without a closed form, but it's true!\n\nIn this course, we'll look at four samplers:\n\n1. rejection\n1. approximate posterior simulation\n1. Metropolis\n1. Hamiltonian Monte Carlo\n\nThe algorithms start simple and intuitive and build in complexity. A deep dive on HMC--especially the hyper-optimized version used by Stan--is beyond the scope of this course. That said, HMC via Stan and it universe of enablers in R have made posterior simulation almost trivial.\n\n## Equivalence\n\nBefore jumping into sampling algorithms, let's demonstrate the correspondence between sampling and closed-form results. \n\nIn the case of the toothpaste cap, problem we have a Bernoulli model, beta prior, and beta posterior from the previous chapter. For the prior, let's suppose $\\alpha = 3$ and $\\beta = 15$. Regardless of the summary we are interested in (e.g., mean, SD, percentiles), we can work with the closed-form result or simulations to obtain the same answer. Notice that even for this very simple closed-form result, the simulations perhaps easier to work with!\n\nIt's trivial for us to simulate from the beta posterior using the `rbeta()` function.^[This makes it a good first example, since the correspondence between `rbeta()` and `dbeta()` is obvious. In more interesting cases, the simulation will be harder.] And the summaries we might want are easy to compute using closed-form results. Let's compare the two approaches.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prior parameters\nalpha_prior <- 3\nbeta_prior <- 15\n\n# data \nk <- 8\nN <- 150\n\n# posterior parameters\nalpha_posterior <- alpha_prior + k\nbeta_posterior <- beta_prior + N - k\n\n# for compact calculations below\na1 <- alpha_posterior\nb1 <- beta_posterior\n\n# posterior simulation; trivial\nn_sims <- 100000\npi_tilde <- rbeta(n_sims, shape1 = a1, shape2 = b1)\n\n# posterior mean\na1 / (a1 + b1)  # closed form\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06547619\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(pi_tilde)  # posterior sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06543861\n```\n\n\n:::\n\n```{.r .cell-code}\n# posterior sd\nsqrt((a1 * b1) / ((a1 + b1)^2 * (a1 + b1 + 1))) # closed form\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01902802\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(pi_tilde)  # posterior sim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01901066\n```\n\n\n:::\n\n```{.r .cell-code}\n# posterior 5th percentile\nqbeta(0.05, shape1 = a1, shape2 = b1)  # closed form\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03737493\n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(pi_tilde, probs = 0.05)  # posterior sims\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        5% \n0.03742531 \n```\n\n\n:::\n\n```{.r .cell-code}\n# posterior 95th percentile\nqbeta(0.95, shape1 = a1, shape2 = b1)  # closed form\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.09945329\n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(pi_tilde, probs = 0.95)  # posterior sims\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       95% \n0.09936871 \n```\n\n\n:::\n:::\n\n\n\n\n\n## A Bayesian Invariance Property\n\nFor ML estimators, we have the invariance property. The invariance property allows us to freely transform our ML estimates. But can we say something similar about Bayesian point estimates, like the posterior mean? *Kinda*, but it works slightly differently.\n\n::: {.column-margin}\n\n**Invariance Property of ML Estimators.** Suppose an ML estimator $\\hat{\\theta}$ of $\\theta$ as in @def-mle and a quantity of interest $\\tau = \\tau(\\theta)$ for any function $\\tau$. The ML estimate $\\hat{\\tau}$ of $\\tau = \\tau(\\theta)$ is $\\tau(\\hat{\\theta})$.\n:::\n\n\n### The Incorrect Way\n\nFirst, there is something that we might *want* to do, but cannot.\n\n::: callout-warning\n\n## Cannot transform posterior mean\n\nSuppose a posterior mean $\\hat{\\theta}_{mean}$ and a transformation $\\tau = \\tau(\\theta)$. Suppose we want the posterior mean of the transformation $\\hat{\\tau}_{mean}$. \n\n$$\n\\hat{\\tau}_{mean} \\neq \\tau \\left( \\hat{\\theta}_{mean} \\right)\\text{, except in special cases.}\n$$\n\nThis means: we cannot freely transform posterior means.^[I suppose we *can*, but what we get out of this process isn't also a posterior mean.] \n\n:::\n\nHowever, there is a way to obtain the posterior mean of the transformation. @thm-bayes-invariance tells us how. It comes down to the order of operations.\n\n### The Correct Way\n\nInstead of transforming the posterior mean, you need to *transform the simulations*, then take the mean.\n\n::: {#thm-bayes-invariance}\n\n## Simulation-Based Invariance Property of Posterior Distributions\n\nSuppose $\\{\\tilde{\\theta}^{(s)}\\}_{s=1}^S$ are posterior simulations of $\\theta$. Let $\\tau = \\tau(\\theta)$ be a quantity of interest for any function $\\tau$. Then posterior simulations of $\\tau$ can be obtained by applying $\\tau$ to each draw $\\tilde{\\theta}^{(s)}$ so that $\\tilde{\\tau}^{(s)} = \\tau \\left( \\tilde{\\theta}^{(s)} \\right)$. Summaries of the posterior distribution of $\\tau$ (e.g., mean, median, credible intervals) are obtained by summarizing the transformed draws $\\{\\tilde{\\tau}^{(s)}\\}_{s=1}^S$.\n:::\n\nImportantly, if you transform the posterior mean of the parameter, you no longer have posterior mean. (Same for the median.) Instead, you must transform *each simulation* before taking the mean.^[This difference will usually be small, but Jensen's inequality still applies.]\n\nWe can illustrate the the wrong way (average then transform) and the right way (transform then average). If we compute the odds (of success) $\\pi/(1 - \\pi)$, then the right way and wrong way give similar answers.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# find posterior mean of pi\nmean_pi <- mean(pi_tilde)\n\n# wrong way; can't transform posterior means\nmean_pi/(1 - mean_pi)  # NOT the posterior mean of the odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07002067\n```\n\n\n:::\n\n```{.r .cell-code}\n# right way; transform then average\nodds_tilde <- pi_tilde/(1 - pi_tilde)\nmean(odds_tilde) # the posterior mean of the odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.07046881\n```\n\n\n:::\n:::\n\n\n\n\n\nBut if we compute the odds *of failure* $(1 - \\pi)/\\pi$, then we get noticeably different answers.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# wrong way; can't transform posterior means\n(1 - mean_pi)/(mean_pi)  # NOT the posterior mean of the odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 14.2815\n```\n\n\n:::\n\n```{.r .cell-code}\n# right way; transform then average\nodds_of_failure_tilde <- (1 - pi_tilde)/pi_tilde\nmean(odds_of_failure_tilde) # the posterior mean of the odds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15.71027\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.column-margin}\n\nThis happens because the posterior distributions for the odds of success and the odds of failure are skewed differently.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(odds_tilde)\n```\n\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(odds_of_failure_tilde)\n```\n\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n\n\n\n:::\n\n## Rejection Sampling\n\nFor more difficult posteriors, we can use algorithms designed to sample from complicated distributions. Most algorithms, including Stan's hyper-optimized^[My description; not a technical term.] implementation of HMC, use a form of reject.\n\nTo highlight how rejection can help us sample from complicated distributions, let's look at a simple *rejection algorithm* that relies *entirely* on rejection.^[This rejection sampling is rarely useful in practice. It turns out that Stan has made most sampling easy. But the rejection algorithm does highlight the intuition of more complicated algorithms that Stan uses.]\n\n**Algorithm:** Rejection Sampling\n\n::: aside\nTo make the rejection algorithm simple, I've written it to apply specifically to the posterior for the Bernoulli model, which has support $[0, 1]$. The target density doesn't need to be a posterior and can have support other than $[0, 1]$. The proposal distribution doesn't have to be uniform. The key is that $M$ is larger than the maximum of the target distribution and draws are accepted with probability $f(z)/M$. \n:::\n\n*Inputs:* \n\n- The *unnormalized* posterior distribution $f(\\pi \\mid y)$ on [0,1].  \n- Desired number of draws $S$.  \n- An envelope constant $M$ that is larger than $f(\\pi)$ for all $\\pi$. For our simple 1D cases, we can plot the posterior and select $M$ visually. We could also use use `optim()` to find the posterior mode.\n\n*Algorithm*:\n\n1. **Initialize:** Set $s=1$.\n1. **Repeat until $s=S$:**\n   a. Propose $z \\sim \\text{uniform}(0,1)$.  \n   b. Draw $u \\sim \\text{uniform}(0,1)$. Used to control reject/accept rates.\n   c. **Acceptâ€“reject step:**  \n      - If $u \\le \\dfrac{f(z)}{M}$, **accept**. Set $\\pi^{(s)} = z$ and update $s \\leftarrow s+1$. Because $u$ is uniform, this accepts with probability $\\dfrac{f(z)}{M}$.\n      - Otherwise **reject** $z$ and return to Step 2a.\n\n*Output:* Independent samples $\\pi^{(1)}, \\pi^{(2)}, \\dots, \\pi^{(B)}$ from $f(\\pi \\mid y)$.\n\n### Beta(4, 10) Example\n\nThe figure below shows the logic of the rejection algorithm assuming a $\\pi \\sim \\text{beta}(4, 10)$ target distribution. We set $M = 4$ visually, but notice that we could set it at to 3.5 as well. We'll generate proposals from a uniform distribution, but *accept* those proposals a different rates depending on the posterior density *at that proposal.*\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/Illustrating rejection sampler-1.png){width=768}\n:::\n:::\n\n\n\n\n\nThe code below implements the rejection algorithm shown in the figure above.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrej <- function(f, S, M) {\n  \n  # record start time\n  start_time <- Sys.time()\n  \n  # create containers and initialize counters\n  samples <- numeric(S)  # container to store samples\n  rejects <- NULL  # container to track rejected values; for teaching; slow!\n  s <- 1 # currently trying to take sample 1\n  n_prop <- 0  # count proposals (for an acceptance-rate message)\n\n  # so long as the current sample s is less \n  #   than the desired samples S.\n  #   do the following:\n  while (s <= S) { \n    \n    # A: propose z ~ uniform(0,1)\n    z <- runif(1)\n    \n    # B: draw u ~ uniform(0,1)\n    u <- runif(1)\n\n    # C: Accept or reject\n    fz <- f(z) # compute once, for effeciency\n    \n      ## scenario 1: u <= f(z)/M  â†’  Accept\n      if (u <= fz / M) {\n        samples[s] <- z\n        s <- s + 1\n      } \n    \n      ## scenario 2: f(z) > M  â†’  shouldn't happen; error\n      if (fz > M) stop(\"Stop: Envelope M is too small.\")  # find appropriate M\n      \n      ## scenario 3: u > f(z)/M  â†’  Reject\n      ##   tracking these values just for teaching and learning--not needed usually\n      if (u > fz / M) {\n         rejects <- c(rejects, z)\n      }\n    \n    # track total proposals so far\n    n_prop <- n_prop + 1\n  }\n\n # print a summary report\n  message(\n    paste0(\n      \"ðŸ’ª Successfully generated \", scales::comma(S), \" samples! ðŸŽ‰\\n\\n\",\n      \"âœ… Accepted samples: \", scales::comma(S), \"\\n\",\n      \"âŒ Rejected samples: \", scales::comma(length(rejects)), \"\\n\",\n      \"ï¹ª Acceptance rate: \", scales::percent(S / n_prop, accuracy = 1), \"\\n\",\n      \"â° Total time: \", prettyunits::pretty_dt(Sys.time() - start_time)\n    )\n  )\n\n  # return\n  list(\n    n_prop = n_prop,\n    acc_rate = S / n_prop,\n    samples = samples,\n    rejects = rejects\n  )\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# example target distribution; beta(4, 10)\nf <- function(z) {\n  dbeta(z, shape1 = 4, shape2 = 10)\n}\n# perform sampling\nr <- rej(f, 10000, 4)\n```\n:::\n\n::: {.cell .fig-column-margin}\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\nTo develop our intuition, we can plot both the accepted samples and the rejected values on the same histogram. Notice two things.\n\n1. First, when combined---looking at stacked the red and blue bars---the histogram is uniform. This is because the proposals are from a uniform distribution. \n1. Second, after removing the rejected values---looking at the blue accepted samples only---the histogram takes on the shape of the target distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbind_rows(\n  data.frame(type = \"Accepted\", values = r$samples),\n  data.frame(type = \"Rejected\", values = r$rejects)\n) |>\n  mutate(type = factor(type, levels = c(\"Rejected\", \"Accepted\"))) |>\n  ggplot(aes(fill = type, x = values)) +\n  geom_histogram(binwidth = 1/20, boundary = 0) + \n  theme_ipsum(base_family = \"Source Sans 3\") +\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\")) + \n    labs(x = expression(pi), y = \"Count\", \n         fill = \"Result\")\n```\n\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nWe can use the samples to compute the summaries of interest, like the posterior mean. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n4/(4 + 10)  # analtical mean.\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2857143\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(r$samples)  # simulation mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.286343\n```\n\n\n:::\n:::\n\n\n\n\n\nThis samples are independent. So while they are more complicated to generate, they work just as well as draws using `rbeta()`.\n\n### A Weird Example\n\nTo see the power of the rejection algorithm, we can come up with a weird prior.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# funky priors on [0,1]\nprior_saw <- function(p, n_teeth = 5) {\n  ((n_teeth*p) %% 1)\n}\n```\n:::\n\n::: {.cell .fig-column-margin}\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# example unnormalized target distribution\n#   bernoulli likelihood times sawtooth prior\n#   rescaled by 10,000 to make values sensible\nf <- function(z) {\n  10000*z^4 * (1 - z)^10 * prior_saw(z) \n}\n```\n:::\n\n::: {.cell .fig-column-margin}\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# perform sampling\nr <- rej(f, 10000, 4)\n```\n:::\n\n\n\n\nWe can create a histogram of the posterior distribution, which has a very unusal shape.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nbind_rows(\n  data.frame(type = \"Accepted\", values = r$samples),\n  data.frame(type = \"Rejected\", values = r$rejects)\n) |>\n  mutate(type = factor(type, levels = c(\"Rejected\", \"Accepted\"))) |>\n  ggplot(aes(fill = type, x = values)) +\n  geom_histogram(binwidth = 1/50, boundary = 0) + \n  theme_ipsum(base_family = \"Source Sans 3\") +\n  scale_fill_manual(values = c(\"#e41a1c\", \"#377eb8\")) + \n    labs(x = expression(pi), y = \"Count\", \n         fill = \"Result\")\n```\n\n::: {.cell-output-display}\n![](02-rejection-sampling_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\nWe can also compute the mean, SD, and 90% equal-tailed credible interval.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(r$samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3120422\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(r$samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1151129\n```\n\n\n:::\n\n```{.r .cell-code}\nquantile(r$samples, probs = c(0.05, 0.95))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       5%       95% \n0.1380533 0.5256502 \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}