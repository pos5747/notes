{
  "hash": "18c77160918d13c87880d20bf3adb188",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Censoring\n\n\n\n\n\n\n\n\n\n\n\n\nFor some outcomes, we certain observations are censored. For the non-censored observations, we know the value exactly, as usual. But for the censored observations, we only know the *interval* that the value lies within.\n\nFor example, we could imagine a sliding scale on a survey that allows respondents to report their exact income within the \\$10k to \\$150k interval, but the endpoints are \"less than \\$10k\" and \"greater than \\$150k.\" For most respondents, we know their income. But for others, we only know that it is less than \\$10k. And for others, we only know it is greater than \\$150k.\n\n\n## Adjusting the individual likelihood\n\nIn uncensored data, the likelihood contribution of an observation $y_i$ is the pdf/pmf evaluated at the observed outcome $L_i(\\theta) = f(y_i; \\theta)$.\n\nFor censored observations, $y_i$ is unknown, so we replace $f(y_i; \\theta)$ with the *probability of the observed region*.\n\n- **Right-censoring at $c$** (we only know $y_i>c$)\n  $$\n  L_i(\\theta) = \\Pr(y_i>c; \\theta) = 1 - F(c; \\theta).\n  $$\n\n- **Left-censoring at $c$** (we only know $y_i<c$)\n  $$\n  L_i(\\theta) = \\Pr(y_i<c_i; \\theta) = F(c; \\theta).\n  $$\n\n- **Interval censoring on $(a,b)$** (we only know $a<y_i<b$)\n  $$\n  L_i(\\theta) = \\Pr(a<y_i<b; \\theta) = F(b; \\theta) - F(a; \\theta).\n  $$\n\n## Combining into the full likelihood\n\nThe full likelihood multiplies pdfs/pmfs for fully observed cases and cumulative probabilities for censored cases.\n\nHere is an example dataset that shows the observed data and their corresponding likelihood contributions.\n\n$$\n\\begin{array}{c c c c c c c c c c c}\n   y_1=2   & ; & y_2>3   & ; & y_3=1   & ; & y_4<1   & ; & y_5 \\in (1,3) & ; & \\cdots \\\\[6pt]\n   f(2)    & \\times & \\Pr(Y>3) & \\times & f(1)   & \\times & \\Pr(Y<1) & \\times & \\Pr(1<Y<3)   & \\times & \\cdots \\\\[6pt]\n   f(2)    & \\times & 1-F(3)   & \\times & f(1)   & \\times & F(1)     & \\times & F(3)-F(1)    & \\times & \\cdots\n\\end{array}\n$$\nThis gives us the likelihood\n\n$$\n\\text{the likelihood: } \nL(\\theta) = f(2) \\cdot [1-F(3)] \\cdot f(1) \\cdot F(1) \\cdot [F(3)-F(1)]\\cdots\n$$\n\nIt becomes more complicated to write the likelihood, because we need a way to flag the observation that are censored and *how* they are censored. But the intuition remains the same.\n\nHere are a couple of ways we might write this likelihood.\n\n### Option 1\n\nFor a observations $i \\in \\{1, 2, ...  n\\}$, let\n\n- $U = \\{i : \\text{uncensored}\\}$ with observed value $y_i$\n- $R = \\{i : \\text{right-censored at } c_i\\}$\n- $L = \\{i : \\text{left-censored at } c_i\\}$\n- $Q = \\{i : \\text{interval-censored on } (a_i,b_i)\\}$\n\nThen we can write the likelihood as\n\n$$\nL(\\theta)\n= \\overbrace{\\prod_{i \\in U} f(y_i \\mid \\theta)}^{\\text{uncensored}}\\; \\cdot\n  \\overbrace{\\prod_{i \\in R} \\{1 - F(c_i \\mid \\theta)\\}}^{\\text{right censored}}\\; \\cdot\n  \\overbrace{\\prod_{i \\in L} F(c_i \\mid \\theta)}^{\\text{left censored}}\\; \\cdot\n  \\overbrace{\\prod_{i \\in Q} \\{F(b_i \\mid \\theta) - F(a_i \\mid \\theta)\\}}^{\\text{interval censored}}.\n$$\n\n### Option 2\n\nEquivalently, we can use the same trick used by the Bernoulli pmf, with indicator variables $u_i, r_i, l_i, q_i \\in \\{0,1\\}$,\n\n$$\nL(\\theta)\n= \\prod_{i=1}^n\n\\Big[\nf(y_i \\mid \\theta)^{u_i}\n\\{1 - F(c_i \\mid \\theta)\\}^{r_i}\nF(c_i \\mid \\theta)^{l_i}\n\\{F(b_i \\mid \\theta) - F(a_i \\mid \\theta)\\}^{q_i}\n\\Big].\n$$\n\n## Example\n\nTo see how censoring can bias estimates and how adjusting the likelihood can fix the bias, let's simulate some fake data with censoring.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed for reproducibility\nset.seed(1)\n\n# simulate latent outcome from usual normal linear model\nn <- 100\nx <- rexp(100, rate = 1/5)\nX <- cbind(1, x) # intercept + one predictor\nbeta <- c(1, 0.5)\nsigma <- 2\nmu <- X %*% beta\ny_star <- rnorm(n, mean = mu, sd = sigma)  # if fully observed\n\n# censor data\n# note: y = c if censored\nc <- 5\nd <- as.integer(y_star > c)  # 1 = censored, 0 = uncensored\ny <- ifelse(d == 1, c, y_star)  # observed outcome (i.e., the censored outcome)\n\n# make data frame\ndata <- data.frame(y, x, y_star, d)\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          y          x    y_star d\n1 1.1262111  3.7759092 1.1262111 0\n2 5.0000000  5.9082139 5.1466250 1\n3 1.6037021  0.7285336 1.6037021 0\n4 0.7851404  0.6989763 0.7851404 0\n5 5.0000000  2.1803431 5.0021484 1\n6 5.0000000 14.4748427 8.6954605 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# verify the amount of censoring\nmean(d)  # proportion of data that are censored\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.28\n```\n\n\n:::\n:::\n\n\n\n\n\nThe plot below shows the structure of censored data. The blue points are censored. We do not know the values, we only know they fall *above* the threshold (i.e., are right-censored).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# load packages\nlibrary(hrbrthemes)\nlibrary(showtext)\n\n# download and register Source Sans 3 from google fonts\nfont_add_google(\"Source Sans 3\", family = \"Source Sans 3\")\nshowtext_auto() \n\n# create factor\ngg_data <- data |>\n  mutate(d_lbl = factor(d, levels = c(0, 1),                   # original coding of d\n      labels = c(\"Not Censored\", \"Censored\")))\n\n# make plot\nggplot(gg_data, aes(x = x, y = y_star, color = d_lbl)) +\n  geom_hline(yintercept = c) + \n  annotate(\"label\", x = 17.5, y = c, label = \"Censoring Threshold\", size = 3) + \n  geom_segment(\n    data = filter(gg_data, d == 1),\n    aes(x = x, xend = x, y = y_star, yend = y),\n    arrow = arrow(length = unit(0.15, \"cm\")),\n    inherit.aes = FALSE,\n    color = \"grey50\"\n  ) +\n  geom_point() +\n  # arrows from y_star to y, only for censored cases\n  labs(y = \"Outcome\", color = \"Censoring\") +\n  labs(title = \"An Example Censored Dataset\",\n       subtitle = \"Values Above 5 Are Right-Censored\",\n       x = \"Hypothetical Explanatory Variable\",\n       y = \"Latent Outcome Variable\", \n       color = \"Type\") +\n  theme_ipsum(base_family = \"Source Sans 3\") + \n  scale_color_manual(values = c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\"))\n```\n\n::: {.cell-output-display}\n![](03-censoring_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Ignore censoring\n\nOne approach would be to *ignore* the censoring. Remember that, by convention, the censored values have the cutoff value. In the fake data, I chose `c <- 5` for the cutoff. Ignoring the censoring, we estimate a slope of 0.23 give-or-take 0.03. This is much too low.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit ignoring censoring\nfit_lm <- lm(y ~ x)\narm::display(fit_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm(formula = y ~ x)\n            coef.est coef.se\n(Intercept) 1.68     0.24   \nx           0.23     0.03   \n---\nn = 100, k = 2\nresidual sd = 1.59, R-Squared = 0.32\n```\n\n\n:::\n:::\n\n\n\n\n\n### Model censoring\n\nAlternatively, we could model the censoring. After adjusting the likelihood function as described above, the usual recipe works as expected (ML to estimate parameters → Hessian for their variances → invariance property for quantities of interest → delta method for their variances).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log-likelihood w/ censoring\nnormal_rightcens_ll <- function(theta, y, X, c, d) {\n  # tidy up parameters\n  k <- ncol(X)\n  beta <- theta[1:k]\n  sigma <- theta[k + 1]\n  mu <- X %*% beta\n\n  # uncensored\n  ll_unc <- dnorm(y, mean = mu, sd = sigma, log = TRUE)\n\n  # right-censored at c: log Pr(Y > c)\n  ll_cens <- pnorm(c, mean = mu, sd = sigma, lower.tail = FALSE, log.p = TRUE)\n\n  # multiply\n  ll <- sum((1 - d) * ll_unc + d * ll_cens)\n  return(ll)\n}\n\n# optim\ntheta_start <- c(rep(0, ncol(X)), 2)\nest <- optim(\n  par     = theta_start,\n  fn      = normal_rightcens_ll,\n  y       = y,\n  X       = X,\n  c       = c,\n  d       = d,\n  method  = \"BFGS\",\n  control = list(fnscale = -1),\n  hessian = TRUE\n)\n\n# point estimates\nest$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8941892 0.5134936 1.8645425\n```\n\n\n:::\n:::\n\n\n\n\n\nThe figure below shows the estimated intercept, slope, and error SD from the two approaches, compared to the truth. This figure shows that OLS is underestimating the slope, as you might have guessed from the scatterplot above.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03-censoring_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## More Reading\n\nThe normal model with censoring that I used above to illustrate the idea is sometimes called the \"tobit model.\" @king1998 [pp. 208-210] provides a brief discussion of censoring. @long1997 [pp. 187-215] provides a chapter-length description of censoring in the context of the tobit model. @boxsteffensmeier2004 [pp. 15-19] discuss censoring in the context of duration models, where is is seemingly omnipresent.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}