{
  "hash": "43880c7576c1ed8577bdb3fe644cd3fb",
  "result": {
    "engine": "knitr",
    "markdown": "# Sampling Distribution\n\n\n\n\n\n\n\n\n\n\n\nBefore we hop into new material, I want to rewind and review some *old*, but foundational, ideas.\n\nWhat's the most important concept in statistical inference? It could be **the sampling distribution**. For effect, let me back off the hedge.\n\n> The most important concept in statistical inference is the **sampling distribution**.\n\nTo define a sampling distribution, you must imagine repeating a study over and over. If each study has a random component, then the estimate across studies will vary randomly. A \"random component\" might be: random sampling, random assignment to treatment and control, or an imagined stochastic component like \"the errors are like draws from a normal distribution.\" The distribution of the estimates across the many imagined studies is called the sampling distribution.\n\nTo build our intuition, let's look at an example.\n\n## Example: The Toothpaste Cap Problem\n\nFor a sample of 150 tosses, we recognize that the ML estimate $\\hat{\\pi} = \\text{fraction of tops among the tosses}$ does not (usually) exactly equal the parameter $\\pi$. Instead, the particular $\\hat{\\pi}$ that the study produces is a draw from a distribution.\n\nLet's illustrate that with a simulation. For these simulations, I suppose that we toss the toothpaste cap 150 times and the chance of a top is 5%.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sims <- 10  # number of repeated studies\nml_est <- numeric(n_sims)  # a container for the estimates\nfor (i in 1:n_sims) {\n  y <- rbinom(150, size = 1, prob = 0.05)  # chance of a top is 5%\n  ml_est[i] <- mean(y)  # observed fraction of tops (i.e., 1s) in y\n}\nprint(ml_est, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.060 0.060 0.067 0.073 0.040 0.060 0.033 0.047 0.073 0.073\n```\n\n\n:::\n:::\n\n\n\n\n\nAs you can see, the ML estimates vary to from sample to sample---different data sets produce different ML estimates.\n\nIf we repeat the simulations a large number of times, we can use a histogram to visualize the sampling distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sims <- 10000\nml_est <- numeric(n_sims)  # container to store results\nfor (i in 1:n_sims) {\n  y <- rbinom(150, size = 1, prob = 0.05)\n  ml_est[i] <- mean(y)\n}\n\n# create a histogram of the sampling distribution\ngg_data <- data.frame(ml_est = ml_est)\nggplot(gg_data, aes(x = ml_est)) + \n  geom_histogram(\n    # the settings below make the bars appear \"smooth\" for this problem\n    center = 8/150, # centered at an observable value.\n    binwidth = 1/150, # one bar per observable value\n    # color to distinguish bars\n    fill = NA, color = \"grey20\")\n```\n\n::: {.cell-output-display}\n![](01-sampling-distribution_files/figure-html/unnamed-chunk-3-1.png){width=384}\n:::\n:::\n\n\n\n\n\nWe can also work with the sampling distribution analytically.\n\nThere are three features that we care about.\n\n1.  What is the **mean** of the sampling distribution?\n2.  What is the **SD** of the sampling distribution?\n3.  What is the **shape** of the sampling distribution?\n\nFirst, we can find the mean. For the ML estimator in the toothpaste cap problem, this is straightforward.\n\n$$\n\\small\n\\begin{align*}\nE(\\hat{\\pi}) \n&= E\\left( \\frac{1}{150} \\sum_{i=1}^{150} y_i \\right) \n&& \\text{definition of } \\hat{\\pi} \\\\\n&= \\frac{1}{150} \\sum_{i=1}^{150} E(y_i) \n&& \\text{linearity of expectation; } E(cX) = cE(X) \\\\\n&= \\frac{1}{150} \\cdot 150 \\cdot E(y_i) \n&& \\text{each } y_i \\text{ has same expectation because they are iid} \\\\\n&= E(y_i) \n&& \\text{simplify constants; } \\frac{1}{150} \\cdot 150 = 1\\\\\n&= 0.05 \n&& \\text{each } y_i \\text{ is a Bernoulli random variable with mean } \\pi = 0.05\n\\end{align*}\n$$\n\nSecond, we can work out the variance and SD. Again, for this problem, this is easy.\n\n$$\n\\small\n\\begin{align*}\n\\text{Var}(\\hat{\\pi}) \n&= \\text{Var}\\left( \\frac{1}{150} \\sum_{i=1}^{150} y_i \\right) \n&& \\text{definition of } \\hat{\\pi} \\\\\n&= \\frac{1}{150^2} \\sum_{i=1}^{150} \\text{Var}(y_i) \n&& \\text{variance rule for sum of independent variables; } \\text{Var}(cX) = c^2 \\text{Var}(X) \\\\\n&= \\frac{1}{150^2} \\cdot 150 \\cdot \\text{Var}(y_i) \n&& \\text{each } y_i \\text{ has same variance because they are iid} \\\\\n&= \\frac{1}{150} \\cdot \\text{Var}(y_i) \n&& \\text{simplify constants; } \\frac{1}{150^2} \\cdot 150 = \\frac{1}{150} \\\\\n&= \\frac{1}{150} \\cdot 0.05 \\cdot (1 - 0.05) \n&& \\text{each } y_i \\text{ is Bernoulli w/ variance } \\pi(1 - \\pi) = 0.05\\cdot (1 - 0.05)\\\\\n\n&= \\frac{0.05 \\cdot 0.95}{150} \n&& \\text{simplify product}\n\\end{align*}\n$$\n\nWe can compute this fraction in R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# variance\n(0.05 * 0.95) / 150\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0003166667\n```\n\n\n:::\n:::\n\n\n\n\n\nThe SD is simply the square root of the variance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# SD\nsqrt((0.05 * 0.95) / 150)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01779513\n```\n\n\n:::\n:::\n\n\n\n\n\nWe can compare these analytical results to our simulations above.[^01-sampling-distribution-1]\n\n[^01-sampling-distribution-1]: In this case, the analytical results are exactly correct and the simulations are (arbitrarily precise) approximations. In some cases, analytical results are approximations and simulations are arbitrarily precise approximations, making the simulations more reliable.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mean of sampling distribution\nmean(ml_est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04995667\n```\n\n\n:::\n\n```{.r .cell-code}\n# sd of sampling distribution\nsd(ml_est)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0175467\n```\n\n\n:::\n:::\n\n\n\n\n\nLastly, we can approximate the shape of the sampling distribution. Since we are summing many (i.e., 150) iid random variables (i.e., Bernoulli trials), the central limit theorem[^central-limit-theorem] suggests that this histogram will be approximately normal.\n\n[^central-limit-theorem]: Suppose $y_1, y_2, \\dots, y_N$ are iid random variables with finite mean $\\mu$ and finite variance $\\sigma^2$ Then, as $N \\to \\infty$, $\\sqrt{N}\\cdot \\left[ \\frac{ \\operatorname{avg}(y_n) - \\mu}{\\sigma} \\right]$ converges in distribution to the standard normal distribution, where . Said less formally, the distribution of the sample average approaches a normal distribution as the sample size grows large, regardless of the distribution of $y_i$ (but finite mean and variance are required).\n\n\nThe figure below shows the sampling distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(gg_data, aes(x = ml_est)) + \n  geom_histogram(\n    aes(y = after_stat(density)),  # use density scale for y-axis\n    center = 8/150, \n    binwidth = 1/150, \n    fill = \"grey80\") + \n  # add normal curve to histogram\n  stat_function(fun = dnorm, args = list(mean = mean(gg_data$ml_est), \n                                         sd = sd(gg_data$ml_est)))\n```\n\n::: {.cell-output-display}\n![](01-sampling-distribution_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Example: Normal Model\n\nAs a second example, let's review the familiar normal model.\n\nSuppose we collect a sample of size $N$ from a normal distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$. We estimate the mean using the sample average so that $\\hat{\\mu} = \\operatorname{avg}(y) = \\frac{1}{n} \\sum_{i=1}^N y_i$.[^01-sampling-distribution-2]\n\n[^01-sampling-distribution-2]: This is the ML estimator as well! But it's also motivated in other (sometimes even better!) ways.\n\nAs is well-known, the expected value of $\\hat{\\mu}$ is $\\mu$.\n\n$$\n\\small\n\\begin{align*}\nE \\left[ \\operatorname{avg}(y) \\right] \n&= E\\left( \\frac{1}{N} \\sum_{i=1}^N y_i \\right)\n&& \\text{definition of } \\operatorname{avg}(y) \\\\\n&= \\frac{1}{N} \\sum_{i=1}^N E(y_i) \n&& \\text{linearity of expectation} \\\\\n&= \\frac{1}{N} \\cdot N \\cdot E(y_i) \n&& y_i \\text{ are iid} \\\\\n&= E(y_i) = \\mu \n&& \\text{simplify constants}\n\\end{align*}\n$$\n\nAnd the variance has a familiar form.\n\n$$\n\\begin{align*}\n\\text{Var}[\\operatorname{avg}(y)]\n&= \\text{Var}\\left( \\frac{1}{N} \\sum_{i=1}^N y_i \\right)\n&& \\text{definition of } \\operatorname{avg}(y) \\\\\n&= \\frac{1}{N^2} \\sum_{i=1}^N \\text{Var}(y_i)\n&& \\text{variance of independent sum} \\\\\n&= \\frac{1}{N^2} \\cdot N \\cdot \\sigma^2 \n&& y_i \\text{ are normal with variance } \\sigma^2 \\text{ and are iid} \\\\\n&= \\frac{\\sigma^2}{N} \n&& \\text{simplify constants}\n\\end{align*}\n$$\n\nThe SD, then, is a familiar quantity $\\frac{\\sigma}{\\sqrt{N}}$.\n\nLastly, the standardized sample mean follows a $t$ distribution, so that $\\sqrt{N} \\cdot \\left[ \\frac{\\operatorname{avg}(y) - \\mu}{\\operatorname{SD}(y)} \\right]$ follows a *t* distribution in $N - 1$ degrees of freedom. This sampling distribution is the foundation for the one-sample $t$-test.\n\n## Bias\n\nBut how do we use the sampling distribution? In two ways.\n\n1.  *To evaluate estimators.* We think that the sampling distributions of some estimators are preferable to the sampling distributions of other estimators.\n2.  *To create hypothesis tests and confidence intervals.* We recognize that estimates are in fact *just* estimates, so our claims should reflect the uncertainty in those estimates.\n\nOne way to evaluate estimators is to assess their bias.\n\n::: {#def-bias}\n## Bias\n\n-   $\\hat{\\theta}$ is **biased** if $E(\\hat{\\theta}) \\neq \\theta$.\n-   $\\hat{\\theta}$ is **unbiased** if $E(\\hat{\\theta}) = \\theta$.\n-   The **bias** of $\\hat{\\theta}$ is $E(\\hat{\\theta}) - \\theta$.\n:::\n\nWe tend to prefer unbiased to biased estimators and estimators with less bias to estimators with more bias.\n\nImportantly, **ML estimators are not necessarily unbiased**. Of the models we see in this course, *most* are biased.\n\n### Example: Bernoulli Distribution\n\nAbove, we saw that that for the toothpaste cap problem with 150 tosses and a 5% chance of a top, the fraction of tosses in the sample has an expected value of 5%. This means that the ML estimator is unbiased in that situation.\n\nThis is true more generally as well. You could show that a similar result holds for any number of tosses and any chance of a top.\n\nWe can use a Monte Carlo simulation to check this analytical result.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nn_mc_sims <- 100000\npi_hat <- numeric(n_mc_sims)\nfor (i in 1:n_mc_sims) {\n  y <- rbinom(150, size = 1, prob = 0.05)\n  pi_hat[i] <- mean(y)\n}\n\n# expected value of pi-hat\nmean(pi_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05006227\n```\n\n\n:::\n\n```{.r .cell-code}\n# estimated monte carlo error\nsd(pi_hat)/sqrt(n_mc_sims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.631271e-05\n```\n\n\n:::\n:::\n\n\n\n\n\nBut notice that the property of unbiasedness does not follow the estimate through transformation [@rainey2017]. Because the sample is relatively large in this case (150 tosses), the bias is small, but detectable with 100,000 Monte Carlo simulations.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nodds_hat <- pi_hat/(1 - pi_hat)\n\n# actual odds\n0.05/(1 - 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05263158\n```\n\n\n:::\n\n```{.r .cell-code}\n# expected value of odds-hat\nmean(odds_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05307323\n```\n\n\n:::\n\n```{.r .cell-code}\n# estimated monte carlo error\nsd(odds_hat)/sqrt(n_mc_sims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.288517e-05\n```\n\n\n:::\n\n```{.r .cell-code}\n# the z-statistic testing that mean of simulated odds = actual odds\n(mean(odds_hat) - 0.05/0.95)/(sd(odds_hat)/sqrt(n_mc_sims))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.023072\n```\n\n\n:::\n:::\n\n\n\n\n\n### Example: Exponential Distribution\n\nSuppose we sample from an exponential distribution with unknown rate $\\lambda$. The ML estimator of the rate $\\lambda$ is $\\hat{\\lambda}^{ML} = \\frac{1}{\\text{avg}(x)}$.\n\nTo compute bias, we compute the expectation $E \\left( \\hat{\\lambda}^{ML} \\right) = E\\left( \\frac{1}{\\text{avg}(x)} \\right)$.\n\nThere's a trick. Notice that $\\frac{1}{x}$ is convex for $x > 0$.^[A function is **convex** if the function never curves above the line between any two of its points.] Jensen's Inequality states that if $g()$ is a convex function and $X$ is a random variable, then $g(E(X)) \\leq E(g(X))$; moving an expectation inside a convex function decreases the value. Applying Jensen's inequality, we know that $E\\left( \\frac{1}{\\text{avg}(x)} \\right) > \\frac{1}{E(\\text{avg}(x))}$.\n\nBut $\\text{avg}(x)$ is the sample mean of iid exponential random variables, so $E[\\text{avg}(x)] = \\frac{1}{\\lambda}$. It must be, then, that $E \\left( \\hat{\\lambda}^{ML} \\right) = E\\left( \\frac{1}{\\text{avg}(x)} \\right) > \\lambda$. Thus, $\\hat{\\lambda}^{ML}$ overestimates the true rate on average and is a biased estimator.\n\nWe can confirm this result with a Monte Carlo simulation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 2.0        # the true rate\nsample_size <- 5     # small sample size for large bias\nn_mc_sims <- 100000  # number of monte carlo simulations\n\n# do monte carlo simulations\nlambda_hat <- numeric(n_mc_sims)  # container\nfor (i in 1:n_mc_sims) {\n  x <- rexp(sample_size, rate = lambda)\n  lambda_hat[i] <- 1 / mean(x)  # ml estimate\n}\n\n# expected value of lambda-hat\nmean(lambda_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.500481\n```\n\n\n:::\n\n```{.r .cell-code}\n# estimated monte carlo error\nsd(lambda_hat) / sqrt(n_mc_sims)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.004612098\n```\n\n\n:::\n:::\n\n\n\n\n\nThe average of the simulated values of $\\hat{\\lambda}^{ML} \\approx 2.5$ is larger than the true value $\\lambda = 2.0$, just as the result above shows.\n\n## Standard Error\n\nSecond, we can use the sampling distribution (or estimates of features of the sampling distribution) to create hypothesis tests or confidence intervals. \n\nFor many problems, we know that we can create a 90% confidence interval for $\\theta$ using $[\\hat{\\theta} - 1.64 \\cdot \\hat{\\text{SE}}(\\hat{\\theta}), \\hat{\\theta} + 1.64 \\cdot \\hat{\\text{SE}}(\\hat{\\theta})]$, where $\\hat{\\text{SE}}(\\hat{\\theta})$ is the *estimate* of the standard error of the sampling distribution.\n\nSimilarly, we can compute the *p*-value for the test of the one-sided hypothesis that $\\theta > 0$ using $1 - \\Phi\\left( \\frac{\\hat{\\theta}}{\\hat{\\text{SE}}(\\hat{\\theta})} \\right)$, where $\\Phi(\\cdot)$ is the standard normal CDF.\n\n::: {#def-se}\n## Standard Error (SE)\n\nThe **standard error** is the standard deviation of the sampling distribution.\n:::\n\nIn practice, we'll need to *estimate* the SE using a single, observed data set. But before we worry about *estimating* the SE, let's focus on understanding the *actual* SE using a few examples. For a hypothetical model, we can work out the actual SE directly or use a simulation.\n\n### Example: Bernoulli Model\n\nSuppose $n$ samples $y_1, \\dots, y_n$ from a Bernoulli distribution with parameter $\\pi$. Let $\\hat{\\pi} = \\text{avg}(y)$. We can compute the SE of $\\hat{\\pi}$ analytically, but we work with the variance first, since that's easier.\n\n$$\n\\small\n\\begin{align*}\n\\text{Var}(\\hat{\\pi}) \n&= \\text{Var}\\left( \\frac{1}{n} \\sum_{i=1}^n y_i \\right) \\\\\n&= \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(y_i) \\\\\n&= \\frac{1}{n^2} \\cdot n \\cdot \\pi(1 - \\pi) \\\\\n&= \\frac{\\pi(1 - \\pi)}{n}\n\\end{align*}\n$$\n\nThe standard error is the square root of the variance, so that $\\text{SE}(\\hat{\\pi}) = \\sqrt{ \\frac{\\pi(1 - \\pi)}{n} }$.^[To obtain a commonly-used formula and preview a later result, we can plug in the estimator $\\hat{\\pi}$ into $\\text{SE}(\\hat{\\pi}) = \\sqrt{ \\frac{\\pi(1 - \\pi)}{n} }$ to obtain an *estimate* of the SE, so that $\\hat{\\text{SE}}(\\hat{\\pi}) = \\sqrt{ \\frac{\\hat{\\pi}(1 - \\hat{\\pi})}{n} }$.]\n\n### Example: Exponential Model\n\nSuppose we have $n$ samples $y_1, \\dots, y_n$ from an exponential distribution and estimate the rate $\\lambda$ with $\\hat{\\lambda}^{ML} = \\frac{1}{\\text{avg}(y)}$. It isn't easy to work out the variance analytically, but a simulation works just fine for particular values of $n$ and $\\lambda$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 2.0\nn <- 30\nn_mc_sims <- 10000\n\nlambda_hat <- numeric(n_mc_sims)\nfor (i in 1:n_mc_sims) {\n  x <- rexp(n, rate = lambda)\n  lambda_hat[i] <- 1 / mean(x)\n}\n\n# simulated SE\nsd(lambda_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3955538\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}