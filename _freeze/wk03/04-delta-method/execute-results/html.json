{
  "hash": "033f52d6e4c80a9a8dd3e6d663271313",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Delta Method\"\ncache: true\n---\n\n\n\n\n\n\n\nThe invariance property tells us that if $\\hat{\\theta}$ is the ML estimate of $\\theta$, then $\\hat\\tau = \\tau(\\hat{\\theta})$ is the ML estimate of $\\tau(\\theta)$. But this raises a new question: how do we obtain the *standard error* of $\\hat\\tau = \\tau(\\hat{\\theta})$? We have estimated the SE of $\\hat{\\theta}$, but how do we convert that to an estimate of the SE of $\\hat\\tau = \\tau(\\hat{\\theta})$?\n\n**Key Idea:** The delta method uses a Taylor expansion to approximate the variance of $\\hat\\tau = \\tau(\\hat{\\theta})$ using the variance of $\\hat{\\theta}$.\n\n::: {.aside}\n\nA Taylor expansion uses derivatives to approximate a function near a particular point. For a one-variable function $\\tau(\\theta)$ expanded around $\\hat{\\theta}$,  \n\n$$\n\\small\n\\tau(\\theta) \\;\\approx\\; \\tau(\\hat{\\theta}) + \\tau'(\\hat{\\theta})(\\theta - \\hat{\\theta}) + \\tfrac{1}{2}\\tau''(\\hat{\\theta})(\\theta - \\hat{\\theta})^2 + \\cdots\n$$  \n\nWhen $\\theta$ is close to $\\hat{\\theta}$, the higher-order terms are small. Dropping them gives the linear approximation \n$$\n\\small\n\\tau(\\theta) \\;\\approx\\; \\tau(\\hat{\\theta}) + \\tau'(\\hat{\\theta})(\\theta - \\hat{\\theta}).\n$$\n\nThus, small deviations of $\\theta$ from $\\hat{\\theta}$ are scaled by the slope $\\tau'(\\hat{\\theta})$. The delta method captures this scaling \n$$\n\\small \\operatorname{Var}\\!\\big(\\tau(\\hat{\\theta})\\big) \\;\\approx\\; \\big(\\tau'(\\hat{\\theta})\\big)^2 \\, \\operatorname{Var}(\\hat{\\theta}).\n$$\nThe square comes from the rule\n$$\n\\small\n\\operatorname{Var}(cX) = c^2 \\operatorname{Var}(X).\n$$ \n\nIntuitively, you can think of a Taylor expansion as \"zooming in\" until the curve looks straight. Near $\\theta$, small wiggles in $\\hat{\\theta}$ are expanded or shrunk by $\\tau(\\cdot)$ depending on the slope. The delta method adjusts for this shrinking or expanding.\n\n:::\n\n\nIn the one-parameter case, $\\widehat{\\operatorname{Var}}[\\tau(\\hat{\\theta})] \\approx \\Big(\\tau'(\\hat{\\theta})\\Big)^2 \\cdot \\widehat{\\operatorname{Var}}(\\hat{\\theta})$. \n\nIn the multi-parameter case, if $\\hat{\\theta}$ is a vector and $\\tau(\\cdot)$ is a function, then \n$$\n\\widehat{\\operatorname{Var}}(\\hat\\tau) = \\widehat{\\operatorname{Var}}[\\tau(\\hat{\\theta})] \\approx \\nabla \\tau(\\hat{\\theta})^\\top \\cdot \\widehat{\\operatorname{Var}}(\\hat{\\theta}) \\cdot \\nabla \\tau(\\hat{\\theta}), \n$$\nwhere $\\nabla \\tau(\\hat{\\theta})$ is the gradient of $\\tau(\\theta)$ with respect to $\\theta$ evaluated at $\\hat\\theta$.\n\nFrom there, we can create Wald confidence intervals in the usual way. Recall that $\\widehat{\\text{SE}}(\\hat{\\tau}) = \\sqrt{\\widehat{\\operatorname{Var}}(\\hat\\tau)}$.\n\n$$\n\\begin{align*}\n 90\\%~\\text{C.I.}  &= \\hat{\\tau} \\pm 1.64 \\cdot \\widehat{\\text{SE}}(\\hat{\\tau})\\\\\n 95\\%~\\text{C.I.}  &= \\hat{\\tau} \\pm 1.96 \\cdot \\widehat{\\text{SE}}(\\hat{\\tau})\n\\end{align*}\n$$\n\n\n## Bernoulli: From $\\pi$ to odds\n\nSuppose a Bernoulli model of a binary outcome $y$. The ML estimate of $\\pi$ is $\\hat{\\pi} = \\text{avg}(y)$. Using the Fisher information, the variance of $\\hat{\\pi}$ is $\\widehat{\\operatorname{Var}}(\\hat{\\pi}) = \\dfrac{\\hat{\\pi}(1 - \\hat{\\pi})}{N}$.\n\nSuppose we want to transform $\\pi$ to the odds, where $\\text{odds} = \\tau(\\pi) = \\dfrac{\\pi}{1 - \\pi}$. $\\tau(\\pi) = \\dfrac{\\pi}{1 - \\pi}$. We use the invariance property to obtain an ML estimate $\\widehat{\\text{odds}} = \\dfrac{\\hat\\pi}{1 - \\hat\\pi}$.\n\nTo estimate the variance of $\\widehat{\\text{odds}}$, we need to use the delta method.\n\nFirst, find the first derivative of $\\tau(\\pi)$. $\\tau'(\\pi) = \\dfrac{1}{(1 - \\pi)^2}$.\n\nPlugging in, we have\n\n$$\n\\begin{aligned}\n\\widehat{\\operatorname{Var}}(\\widehat{\\text{odds}})\n  &\\approx \\left(\\dfrac{1}{(1 - \\hat{\\pi})^2}\\right)^2 \\cdot \\dfrac{\\hat{\\pi}(1 - \\hat{\\pi})}{N}\\\\[6pt]\n  &= \\dfrac{1}{(1 - \\hat{\\pi})^4} \\cdot \\dfrac{\\hat{\\pi}(1 - \\hat{\\pi})}{N}\\\\[6pt]\n  &= \\dfrac{\\hat{\\pi}(1 - \\hat{\\pi})}{N(1 - \\hat{\\pi})^4}\\\\[6pt]\n  &= \\dfrac{\\hat{\\pi}}{N(1 - \\hat{\\pi})^3}.\n\\end{aligned}\n$$\n\nAnd the standard error is\n\n$$\n\\widehat{\\text{SE}}(\\widehat{\\text{odds}}) \\;\\approx\\; \\sqrt{\\dfrac{\\hat{\\pi}}{N(1 - \\hat{\\pi})^3}}.\n$$\n\nAssuming a data set with 150 trials and 8 successes, we have the following estimates.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of trials and successes\nn <- 150\ny <- 8\n\n# estimate pi\npi_hat <- y / n\npi_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05333333\n```\n\n\n:::\n\n```{.r .cell-code}\n# variance and SE estimate for pi_hat\nvar_pi_hat <- pi_hat * (1 - pi_hat) / n\nse_pi_hat  <- sqrt(var_pi_hat)\nse_pi_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01834646\n```\n\n\n:::\n\n```{.r .cell-code}\n# transform to estimate odds\nodds_hat <- pi_hat / (1 - pi_hat)\nodds_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05633803\n```\n\n\n:::\n\n```{.r .cell-code}\n# variance and SE for odds_hat (delta method)\nvar_odds_hat <- pi_hat / (n * (1 - pi_hat)^3)\nse_odds_hat  <- sqrt(var_odds_hat)\nse_odds_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0204719\n```\n\n\n:::\n:::\n\n\n\n\n\n\n---\n\n## Poisson: From $\\lambda$ to SD\n\nSuppose a Poisson model of a count variable $y$. The ML estimate is $\\hat{\\lambda} = \\text{avg}(y)$. We can use the information matrix to estimate the variance $\\widehat{\\operatorname{Var}}(\\hat{\\lambda}) = \\dfrac{\\hat{\\lambda}}{N}$.\n\nSuppose we want to estimate the standard deviation of the Poisson distribution, which is $\\sigma = \\sqrt{\\lambda}$. Then $\\sigma = \\tau(\\lambda) = \\sqrt{\\lambda}$ and $\\tau'(\\lambda) = \\dfrac{1}{2\\sqrt{\\lambda}}$. Then, by the delta method, $\\widehat{\\operatorname{Var}}(\\hat{\\sigma}) \\approx \\left(\\dfrac{1}{2\\sqrt{\\hat{\\lambda}}}\\right)^2 \\cdot \\dfrac{\\hat{\\lambda}}{N}$.\n\nSimplifying, we have $\\widehat{\\operatorname{Var}}(\\hat{\\sigma}) \\approx \\dfrac{1}{4N}$ and $\\widehat{\\text{SE}}(\\hat{\\sigma}) \\;\\approx\\; \\dfrac{1}{2\\sqrt{N}}$.\n\n*Notice:* This result does not depend on $\\hat{\\lambda}$; this is perhaps unexpected. We can check our work with a quick simulation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fix sample size, vary lambda\nn <- 100\nlambda_vals <- c(2, 10, 50)\n\n# store results\nresults <- data.frame(lambda = lambda_vals, se_sigma_mc = NA)\n\nfor (j in 1:length(lambda_vals)) {\n  lambda_j <- lambda_vals[j]  # current value of lambda\n  sigma_hats <- numeric(10000)   # container for 10,000 estimates\n  \n  for (i in 1:10000) {\n    y <- rpois(n, lambda_j)\n    lambda_hat <- mean(y)\n    sigma_hats[i] <- sqrt(lambda_hat)\n  }\n  \n  results$se_sigma_mc[j] <- sd(sigma_hats)\n}\n\n1/(2*sqrt(n))  # delta method SE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05\n```\n\n\n:::\n\n```{.r .cell-code}\nresults  # monte carlo SE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  lambda se_sigma_mc\n1      2  0.04981397\n2     10  0.05012129\n3     50  0.05006480\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Beta Example: From $(\\alpha, \\beta)$ to $\\mu$\n\nSuppose a beta model of a continuous variable $y$ that lies strictly between zero and one. We used `optim()` to estimate the parameters and their variances. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log-likelihood function (using dbeta!)\nbeta_ll_fn <- function(theta, y) { \n  alpha <- theta[1] \n  beta  <- theta[2] \n  ll <- sum(dbeta(y, shape1 = alpha, shape2 = beta, log = TRUE))\n  return(ll)\n}\n\n# function to fit beta model \nest_beta <- function(y) {\n  # use optim; compute hessian\n  est <- optim(\n    par     = c(2, 2),  # decent starting values for the problem below\n    fn      = beta_ll_fn,\n    y       = y,\n    control = list(fnscale = -1),  \n    method  = \"BFGS\",\n    hessian = TRUE            \n  ) \n  \n  # compute an estimate of covariance matrix (slowly, this first time)\n  info_obs <- -est$hessian  # notice negative sign\n  var_hat  <- solve(info_obs) \n  \n  # check convergence; print warning if needed\n  if (est$convergence != 0) print(\"Model did not converge!\")\n  \n  # return list of elements\n  res <- list(theta_hat = est$par, \n              var_hat   = var_hat) \n  return(res)\n}\n\n# load packages\nlibrary(tidyverse)\nlibrary(Lahman)  # data from Lahman's baseball database\n\n# create data frame with batting average\nbstats <- battingStats() |> \n  filter(yearID == 2023) |>  # data from 2023\n  filter(AB >= 100) |>  # players with at least 100 at-bats\n  select(player_id = playerID, batting_average = BA) |>\n  arrange(-batting_average) |>\n  na.omit() |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 457\nColumns: 2\n$ player_id       <chr> \"arraelu01\", \"acunaro01\", \"freemfr01\", \"diazya01\", \"se…\n$ batting_average <dbl> 0.354, 0.337, 0.331, 0.330, 0.327, 0.319, 0.316, 0.312…\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# estimate beta model using the batting average data\nfit <- est_beta(bstats$batting_average)\nfit$theta_hat  # parameter estimates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  37.07655 114.92550\n```\n\n\n:::\n\n```{.r .cell-code}\nfit$var_hat  # covariance matrix estimates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]     [,2]\n[1,]  5.964783 18.40870\n[2,] 18.408705 57.83667\n```\n\n\n:::\n:::\n\n\n\n\n\nBut the $\\alpha$ and $\\beta$ parameters are not easy to interpret. Suppose we want the mean $\\mu = \\dfrac{\\alpha}{\\alpha + \\beta}$. We can use the invariance property.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# while it's not as descriptive, the formulas for the mean\n#   and variance as a function of alpha and beta are quite \n#   long, so I use a and b for compactness rather than the\n#   more descriptive a_hat or alpha_hat variants.\na <- fit$theta_hat[1]  # alpha_hat\nb <- fit$theta_hat[2] # beta_hat\n\n# mu_hat via invariance property\nmu_hat <- a/(a + b)  \nmu_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2439214\n```\n\n\n:::\n:::\n\n\n\n\n\nBut we also might like to estimate the SE for $\\hat\\mu$. We can do this with the delta method. Recall that the delta method is $\\widehat{\\operatorname{Var}}[\\tau(\\hat{\\theta})] \\approx \\nabla \\tau(\\hat{\\theta})^\\top \\cdot \\widehat{\\operatorname{Var}}(\\hat{\\theta}) \\cdot \\nabla \\tau(\\hat{\\theta})$. This has two parts: the covariance matrix $\\widehat{\\operatorname{Var}}(\\hat{\\theta})$ and gradient $\\nabla \\tau(\\hat{\\theta})$.\n\n### The covariance matrix $\\widehat{\\operatorname{Var}}(\\hat{\\theta})$\n\nIn this case, $\\widehat{\\operatorname{Var}}(\\hat{\\theta}) =\\widehat{\\operatorname{Var}}(\\hat{\\alpha}, \\hat{\\beta})$ is computed by our `est_beta()` function numerically, so this part is ready to go\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$var_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]     [,2]\n[1,]  5.964783 18.40870\n[2,] 18.408705 57.83667\n```\n\n\n:::\n:::\n\n\n\n\n\n### The gradient $\\nabla \\tau(\\hat{\\theta})$\n\nThe gradient of $\\mu = \\tau(\\alpha, \\beta) = \\frac{\\alpha}{\\alpha + \\beta}$ is:\n\n$$\n\\nabla \\tau(\\alpha,\\beta) =\n\\begin{bmatrix}\n\\dfrac{\\partial \\tau}{\\partial \\alpha}\\\\[6pt]\n\\dfrac{\\partial \\tau}{\\partial \\beta}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\dfrac{\\beta}{(\\alpha+\\beta)^2}\\\\[6pt]\n-\\dfrac{\\alpha}{(\\alpha+\\beta)^2}\n\\end{bmatrix}\n$$\n\nPlugging in $\\hat\\alpha$ and $\\hat\\beta$, we have \n\n$$\n\\nabla \\tau(\\hat\\alpha, \\hat\\beta) =\\begin{bmatrix}\\dfrac{\\hat\\beta}{(\\hat\\alpha+\\hat\\beta)^2}\\\\-\\dfrac{\\hat\\alpha}{(\\hat\\alpha+\\hat\\beta)^2}\\end{bmatrix} = \\begin{bmatrix}\\dfrac{114.93}{(37.08+114.93)^2}\\\\[6pt]-\\dfrac{37.08}{(37.08+114.93)^2}\\end{bmatrix} = \\begin{bmatrix} 0.0050 \\\\ -0.0016\\end{bmatrix}\n$$\n\n### The matrix algebra\n\n#### Using R\n\nThis is a simple calculation with R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create gradient using a and b from above\ngrad <- c(b/(a + b)^2, \n          -a/(a + b)^2)\nvar_hat_mu <- grad %*% fit$var_hat %*% grad\nvar_hat_mu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\n[1,] 2.637491e-06\n```\n\n\n:::\n\n```{.r .cell-code}\nse_hat_mu <- sqrt(var_hat_mu)\nse_hat_mu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] 0.001624035\n```\n\n\n:::\n:::\n\n\n\n\n\n*Note:* R treats the two-element numeric vector as a 2x1 matrix or 1x2 matrix as necessary to make the matrix multiplication conformable. This avoids the need to explicitly transpose. We could also make `grad` a 2x1 column matrix and explicitly transpose if we wanted.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrad <- matrix(c(b/(a + b)^2, -a/(a + b)^2), \n               nrow = 2)\ngrad\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\n[1,]  0.004974134\n[2,] -0.001604724\n```\n\n\n:::\n\n```{.r .cell-code}\nvar_hat_mu <- t(grad) %*% fit$var_hat %*% grad\nvar_hat_mu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\n[1,] 2.637491e-06\n```\n\n\n:::\n:::\n\n\n\n\n\n#### \"By hand\"\n\nTo remind us what R is doing with `t(grad) %*% fit$var_hat %*% grad`, here is what the matrix multiplication looks like \"by hand.\"\n\n$$\n\\begin{aligned}\n\\widehat{\\mathrm{Var}}(\\widehat{\\mu})\n&\\approx\n\\overbrace{\\begin{bmatrix} 0.0050 & -0.0016 \\end{bmatrix}}^{\\nabla\\tau(\\hat\\alpha,\\hat\\beta)^{\\!\\top}}\n\\Biggl(\n\\underbrace{\\begin{bmatrix} 5.96 & 18.41\\\\[2pt] 18.41 & 57.84 \\end{bmatrix}}_{\\widehat{\\mathrm{Var}}(\\hat\\alpha,\\hat\\beta)}\n\\underbrace{\\begin{bmatrix} 0.0050\\\\[2pt] -0.0016 \\end{bmatrix}}_{\\nabla\\tau(\\hat\\alpha,\\hat\\beta)}\n\\Biggr)\n&& \\text{plug in values} \\\\[10pt]\n&=\n\\overbrace{\\begin{bmatrix} 0.0050 & -0.0016 \\end{bmatrix}}^{\\nabla\\tau^{\\!\\top}}\n\\begin{bmatrix}\n5.96(0.0050) + 18.41(-0.0016)\\\\[2pt]\n18.41(0.0050) + 57.84(-0.0016)\n\\end{bmatrix}\n&& \\text{multiply RHS;  (2 x 2) x (2 x 1)} \\\\[10pt]\n&=\n\\overbrace{\\begin{bmatrix} 0.0050 & -0.0016 \\end{bmatrix}}^{\\nabla\\tau^{\\!\\top}}\n\\begin{bmatrix}\n0.00037\\\\[2pt]\n-0.0010\n\\end{bmatrix}\n&& \\text{simplify} \\\\[10pt]\n&=\n0.0050\\cdot 0.00037 \\;+\\; (-0.0016)\\cdot(-0.0010)\n&& \\text{multiply; (1 x 2) x (2 x 1)} \\\\[6pt]\n&\\approx\n0.0000019 \\;+\\; 0.0000016\n&& \\text{simplify} \\\\[6pt]\n&=\n0.0000026\n&& \\text{simplify}\n\\end{aligned}\n$$\n\n\nWe can compare this to the classical mean and SE estimate. The classical and  beta model estimate and SE are very similar, but not identical.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# classical mean\nmean(bstats$batting_average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2439672\n```\n\n\n:::\n\n```{.r .cell-code}\n# classical SE\nsd(bstats$batting_average)/sqrt(length(bstats$batting_average))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001589904\n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n```{=html}\n<!-- preamble start -->\n\n    <!-- tinytable mathjax start -->\n    <script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script>\n    MathJax = {\n      tex: {\n        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n      },\n      svg: {\n        fontCache: 'global'\n      }\n    };\n    </script>\n    <!-- tinytable mathjax end -->\n\n    <script>\n\n      function styleCell_ee674t9dwqo4w2zifda5(i, j, css_id) {\n          var table = document.getElementById(\"tinytable_ee674t9dwqo4w2zifda5\");\n          var cell = table.querySelector(`[data-row=\"${i}\"][data-col=\"${j}\"]`);\n          if (cell) {\n              console.log(`Styling cell at (${i}, ${j}) with class ${css_id}`);\n              cell.classList.add(css_id);\n          } else {\n              console.warn(`Cell at (${i}, ${j}) not found.`);\n          }\n      }\n      function spanCell_ee674t9dwqo4w2zifda5(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_ee674t9dwqo4w2zifda5\");\n        const targetCell = table.querySelector(`[data-row=\"${i}\"][data-col=\"${j}\"]`);\n        if (!targetCell) {\n          console.warn(`Cell at (${i}, ${j}) not found.`);\n        }\n\n        // Get all cells that need to be removed\n        const cellsToRemove = [];\n        for (let r = 0; r < rowspan; r++) {\n          for (let c = 0; c < colspan; c++) {\n            if (r === 0 && c === 0) continue; // Skip the target cell\n            const cell = table.querySelector(`[data-row=\"${i + r}\"][data-col=\"${j + c}\"]`);\n            if (cell) {\n              cellsToRemove.push(cell);\n            }\n          }\n        }\n\n        // Remove all cells\n        cellsToRemove.forEach(cell => cell.remove());\n\n        // Set rowspan and colspan of the target cell if it exists\n        if (targetCell) {\n          targetCell.rowSpan = rowspan;\n          targetCell.colSpan = colspan;\n        }\n      }\n      // tinytable span after\n      window.addEventListener('load', function () {\n          var cellsToStyle = [\n            // tinytable style arrays after\n          { positions: [ { i: '2', j: 0 }, { i: '2', j: 1 }, { i: '2', j: 2 }, { i: '2', j: 3 },  ], css_id: 'tinytable_css_tfcgpn52jsqg9c1ohgim',}, \n          { positions: [ { i: '0', j: 0 }, { i: '0', j: 1 }, { i: '0', j: 2 }, { i: '0', j: 3 },  ], css_id: 'tinytable_css_y5wcnmejntrd01yida09',}, \n          ];\n\n          // Loop over the arrays to style the cells\n          cellsToStyle.forEach(function (group) {\n              group.positions.forEach(function (cell) {\n                  styleCell_ee674t9dwqo4w2zifda5(cell.i, cell.j, group.css_id);\n              });\n          });\n      });\n    </script>\n\n    <style>\n      /* tinytable css entries after */\n      .table td.tinytable_css_tfcgpn52jsqg9c1ohgim, .table th.tinytable_css_tfcgpn52jsqg9c1ohgim { border-bottom: solid #d3d8dc 0.1em; }\n      .table td.tinytable_css_y5wcnmejntrd01yida09, .table th.tinytable_css_y5wcnmejntrd01yida09 { border-top: solid #d3d8dc 0.1em; border-bottom: solid #d3d8dc 0.05em; }\n    </style>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_ee674t9dwqo4w2zifda5\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\" data-row=\"0\" data-col=\"0\">Model</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"1\">How mean and SE are estimated</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"2\">Mean</th>\n                <th scope=\"col\" data-row=\"0\" data-col=\"3\">SE</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td data-row=\"1\" data-col=\"0\">Classical</td>\n                  <td data-row=\"1\" data-col=\"1\">$\\hat\\mu = \\operatorname{avg}(y)$. $\\widehat{\\text{SE}} = \\frac{\\operatorname{SD}(y)}{\\sqrt{N}}$.</td>\n                  <td data-row=\"1\" data-col=\"2\">0.24397</td>\n                  <td data-row=\"1\" data-col=\"3\">0.001590</td>\n                </tr>\n                <tr>\n                  <td data-row=\"2\" data-col=\"0\">Beta model</td>\n                  <td data-row=\"2\" data-col=\"1\">Estimate $\\hat\\alpha$ and $\\hat\\beta$ with ML. Estimate variance of $\\hat\\alpha$ and $\\hat\\beta$ with information matrix. Estimate $\\hat\\mu$ with invariance property. Estimate SE using delta method.</td>\n                  <td data-row=\"2\" data-col=\"2\">0.24392</td>\n                  <td data-row=\"2\" data-col=\"3\">0.001624</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n<!-- hack to avoid NA insertion in last line -->\n```\n\n:::\n:::\n\n\n\n\n\n### Numerical gradient\n\nFor cases where the gradient of $\\tau(\\theta)$ is complex, we can compute a numerical gradient. Again, the algorithm finds the gradient by nudging $\\alpha$ and $\\beta$ and checking the change in $\\tau$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(numDeriv)   # for numerical gradients\n\n# create the function tau\ntau_fn <- function(theta) {  \n  a <- theta[1]\n  b <- theta[2]\n  a / (a + b)\n}\n\n# compute the gradient of tau\ngrad <- grad(func = tau_fn, x = fit$theta_hat)\n\n# delta method\nvar_hat_mu <- grad %*% fit$var_hat %*% grad  # R transposes grad as needed\nvar_hat_mu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\n[1,] 2.637491e-06\n```\n\n\n:::\n\n```{.r .cell-code}\n# sqrt of variance to find SE\nse_hat_mu <- sqrt(var_hat_mu) \nse_hat_mu\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] 0.001624035\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}