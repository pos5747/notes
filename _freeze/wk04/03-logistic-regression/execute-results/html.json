{
  "hash": "f272c50fac1b544eee25063675c1cb02",
  "result": {
    "engine": "knitr",
    "markdown": "# Logistic Regression\n\n\n\n\n\n\n\n\n\nIn the case of the normal model, we used $y_i \\sim N(\\mu_i, \\sigma^2)$, where $\\mu_i = X_i\\beta$. The normal model does a great job with roughly continuous outcomes like ENEP.\n\nBut sometimes we care about **binary outcomes**.\n\n-   Binary outcomes are categorical outcome variables with exactly two categories, such as whether or not someone voted, whether two countries are at war, and so on.\n-   In mathematical theory, it's helpful to code the binary variables as $y_i \\in \\{0, 1\\}$, with one representing \"an event\" and zero representing \"a non-event.\" In R, it's probably better to have proper factors and labels, though the 0/1 coding is common and works well.\n-   In generic language, we'll say that $y_i = 1$ means that \"an event has occurred\" and $y_i = 0$ means that \"an event has not occurred.\"\n-   This allows us to talk about the \"probability of an event\" (e.g., the probability of war, etc)\n\n### The Linear Probability Model\n\nThe normal model cannot describe a binary outcome well, becasue it doesn't make much conceptual sense to model 0s and 1s as following a normal distribution. That said, we can use the linear model (i.e., OLS) with binary outcome variables.\n\n-   Recall that we the linear model is represented by the equation $E(y_i) = X_i\\beta$.\n-   It is important to note that a probability is just a particular kind of expected value---a probability is an expected value of a binary variable.\n-   Since $y_i$ is binary, the $E(y_i) = \\Pr(y_i = 1) = \\Pr(y_i)$, giving us $\\Pr(y_i) = X_i\\beta$.\n\nThe LPM has two advantages:\n\n1.  It's is *very* easy to estimate (i.e., OLS; $\\hat{\\beta} = (X'X)^{-1}X'y$).\n2.  It is easy to interpret (i.e., a one unit change in $x_j$ leads to a $\\hat{\\beta_j}$ unit increase in $\\Pr(y)$).\n\nThe LPM has several disadvantages:\n\n1.  **Unbounded Predictions** Because the potential values for the explanatory variables are unbounded, you can obtain predicted probabilities above one and below zero. Of course, these predictions make no sense.\n2.  **Conditional Heteroskedasticity** The normal-linear model assumes a constant variance $\\sigma^2$. However, it is impossible to have homoskedastic residuals of a binary outcome if the probability of an event varies. Specifically, if $y_i$ is binary, then $\\text{Var}(y_i) = \\Pr(y_i)[1 - \\Pr(y_i)]$, which, for the LPM, equals $X_i\\beta(1 - X_i\\beta)$. (Non-zero coefficients imply heteroskedasticity.)\n3.  **Non-Normal Errors** Normal errors implies that the residuals can take on any value along the real line, with values closer to zero being more likely and errors outside three standard deviations being quite unlikely. However, if $y_i$ is binary, then the residual can take on only two values: $-Pr(y_i)$ or $1 - Pr(y_i)$.\n4.  **Functional Form** Theoretically, you'd probably expect explanatory variables to have smaller effects as $Pr(y_i)$ approaches zero or one (called \"compression,\" see @Berry2010 and @Rainey2015). The LPM assumes that the effects are constant.\n\n## `turnout` data set\n\nAs an intial simple example, let's use the `turnout` data set from the old {Zelig} package. {Zelig} has been replaced with {clarify}, but thankful jrnold saved Zelig's example data in the package [{ZeligData}](https://github.com/jrnold/ZeligData).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install jrnold's package from github\ndevtools::install_github(\"jrnold/ZeligData\")\n\n# load only the turnout data frame\nturnout <- ZeligData::turnout  # see ?ZeligData::turnout for details\n\nglimpse(turnout)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,000\nColumns: 5\n$ race    <fct> white, white, white, white, white, white, white, white, white,â€¦\n$ age     <int> 60, 51, 24, 38, 25, 67, 40, 56, 32, 75, 46, 52, 22, 60, 24, 30â€¦\n$ educate <dbl> 14, 10, 12, 8, 12, 12, 12, 10, 12, 16, 15, 12, 12, 12, 14, 10,â€¦\n$ income  <dbl> 3.3458, 1.8561, 0.6304, 3.4183, 2.7852, 2.3866, 4.2857, 9.3205â€¦\n$ vote    <int> 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,â€¦\n```\n\n\n:::\n:::\n\n\n\n\nWe can fit use use least squares to fit a linear probability model to these data, treating the binary 0/1 `vote` variable as numeric.\n\nBecause we now have a response `y` and a design matrix `X` we have to be a little careful that the handling of `y` and `X` are consistent (e.g., dropping rows with missing data). Using `model.frame()` *before* using `model.matrix()` is the proper way to do this. The canonical pipeline is to feed the formula and the data frame through `model.frame()` â†’ `model.matrix()` â†’ `model.response()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit linear probability model\nf <- vote ~ age + educate + income + race\n\n# make X and y\nmf <- model.frame(f, data = turnout)\nX <- model.matrix(f, data = mf)\ny <- model.response(mf)\n\n# ols, (X'X)^{-1}X'y\nbeta_hat <- solve(t(X)%*%X)%*%t(X)%*%y\nbeta_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   [,1]\n(Intercept) 0.053049343\nage         0.004399049\neducate     0.028949444\nincome      0.022116248\nracewhite   0.068393114\n```\n\n\n:::\n:::\n\n\n\n\n::: aside\nOr we can use `lm()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(f, data = turnout)\narm::display(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nlm(formula = f, data = turnout)\n            coef.est coef.se\n(Intercept) 0.05     0.05   \nage         0.00     0.00   \neducate     0.03     0.00   \nincome      0.02     0.00   \nracewhite   0.07     0.03   \n---\nn = 2000, k = 5\nresidual sd = 0.41, R-Squared = 0.11\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWe have modeled $\\pi = X\\beta$, so we can compute $\\hat{\\pi} = X\\hat{\\beta}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# mu-hat\npi_hat <- X%*%beta_hat\n```\n:::\n\n\n\n\nThis gives us some \"unusual\" probabilities.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pi_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       V1        \n Min.   :0.2594  \n 1st Qu.:0.6521  \n Median :0.7378  \n Mean   :0.7460  \n 3rd Qu.:0.8318  \n Max.   :1.2297  \n```\n\n\n:::\n\n```{.r .cell-code}\nggplot() + \n  geom_histogram(aes(x = pi_hat))\n```\n\n::: {.cell-output-display}\n![](03-logistic-regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n### The Logit Model\n\nAs an initial effort to handle the \"non-normal\" distribution of the data, we might then use the Bernoulli model $y_i \\sim \\text{Bernoulli}(\\pi_i)$, where $\\pi_i = X_i\\beta$.\n\nHowever, $\\pi_i = X_i\\beta$ has a *big* problem: $X_i\\beta$ might be less than zero or greater than one. This renders the approach unworkable.\n\nTo suitably link the bounded parameter $\\pi_i$ and with the unbounded linear predictor $X_i\\beta$, we use the \"inverse link function.\" The inverse link function wraps around the linear predictor and forces its values into the desired domain.[^03-logistic-regression-1]\n\n[^03-logistic-regression-1]: Many of the \"disadvantages\" of the LPM above follow from the fact that the linear predictor $\\mu = X\\beta$ is unbounded. For the normal model, the inverse link function is **not necessary** because the parameter of interest $\\mu$ is unbounded and maps to the entire real line. But for other models, the key parameter has a restricted domain. In the case of the Bernoulli distribution, $\\pi_i \\in [0, 1] \\subset \\mathbb{R}$.\n\nFor the Bernoulli distribution, we might use the inverse link function $g^{-1}(x) = \\frac{e^x}{1 + e^x}$.[^03-logistic-regression-2] This is called the \"inverse logit\" and it has an \"S\"-shape.[^03-logistic-regression-3] It's job is to map $X\\beta$ into $[0, 1]$.[^03-logistic-regression-4]\n\n[^03-logistic-regression-2]: This is not the only choice. Probit models, for example, use the standard normal cdf for the inverse link function.\n\n[^03-logistic-regression-3]: It's also the cdf of the standard logistic distribution.\n\n[^03-logistic-regression-4]: We could equivalently talk about \"link functions\" rather than \"*inverse* link functions.\" The link function would expand the bounded parameter to the entire real line; the *inverse* link function compresses the unbounded $X_i \\beta$ into the parameter's bounds.\n\nWe can plot the inverse logit to see how it works.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninv_logit <- function(x) {\n  (exp(x))/(1 + exp(x))\n}\n\nggplot() + \n  xlim(-10, 10) + \n  stat_function(fun = inv_logit) + \n  labs(x = \"â† unbounded linear predictor â†’\",\n       y = \"probability bounded between 0 and 1\")\n```\n\n::: {.cell-output-display}\n![](03-logistic-regression_files/figure-html/unnamed-chunk-7-1.png){width=384}\n:::\n:::\n\n\n\n\nHint: The inverse-logit function is the cdf of the standard logistic distribution, so you can just use `plogis()` in R, rather than hard-coding the `inv_logit()` function I create above.\n\nWe can write the model this way.\n\n$$\ny_i \\sim \\text{Bernoulli}(\\pi_i)\\text{, where } \\pi_i = \\text{logit}^{-1}(X_i\\beta).\n$$\n\nThis is logistic regression or the logit model. We can fit this model using maximum likelihood.\n\n### Fitting with `optim()\n\nTo develop the log-likelihood of the logit model, we start with the Bernoulli likelihood from before.\n\n$$\nf(y; \\beta) = L(\\beta) = \\prod_{i = 1}^{N}\\pi_i^{y_i} (1 - \\pi_i)^{(1 - y_i)}\\text{, where } \\pi_i = \\text{logit}^{-1}(X_i\\beta)\n$$ Taking the log, we have\n\n$$\n\\log L(\\beta) = \\sum_{i = 1}^{N} y_i \\log \\pi_i +  \\sum_{i = 1}^{N}(1 - y_i) \\log(1 - \\pi_i)\\text{, where } \\pi_i = \\text{logit}^{-1}(X_i\\beta)\n$$\n\nWe can program this into R for use in `optim()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_ll <- function(beta, y, X) {\n  linpred <- X%*%beta  \n  p <- plogis(linpred) # pi is special in R, so I use p\n  ll <- sum(dbinom(y, size = 1, prob = p, log = TRUE))\n  return(ll)\n}\n```\n:::\n\n\n\n\nThe tricky part about using `optim()` here is not the log-likelihood function, but setting up `X` and `y`. The code below creates the outcome vector $y$ and the matrix $X$ of explanatory variables (with a leading columns of 1s).\n\nWe already made `X` and `y` above, so we can use `optim()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar_start <- rep(0, ncol(X))\nopt <- optim(par_start, \n             fn = logit_ll, \n             y = y, \n             X = X, # â† covariates! ðŸŽ‰\n             method = \"BFGS\",\n      control = list(fnscale = -1))\nopt$par\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -3.03729123  0.02838767  0.17575513  0.17712903  0.25058596\n```\n\n\n:::\n:::\n\n\n\n\n::: aside\nBe skeptical of your code! Here's the proper way to fit a logit model using `glm()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- glm(f, family = binomial, data = turnout)\narm::display(fit, digits = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nglm(formula = f, family = binomial, data = turnout)\n            coef.est coef.se\n(Intercept) -3.0343   0.3260\nage          0.0284   0.0035\neducate      0.1756   0.0203\nincome       0.1771   0.0272\nracewhite    0.2508   0.1465\n---\n  n = 2000, k = 5\n  residual deviance = 2024.0, null deviance = 2266.7 (difference = 242.8)\n```\n\n\n:::\n:::\n\n\n\n:::\n\nWe can write a nice function that takes a formula and a data frame.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make function that fits beta model\nest_logit <- function(f, data) {\n  \n  # make X and y\n  mf <- model.frame(f, data = data)\n  X <- model.matrix(f, data = mf)\n  y <- model.response(mf)\n  \n  # create starting values\n  par_start <- rep(0, ncol(X))\n  \n  # run optim()\n  est <- optim(par_start, \n               fn = logit_ll, \n               y = y,\n               X = X,\n               hessian = TRUE, # for SEs!\n               control = list(fnscale = -1),\n               method = \"BFGS\") \n  \n  # check convergence; print warning if not\n  if (est$convergence != 0) print(\"Model did not converge!\")\n  \n  # create list of objects to return\n  res <- list(beta_hat = est$par,\n              var_hat = solve(-est$hessian))\n  \n  # return the list\n  return(res)\n}\n\n# fit logit model\nf <- vote ~ age + educate + income + race\nfit <- est_logit(f, data = turnout)\nprint(fit, digits = 2)  # print estimates w/ reasonable digits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$beta_hat\n[1] -3.037  0.028  0.176  0.177  0.251\n\n$var_hat\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,]  0.10622 -8.2e-04 -5.1e-03 -4.2e-04 -1.0e-02\n[2,] -0.00082  1.2e-05  2.9e-05  7.3e-06 -5.5e-05\n[3,] -0.00514  2.9e-05  4.1e-04 -1.6e-04 -3.2e-04\n[4,] -0.00042  7.3e-06 -1.6e-04  7.4e-04 -4.9e-04\n[5,] -0.01016 -5.5e-05 -3.2e-04 -4.9e-04  2.1e-02\n```\n\n\n:::\n:::\n\n\n\n\n## Interpretation\n\nBut the inverse link function makes our life considerably more complicated. In the linear model, the effect of a one-unit change in $x_j$ was simply $\\beta_j$. *This is no longer the case.*\n\n### Effects are not constant\n\nTo see how the inverse link function changes things, we can take the first derivative of $\\pi$ with respect to $\\x_1$.\n\nRecall that $\\pi_i = \\mathrm{logit}^{-1}(X_i\\beta)$. To keep this derivative simple, we'll break it into three parts: (i) first, find $\\frac{\\partial \\pi_i}{\\partial \\eta_i}$, then find $\\frac{\\partial \\eta_i}{\\partial x_{i1}}$, and then use the chain rule $\\frac{\\partial \\pi_i}{\\partial x_{i1}} = \\frac{\\partial \\pi_i}{\\partial \\eta_i}\\cdot \\frac{\\partial \\eta_i}{\\partial x_{i1}}$.\n\n**Step 1:** $\\frac{\\partial \\pi_i}{\\partial \\eta_i}$\n\n$$\n\\begin{aligned}\n\\pi_i \n  &= \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}}\n  &\\qquad& \\text{definition of inverse logit} \\\\[6pt]\n\\frac{\\partial \\pi_i}{\\partial \\eta_i}\n  &= \\frac{(1+e^{\\eta_i})\\,e^{\\eta_i} - e^{\\eta_i}\\,e^{\\eta_i}}{(1+e^{\\eta_i})^2}\n  &\\qquad& \\text{quotient rule} \\\\[6pt]\n  &= \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}\\left(1 - \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}\\right)\n  &\\qquad& \\text{factor out \\(e^{\\eta_i}/(1+e^{\\eta_i})\\)} \\\\[6pt]\n  &= \\pi_i\\,(1-\\pi_i)\n  &\\qquad& \\text{substitute \\(\\pi_i = \\frac{e^{\\eta_i}}{1+e^{\\eta_i}}\\)} \\\\[10pt]\n  \\end{aligned}\n$$\\\n\n**Step 2:** $\\frac{\\partial \\eta_i}{\\partial x_{i1}}$\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\eta_i}{\\partial x_{i1}}\n  &= \\beta_1\n  &\\qquad& \\text{since }\\eta_i=\\sum_{j} x_{ij}\\beta_j \\\\[10pt]\n\\end{aligned}\n$$\n\n**Step 3:** Chain rule\n\n$$\n\\begin{aligned}\n\\frac{\\partial \\pi_i}{\\partial x_{i1}}\n  &= \\frac{\\partial \\pi_i}{\\partial \\eta_i}\\cdot \\frac{\\partial \\eta_i}{\\partial x_{i1}}\n   \\;=\\; \\pi_i(1-\\pi_i)\\,\\beta_1 = \\left[ \\text{logit}^{-1}(X_i\\beta) \\right] \\cdot \\left[1 - \\text{logit}^{-1}(X_i\\beta) \\right] \\cdot \\beta_1\n\\end{aligned}\n$$\n\nThe marginal effect of $x_{i1}$ on the probability of an event is **not constant**. It depends on the value of $x_1$ \\*and all the other $x$s*!\n\nTo see this clearly, we can assume the logit model $\\Pr(y) = \\operatorname{logit}^{-1}(-4 + x_1 + x_2)$ and plot $\\Pr(y)$ as $x_1$ varies for different values of $x_2$. Notice that the marginal effect $\\frac{\\partial \\Pr(y)}{\\partial x_1}$ varies *depending on the value of $x_2$. Remember, there are no product terms in this model. However, the effect of the variables are interactive, at least on $\\Pr(y)$.\n\n::: {.column-margin}\nThis figure is a variant Panel A of Figure 2 on p. 252 of @Berry2010.\n\n![](images/clipboard-1632969731.png){width=\"90%\"}\n:::\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![This figure illustrates that interaction arises even when the model has no product term. The model is $\\Pr(y) = \\operatorname{logit}^{-1}(-4 + x_1 + x_2)$ with no $x_1x_2$ term. Each curve plots $\\Pr(y)$ as a function of $x_1$ for three fixed values of $x_2 \\in \\{0,1,2\\}$. Because the inverse-logit link compresses probabilities toward 0 and 1, the marginal effect of $x_1$ on $\\Pr(y)$, $\\frac{\\partial \\Pr(y)}{\\partial x_1} = \\Pr(y)\\bigl(1-\\Pr(y)\\bigr)$, depends on the value of $x_2$. Points and labels indicate selected derivatives. For example, at $x_1 \\approx 4.51$ the marginal effect is $\\partial \\Pr(y)/\\partial x_1 \\approx .234$ when $x_2=0$ and $\\approx .070$ when $x_2=2$.](03-logistic-regression_files/figure-html/unnamed-chunk-12-1.png){width=720}\n:::\n:::\n\n\n\n\n### Two Quantities of Interest\n\nGiven that the coefficients are directly interpretable, we can focus on other, simpler quantities of interest.\n\nWe have to core quantities of interest:\n\n1. **expected value** $E(y \\mid X_c)$. The expected value of the outcome $y$ given a particular, **c**hosen covariate $X_c$. Here, $X_c$ is a $1 \\times (k + 1)$ row matrix that contains particular values for $x_1, x_2, ..., x_k$. \n1. **first difference** $E(y \\mid X_{hi}) - E(y \\mid X_{lo})$. The first difference is the difference between two expected values. First, we find an expected value of the outcome $y$ given a particular covariates $X_{lo}$, where one covariate is set to a \"low\" value of interest. Second, we find an expected value given covariates $X_{hi}$, where that same covariate is changed to a \"high\" value of interest and the others are left unchanged.\n\nAs possible, I borrow my language around \"quantities of interest\", including \"expected values\" and \"first differences\" from @King2000, which has been hugely influential in political science.\n\n\n#### Expected value for a typical case\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create chosen values for X\n# note: naming columns helps a bit later\nX_c <- cbind(\n  \"constant\" = 1, # intercept\n  \"age\"      = median(turnout$age), \n  \"educate\"  = median(turnout$educate),\n  \"income\"   = median(turnout$income),\n  \"white\"    = 1 # white indicators = 1 \n)\n\nev_fn <- function(beta, X) {\n  plogis(X%*%beta)\n}\n\n# invariance property\nev_hat <- ev_fn(fit$beta_hat, X_c)\n\n# delta method\nlibrary(numDeriv)  # for grad\ngrad <- grad(\n  func = ev_fn, # what function are we taking the derivative of?\n  x = fit$beta_hat, # what variable(s) are we taking the derivative w.r.t.?\n  X = X_c)  # what other values are needed?\nse_ev_hat <- sqrt(grad %*% fit$var_hat %*% grad)\n```\n:::\n\n\n\n\n#### Expected value for many cases\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# - use ev_fn() from above\n\n# create chosen values for X\nX_c <- cbind(\n  \"constant\" = 1, # intercept\n  \"age\"      = min(turnout$age):max(turnout$age), \n  \"educate\"  = median(turnout$educate),\n  \"income\"   = median(turnout$income),\n  \"white\"    = 1 # white indicators = 1 \n)\n\n# containers for estimated quantities of interest and ses\nev_hat <- numeric(nrow(X_c))\nse_ev_hat <- numeric(nrow(X_c))\n\n# loop over each row of X_c and compute qi and se\nfor (i in 1:nrow(X_c)) {\n  # for the ith row of X...\n  \n  # invariance property\n  ev_hat[i] <- ev_fn(fit$beta_hat, X_c[i, ])\n  \n  # delta method\n  grad <- grad(\n    func = ev_fn, # what function are we taking the derivative of?\n    x = fit$beta_hat, # what variable(s) are we taking the derivative w.r.t.?\n    X = X_c[i, ])  # what other values are needed?\n  se_ev_hat[i] <- sqrt(grad %*% fit$var_hat %*% grad)\n}\n\n# put X_c, qi estimates, and se estimates in data frame\nqi <- cbind(X_c, ev_hat, se_ev_hat) |>\n  data.frame() |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 79\nColumns: 7\n$ constant  <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ age       <dbl> 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, â€¦\n$ educate   <dbl> 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, â€¦\n$ income    <dbl> 3.3508, 3.3508, 3.3508, 3.3508, 3.3508, 3.3508, 3.3508, 3.35â€¦\n$ white     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦\n$ ev_hat    <dbl> 0.5983202, 0.6051232, 0.6118858, 0.6186055, 0.6252802, 0.631â€¦\n$ se_ev_hat <dbl> 0.02562903, 0.02480870, 0.02399725, 0.02319621, 0.02240713, â€¦\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot\nggplot(qi, aes(x = age, y = ev_hat, \n               ymin = ev_hat - 1.64*se_ev_hat, \n               ymax = ev_hat + 1.64*se_ev_hat)) + \n  geom_ribbon() + \n  geom_line()\n```\n\n::: {.cell-output-display}\n![](03-logistic-regression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n::: aside\n\nCompare this to {marginaleffects}.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fit model\nglm_fit <- glm(f, data = turnout, family = binomial)\n\n# use marginaleffects to do transformation and delta method\nlibrary(marginaleffects)\nme_qi <- predictions(glm_fit, \n                     newdata = datagrid(\n                       age = \\(x) seq(min(x), max(x)),  # set age using fn defined with \\() syntax\n                       educate  = median,  # set educate to sample median\n                       income = median,  # set income to sample median\n                       race = \"white\"  # set race to white\n                     ),\n                     conf_level = 0.90)\n\n# plot\nggplot(me_qi, aes(x = age, y = estimate, \n               ymin = conf.low, \n               ymax = conf.high)) + \n  geom_ribbon() + \n  geom_line()\n```\n\n::: {.cell-output-display}\n![](03-logistic-regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n:::\n\n#### First difference\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make X_lo\nX_lo <- cbind(\n  \"constant\" = 1, # intercept\n  \"age\"      = quantile(turnout$age, probs = 0.25), # 31 years old; 25th percentile\n  \"educate\"  = median(turnout$educate),\n  \"income\"   = median(turnout$income),\n  \"white\"    = 1 # white indicators = 1 \n)\n\n# make X_hi by modifying the relevant value of X_lo\nX_hi <- X_lo\nX_hi[, \"age\"] <- quantile(turnout$age, probs = 0.75) # 59 years old; 75th percentile\n\n# function to compute first difference\nfd_fn <- function(beta, hi, lo) {\n  plogis(hi%*%beta) - plogis(lo%*%beta)\n}\n\n# invariance property\nfd_hat <- fd_fn(fit$beta_hat, X_hi, X_lo)\n\n# delta method\ngrad <- grad(\n  func = fd_fn, \n  x = fit$beta_hat, \n  hi = X_hi,\n  lo = X_lo)  \nse_fd_hat <- sqrt(grad %*% fit$var_hat %*% grad)\n\n# estimated fd\nfd_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n25% 0.1416257\n```\n\n\n:::\n\n```{.r .cell-code}\n# estimated se\nse_fd_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]\n[1,] 0.0170934\n```\n\n\n:::\n\n```{.r .cell-code}\n# 90% ci\nfd_hat - 1.64*se_fd_hat  # lower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n25% 0.1135925\n```\n\n\n:::\n\n```{.r .cell-code}\nfd_hat + 1.64*se_fd_hat  # upper\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         [,1]\n25% 0.1696588\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: callout-warning\n\nThe language around quantities of interest is inconsistent. Scholars use different terms to mean the same concept and the same terms to mean different concepts. A prime example is \"marginal effect.\" Some scholars use \"marginal effect\" to mean the discrete effect $E(y \\mid X_{hi}) - E(y \\mid X_{lo})$--what I call a \"first difference\" above. Other scholars use \"marginal effect\" to mean the *instantaneous* effect $\\frac{\\partial E(y \\mid X_c)}{\\partial x_j}$. Given this inconsistency, it's best to *read carefully* what other authors mean and *write explicitly* what you mean.\n\n:::\n\n### Fitting with `glm()`\n\nRather than use `optim()` we can fit the logit model with `glm()` by supplying the argument `family = binomial`.^[The Bernoulli distribution that we used to motivate the logit model is a special case of the binomial distribution. Unless we specify otherwise, `family = binomial` uses the Bernoulli.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- vote ~ age + educate + income + race\nfit <- glm(f, data = turnout, family = binomial)\n```\n:::\n\n\n\n\nWe'll later see how to use {marginal effects} to compute quantities of interest. But for now, realize that we can find the coefficient estimates with `coef()` and the estimated covariance matrix with `vcov()`. We can use these to manually obtain estimates of quantities of interest and their standard errors without marginal effects. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- coef(fit)\nv_hat <- vcov(fit)\n```\n:::\n\n\n\n\n::: callout-warning\n\nSoftware to automatically handle computations is *really* valuable. When users write their own code to handle basic, common calculations (e.g., using `optim()` to fit a logit model), they run a huge risk of introducing errors. Well-tested, widely-used software written by actual programmers has far fewer bugs that single-use code written by substantive researchers. That said, it is really easy to *misunderstand* what \"easy-to-use\" software is actually doing. For example, documentation uses the term \"marginal effect\" inconsistently. And model fitting functions sometimes use unexpected parameterizations. Without paying careful attention, it's easy to incorrectly use well-tested software. When you are first learning a software package, it's important to check that you are computing what you think you are computing (e.g., a rate, not a mean; a instantaneous effect, not a discrete effect).\n\n:::\n\n\n\n## The Scobit Data\n\nLet's fit the normal model to data from @WolfingerRosenstone1980, @Nagler1994, and @Berry2010.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# # load scobit data\n# scobit <- haven::read_dta(\"data/scobit.dta\") %>%\n#   filter(newvote != -1) %>%  # weird -1s in data; unsure if sufficient\n#   glimpse()\n# \n# # fit linear probability model\n# f <- newvote ~ poly(neweduc, 2, raw = TRUE) + closing + poly(age, 2, raw = TRUE) + south + gov\n# fit <- lm(f, data = scobit)\n# \n# # simulate from predictive distribution\n# mu_hat <- predict(fit)  # the linear predictor for each row of data frame\n# sigma_hat <- sqrt(sum(residuals(fit)^2))\n# y_tilde <- rnorm(nrow(scobit), mu_hat, sigma_hat)\n# \n# # plot simulated against observed values\n# par(mfrow = c(1, 2))\n# hist(scobit$newvote)\n# hist(y_tilde)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}