{
  "hash": "b47840782ff5fb2df99994e59baed920",
  "result": {
    "engine": "knitr",
    "markdown": "# GLMs\n\n\n\n\n\n\n\n\n\nThis is a course on probability models. A commonly referenced subset of these models are referred to as generalized linear models or GLMs. Because GLMs are commonly referenced, it's worth understanding the theory. But for our purposes, there isn't a practical difference between probability models that are GLMs and probabilities models that are not GLMs.\n\nIn fact, some use the term \"GLM\" to mean probability models broadly, not just those models covered in the theory below.\\\n\n![](images/clipboard-1034886583.png){width=50%}\n\\\n![](images/clipboard-579218103.png){width=50%}\n\n@mccullagh1989 gave us the initial theory of the GLM. They show how many probability models fall into a single unified framework that they called *generalized linear models* (GLMs). The theory is really beautiful, simple, and powerful. @gill2019 offers a good introduction for political scientists.\n\nA GLM has three components:\n\n1.  **Random component.** $y_i$ follows a distribution in the exponential family.\n1.  **Systematic component.** Linear predictor $\\eta_i = X_i\\beta$.\n1.  **Link function.** Monotone map $g(\\mu_i) = \\eta_i$ with $\\mu_i = \\mathbb{E}(Y_i)$.\n\n## Exponential families\n\nA density/pmf is in the **exponential family** if it can be written \n\n$$\nf(y\\mid \\theta,\\phi)\n=\\exp\\!\\left\\{\\frac{y\\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi)\\right\\}.\n$$\n\n-   $\\theta$: canonical parameter\n-   $\\phi$: dispersion parameter\n-   $\\mu = \\mathbb{E}(Y) = b'(\\theta)$\n-   $\\operatorname{Var}(Y) = b''(\\theta)\\,a(\\phi)$\n\nIf we choose $g(\\mu)$ so that the linear predictor $X_i \\beta$ equals the canonical parameter $\\eta_i$, then we say that $g(\\cdot)$ is the *canonical link.* Canonical links simplify the score equations and IRWLS weights, but are not required in practice.\n\n\n### Core examples\n\n**Logistic regression.** $Y_i \\sim \\operatorname{Bernoulli}(\\pi_i)$ with canonical link $g(\\pi_i)=\\log\\!\\dfrac{\\pi_i}{1-\\pi_i}=X_i\\beta$.\n\n**Poisson regression (counts).** $Y_i \\sim \\operatorname{Poisson}(\\lambda_i)$ with canonical link $g(\\lambda_i)=\\log\\lambda_i=X_i\\beta$.\n\n### Negative binomial edge case\n\nWith fixed dispersion $\\kappa$, an exponential-family form exists. In practice $\\kappa$ is estimated, so NB is \"GLM-like\" but not strictly a GLM.\n\n### Common GLMs (canonical forms)\n\n| Family | Outcome type | Canonical link $g(\\mu)$ | Variance function $V(\\mu)$ |\n|:---------------|:---------------|:-----------------------|:----------------|\n| Normal | Continuous | Identity $\\mu$ | $1$ |\n| Bernoulli | Binary | Logit $\\log\\!\\dfrac{\\mu}{1-\\mu}$ | $\\mu(1-\\mu)$ |\n| Poisson | Counts | Log $\\log \\mu$ | $\\mu$ |\n| Gamma | Positive continuous | Inverse $1/\\mu$ | $\\mu^2$ |\n\n## Estimation: IRLS \n\nGLMs are estimated by **maximum likelihood**. It turns out that iteratively reweighted least squares (IRLS) is an exceptionally robust numerical algorithm to find the ML estimates (and their variance estimates) for GLMs.\n\n-   Working response: $z_i = \\eta_i + (y_i-\\mu_i)\\dfrac{d\\eta_i}{d\\mu_i}$.\\\n-   Weights: $w_i = \\dfrac{1}{\\phi}\\left(\\dfrac{d\\mu_i}{d\\eta_i}\\right)^2 \\big/ V(\\mu_i)$.\\\n-   Update: $\\hat\\beta \\leftarrow (X^\\top W X)^{-1} X^\\top W z$. \n\nRepeat until convergence.\n\nThis algorithm is historically important, but happens \"behind the scenes\" in modern computation. It's more conceptually useful to imagine a generic gradient ascent algorithm applied to directly to the multidimensional log-likelihood.\n\n## Expected Fisher information\n\nAt the MLE, $\\mathcal{I}(\\beta) = \\tfrac{1}{\\phi} X^\\top W X$. This looks very similar to the weighted cross-product matrix used in the IRLS updating step. There's a connection here. IRLS arises from applying Newtonâ€“Raphson to the gradient of the log-likelihood, and the Hessian of the log-likelihood (whose expectation is the Fisher information) uses the same $W$ matrix. \n\nAs an interesting result, the weights $W$ indicate both the direction of the iterative updates and the curvature of the likelihood surface. When the algorithm has converged, the final $X^\\top W X$ approximates the covariance matrix for $\\hat\\beta$.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}