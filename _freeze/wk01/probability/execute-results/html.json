{
  "hash": "ea400b350c29dee5b0f3221a0e87b7b4",
  "result": {
    "engine": "knitr",
    "markdown": "# Probability Theory\n\nProbability theory gives us a language to describe uncertainty. \n\n## Probability, Outcomes, and Events\n\nProbability theory begins with the idea of a **random process**, which an ``experiment'' or procedure whose outcome is not known in advance.\n\n::: {#def-random-process}\nA **random process** is a repeatable procedure to obtain an observation from a defined set of outcomes.\n:::\n\n::: {#def-sample}\nThe **sample space** $\\Omega$ is the set of all possible outcomes of a random process.\n:::\n\n::: {#def-realization}\nA **realization** of the random process produces an outcome from the sample space.\n:::\n\n::: {#def-event}\nAn **event** is any subset  of outcomes $A \\subseteq \\Omega$. The probability of an event $A$ is denoted $\\Pr(A)$.\n:::\n\n\n\n## Axioms of Probability\n\nThe probability function $P(\\cdot)$ must satisfy the following three rules.\n\n::: {#thm-axioms}\nLet $P$ be a probability function defined on a sample space $\\Omega$. Then:\n\n1. $P(A) \\ge 0$ for all events $A$.\n2. $P(\\Omega) = 1$.\n3. For every infinite sequence of disjoint events $A_1, A_2, ...$, $\\Pr \\left( \\displaystyle \\bigcup_{i = 1}^\\infty A_i \\right) = \\displaystyle \\sum_{i = 1}^\\infty \\Pr(A_i)$.^[Some notes on Axiom 3. Examples of an infinite sequence of disjoint events? For $\\Omega = \\mathbb{R}^+$? For $S =\\{0, 1\\}$? An infinite sequence of disjoint events is difficult to conceptualize. For $S = \\mathbb{R}^+$, *one* such sequence would be $[0, 1), [1, 2), [2, 3), ...$. For $S =\\{0, 1\\}$, *one* such sequence would be $\\{0\\}, \\{1\\}, \\emptyset, \\emptyset, \\emptyset,...$.]\n:::\n\n::: {#def-probability}\nFor a sample space $\\Omega$, a **probability** is a collection of real numbers assigned to all events $A$ consistent with Axioms 1, 2, and 3.\n:::\n\n\n::: {#exm-axiom1}\nLet $\\Omega = \\{\\text{H}, \\text{T}\\}$ represent the outcomes of a fair coin flip.  \nDefine a probability function:  \n- $P(\\{\\text{H}\\}) = 0.5$  \n- $P(\\{\\text{T}\\}) = 0.5$\n\nThen the three axioms of probability are satisfied as follows:\n\n1. $P(\\{\\text{H}\\}) = 0.5 \\ge 0$, $P(\\{\\text{T}\\}) = 0.5 \\ge 0$\n1. $P(\\Omega) = P(\\{\\text{H}, \\text{T}\\}) = 0.5 + 0.5 = 1$\n1. Let $A = \\{\\text{H}\\}$, $B = \\{\\text{T}\\}$. These are disjoint, so, $P(A \\cup B) = P(A) + P(B) = 0.5 + 0.5 = 1$.\n:::\n\n\n## Some Results of the Axioms\n\n::: {#thm-empty-zero}\n$\\Pr(\\emptyset) = 0$. \n:::\n\n::: {.proof}\n$\\Pr(\\emptyset) = \\Pr(\\cup_{i = 1}^\\infty \\emptyset) = \\sum_{i = 1}^\\infty \\Pr(\\emptyset)$. $\\Pr(\\emptyset) = \\sum_{i = 1}^\\infty \\Pr(\\emptyset)$ iff $\\Pr(\\emptyset) = 0$.\n:::\n\n::: {#thm-add-disjoint}\nFor every finite sequence of $n$ disjoint events $A_1, A_2, ..., A_n$,\n\\begin{equation}\n\\Pr \\left( \\displaystyle \\bigcup_{i = 1}^n A_i \\right) = \\displaystyle \\sum_{i = 1}^n \\Pr(A_i). \\nonumber\n\\end{equation}\n:::\n\n::: {#thm-addition-rule}\n**Addition Rule for Two Disjoint Events**  \nFor disjoint events $A$ and $B$, $\\Pr ( A \\cup B) = \\Pr(A) + \\Pr(B)$\n:::\n\n::: {#thm-monotonicity}\nIf event $A \\subseteq B$, then $\\Pr(A) \\leq \\Pr(B)$. \n:::\n\n::: {#thm-bounds}\nFor event $A$, $0 \\leq \\Pr(A) \\leq 1$. \n:::\n\n::: {#thm-addition-for-2}\n**Addition Rule for Two Events**  \nFor any events $A$ and $B$, $\\Pr ( A \\cup B) = \\Pr(A) + \\Pr(B) - \\Pr(A \\cap B)$.\n:::\n\n::: {#thm-addition-for-3}\n**Addition Rule for Three Events**  \nFor any events $A$, $B$, and $C$,\n \\begin{align*}\n \\Pr ( A \\cup B \\cup C) &= \\Pr(A) + \\Pr(B) + \\Pr(C)\\\\\n                                   &- \\left[ \\Pr(A \\cap B) + \\Pr(A \\cap C) + \\Pr(B \\cap C) \\right]\\\\\n                                   &+ \\Pr(A \\cap B \\cap C).\n                                   \\end{align*}\n:::\n\n\n\n## Conditional Probability and Independence\n\n::: {#def-conditional-probability}\n**Conditional Probability**  \n$\\Pr(A \\mid B) = \\dfrac{\\Pr(A \\cap B)}{\\Pr(B)}$ for $\\Pr(B) > 0$. If $\\Pr(B) = 0$, then $\\Pr(A \\mid B)$ is undefined.\n:::\n\nWe interpret the conditional probability $\\Pr(A \\mid B)$ as the probability of $A$ given that $B$ happens (or has already happened). Suppose a bag with two green marbles and two red marbles. I draw two marbles without replacement and see that the first is green. Then the probability that the second is green, given that the first is/was green, is\n\n$$\n\\Pr(\\text{second is green} \\mid \\text{first is green}) = \\frac{\\Pr(\\text{second is green AND first is green})}{\\Pr (\\text{first is green)}}.\n$$\n\n::: {#def-independence}\n**Independence of Two Events**  \nEvents $A$ and $B$ are **independent** if $\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)$.\n\nIf $\\Pr(A) > 0$ and $\\Pr(B) > 0$, then @def-conditional-probability and @def-independence imply that two events are independent if and only if their conditional probabilities equal their unconditional probabilities so that $\\Pr(A \\mid B) = \\Pr(A)$ and $\\Pr(B \\mid A) = \\Pr(B)$.\n:::\n\n::: {#def-independence-n}\n**Independence of $n$ Events**  \nEvents $A_1, A_2, ..., A_n$ are independent if for every subset $A_a,..., A_m$ with at least two events, $\\Pr(A_a \\cap ... \\cap A_m) = \\Pr(A_a)...\\Pr(A_m)$.\n:::\n\nThe \"every subset\" part of @def-independence-n is subtle, so let's create a specific example. \"Every subset\" of $A$, $B$, and $C$ with at least two events includes the following: $\\{A, B\\}$, $\\{A, C\\}$, $\\{B, C\\}$, and $\\{A, B, C\\}$.\n\n\n## Fundamental Laws\n\n::: {#def-partition}\nTo create a **partition** $B_1, B_2, ..., B_k$ of the sample space $S$, divide $S$ into $k$ disjoint events $B_1, B_2, ..., B_k$ so that $\\bigcup_{i = 1}^n B_i = S$.\n:::\n\n::: {#thm-ltp}\n**Law of Total Probability**  \nSuppose a partition $B_1, B_2, ..., B_k$ of the sample space $S$ where $\\Pr(B_j) > 0$ for $j = 1, 2, ... , k$. Then\n\n$$\n\\Pr(A) = \\sum_{j = 1}^k \\Pr(B_j )\\Pr(A \\mid B_j).\n$$\n:::\n\n::: {#thm-bayes}\n**Bayes' Rule**  \nSuppose a partition $B_1, B_2, ..., B_k$ of the sample space $S$ where $\\Pr(B_j) > 0$ for $j = 1, 2, ... , k$. Suppose an event $A$, where $\\Pr(A) > 0$. Then\n\n$$\n\\Pr(B_i \\mid A) = \\dfrac{\\Pr(B_i) \\Pr(A \\mid B_i)}{\\sum_{j = 1}^k \\Pr(B_j )\\Pr(A \\mid B_j)}.\n$$\n:::\n\n::: {#thm-bayes-simple}\n**Bayes' Rule for a simpler partition**  \nSuppose the simple partition $B$ and $B^c$ of the sample space $S$ where $\\Pr(B) > 0$ and $\\Pr(B^c) > 0$. Suppose an event $A$, where $\\Pr(A) > 0$. Then\n\n$$\n\\Pr(B \\mid A) = \\dfrac{\\Pr(B) \\Pr(A \\mid B)}{\\Pr(B) \\Pr(A \\mid B) + \\Pr(B^c) \\Pr(A \\mid B^c)}.\n$$\n:::\n\n\n::: {#exr-bayes-rule}\nYou're considering getting tested for a rare disease that 1 in 100,000 people have. If given to a person with the disease, the test will produce a positive result 99\\% of the time. If given to a person without the disease, the test will produce a positive result 0.1\\% of the time (i.e., 1 in 1,000). You are given the test and the result comes back positive. Use Bayes' rule to compute the chance that you have the disease. \n\n**Solution**\n\nLet $D$ denote having the disease and $T$ a positive test.\n\n- $P(D) = 1/100{,}000 = 10^{-5}$\n- $P(T \\mid D) = 0.99$\n- $P(T \\mid D^c) = 0.001$\n\nCompute the marginal\n\n$$\nP(T) \\;=\\; P(T\\mid D)P(D) + P(T\\mid D^c)P(D^c)\n= 0.99(10^{-5}) + 0.001(1-10^{-5})\n= 9.9\\times 10^{-6} + 0.00099999\n= 0.00100989.\n$$\n\nApply Bayes' rule\n\n$$\nP(D\\mid T) \\;=\\; \\frac{P(T\\mid D)P(D)}{P(T)}\n= \\frac{0.99 \\times 10^{-5}}{0.00100989}\n\\approx 0.0098.\n$$\n\nSo the chance you have the disease given a positive test is about $0.98\\%$ (i.e., less than 1%).\n\n:::\n\n## Random Variables\n\nA **random variable** is a numerical summary of the **possible outcomes** from a random process.\n\n::: {#def-rv}\nA **random variable** $X$ is a function from a sample space $\\Omega$ to the real numbers:\n\n$$\nX: \\Omega \\rightarrow \\mathbb{R}\n$$\n\nThat is, $X(\\omega)$ is the number assigned to the outcome $\\omega$.\n:::\n\nThe random part comes from the fact that the outcome $\\omega$ is not known in advance so the value $X(\\omega)$ is also uncertain.\n\n\nWe often use random variables to model outcomes of interest\n\n- Whether or not someone votes\n- The time until a bill passes in a legislature\n- The ideology score of a member of Congress\n- The percentage of survey respondents who support a policy\n- The number of protests in a country during a given year\n\nWe usually classify random variables as\n\n- **Discrete**: takes values in a finite or countably infinite set (e.g., $\\{0, 1, 2, \\dots\\}$)\n- **Continuous**: takes values in an interval of the real line (e.g., $[0, \\infty)$)\n\n::: {#exm-rv1}\nLet $X$ be the number protests in a given country-year.  \n- Possible values: $0, 1, 2, \\dots$  \n- So $X$ is a **discrete** random variable.\n\nLet $Y$ be the time between protests.  \n- Possible values: any real number $\\ge 0$  \n- So $Y$ is a **continuous** random variable.\n:::\n\n\n### Random Variables and Events\n\nRandom variables allow us to define numerical events using real numbers.\n\nFor example, if $X$ is the number of protests:\n\n- $X = 27$ is shorthand for the event $\\{\\omega \\in \\Omega : X(\\omega) = 27\\}$\n- $X \\le 27$ is shorthand for $\\{\\omega \\in \\Omega : X(\\omega) \\le 27\\}$\n\n\n\n## PMFs, PDFs, and CDFs\n\n### PMF (Probability Mass Function)\n\nA discrete random variable $X$ has a **PMF** $p(x)$ satisfying $p(x) \\ge 0$ for all $x$ and $\\sum_x p(x) = 1$.\n\n### PDF (Probability Density Function)\n\nA continuous random variable $X$ has a **PDF** $f(x)$ satisfying $f(x) \\ge 0$ for all $x$ and $\\int_{-\\infty}^{\\infty} f(x)\\, dx = 1$.\n\nFor continuous variables, probabilities are areas under the density curve:\n\n$$\nP(a \\le X \\le b) = \\int_a^b f(x)\\, dx\n$$\n\n### CDF (Cumulative Distribution Function)\n\nThe **CDF** $F(x)$ of a random variable $X$ is $F(x) = P(X \\le x)$.\n\n- For discrete $X$: $F(x) = \\sum_{t \\le x} p(t)$.\n- For continuous $X$: $F(x) = \\int_{-\\infty}^x f(t)\\, dt$.\n\n::: {#exm-pmfpdf1}\nLet $X \\sim \\text{Bernoulli}(0.7)$. Then:\n\n- PMF: $P(X = 1) = 0.7$, $P(X = 0) = 0.3$\n- CDF: $F(x) = 0$ for $x < 0$, $0.3$ for $0 \\le x < 1$, $1$ for $x \\ge 1$\n\nLet $Y \\sim \\text{Exponential}(\\lambda = 2)$. Then:\n\n- PDF: $f(y) = 2 e^{-2y}$ for $y \\ge 0$\n- CDF: $F(y) = 1 - e^{-2y}$ for $y \\ge 0$\n:::\n\n\n## Expected Value, Variance, and Moments\n\n::: {#def-expect}\nThe **expected value** of a random variable $X$ is $\\mathbb{E}[X] = \\sum_x x \\cdot p(x)$ for discrete random variables and $\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x)\\, dx$ for continuous random variables.\n:::\n\n::: {#def-var}\nThe **variance** of $X$ is $\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n:::\n\n\n::: {#exm-expect1}\nLet $X \\sim \\text{Poisson}(\\lambda = 3)$. Then $\\mathbb{E}[X] = 3$ and $\\text{Var}(X) = 3$.\n\nLet $Y \\sim \\text{Exponential}(\\lambda = 2)$. Then $\\mathbb{E}[Y] = \\frac{1}{2} = 0.5$ and $\\text{Var}(Y) = \\frac{1}{4} = 0.25$.\n:::\n\n::: {#exm-expect2}\nLet $X \\sim \\text{Bernoulli}(0.7)$. Compute $\\mathbb{E}[X]$ and $\\text{Var}(X)$.\n\n*Solution.*  \nPMF: $P(X = 1) = 0.7$, $P(X = 0) = 0.3$\n\n- $\\mathbb{E}[X] = 0 \\cdot 0.3 + 1 \\cdot 0.7 = 0.7$\n- $\\mathbb{E}[X^2] = 0^2 \\cdot 0.3 + 1^2 \\cdot 0.7 = 0.7$\n- $\\text{Var}(X) = 0.7 - (0.7)^2 = 0.7 - 0.49 = 0.21$\n\n:::\n\n\n\n### Properties of Expectations, Variances, and Covariances\n\n::: {#thm-linearity-expect}\n## Linearity of Expectation\nFor any constants $a, b$ and random variables $X, Y$,\n$\\mathbb{E}[X + bY] = a\\,\\mathbb{E}[X] + b\\,\\mathbb{E}[Y]$.\n:::\n\n::: {#thm-expect-constant}\n## Expectation of a Constant \nFor any constant $c$, $\\mathbb{E}[c] = c$.\n:::\n\n::: {#thm-expect-product}\n## Expectation of a Product\nFor any random variables $X$ and $Y$,  \n$\\mathbb{E}[XY] = \\mathrm{Cov}(X, Y) + \\mathbb{E}[X]\\,\\mathbb{E}[Y]$.  \nIf $X$ and $Y$ are independent, then $\\mathbb{E}[XY] = \\mathbb{E}[X]\\,\\mathbb{E}[Y]$.\n:::\n\n::: {#thm-var-scaling}\n## Variance Scaling\nFor any constant $a$, $\\mathrm{Var}(aX) = a^2\\,\\mathrm{Var}(X)$.\n:::\n\n::: {#thm-var-sum}\n## Variance of a Sum\nFor any random variables $X$ and $Y$,  \n$\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2\\,\\mathrm{Cov}(X, Y)$.  \nIf $X$ and $Y$ are independent, then $\\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$.\n:::\n\n::: {#thm-cov-scaling}\n## Covariance Scaling  \nFor any constants $a, b$, $\\mathrm{Cov}(aX, bY) = ab\\,\\mathrm{Cov}(X, Y)$.\n:::\n\n::: {#thm-cov-linearity}\n## Linearity of Covariance \nFor any random variables $X, Y, Z$,  \n$\\mathrm{Cov}(X + Y, Z) = \\mathrm{Cov}(X, Z) + \\mathrm{Cov}(Y, Z)$, and similarly in the second argument.\n:::\n\n::: {#thm-indep-zero-cov}\n## Independence Implies Zero Covariance\nIf $X$ and $Y$ are independent, then $\\mathrm{Cov}(X, Y) = 0$.  \nHowever, $\\mathrm{Cov}(X, Y) = 0$ does not necessarily imply independence.\n:::\n\n\n\n## Joint, Marginal, and Conditional Distributions\n\nLet $X$ and $Y$ be random variables.\n\n### Joint Distribution\n\n- Discrete: $p(x, y) = P(X = x, Y = y)$\n- Continuous: $f(x, y)$ such that $P((X, Y) \\in A) = \\iint_A f(x, y)\\, dx\\, dy$\n\n### Marginal Distribution\n\n- Discrete: $p_X(x) = \\sum_y p(x, y)$\n- Continuous: $f_X(x) = \\int f(x, y)\\, dy$\n\n### Conditional Distribution\n\n- Discrete: $P(Y = y \\mid X = x) = \\frac{P(X = x, Y = y)}{P(X = x)}$\n- Continuous: $f(y \\mid x) = \\frac{f(x, y)}{f_X(x)}$\n\n\n::: {#exm-joint1}\nLet $X, Y$ be discrete with joint PMF:\n\n| $X \\backslash Y$ | 0   | 1   |\n|------------------|-----|-----|\n| 0                | 0.1 | 0.3 |\n| 1                | 0.2 | 0.4 |\n\nCompute $P(X = 1)$, $P(Y = 0)$, and $P(Y = 1 \\mid X = 1)$.\n\n*Solution.*\n\n- $P(X = 1) = 0.2 + 0.4 = 0.6$\n- $P(Y = 0) = 0.1 + 0.2 = 0.3$\n- $P(Y = 1 \\mid X = 1) = \\frac{0.4}{0.6} = 0.\\overline{6}$\n\n:::\n\n\n\n### Covariance and Correlation\n\n::: {#def-cov}\nThe **covariance** between $X$ and $Y$ is $\\text{Cov}(X, Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\mathbb{E}[Y]$.\n:::\n\n::: {#def-cor}\nThe **correlation** between $X$ and $Y$ is $\\text{Corr}(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\text{Var}(Y)}}$.\n:::\n\n::: {#exm-cov1}\nLet $X, Y$ be defined as in @exm-joint1. Compute $\\text{Cov}(X, Y)$.\n\n*Solution.*\n\nFirst compute\n\n- $\\mathbb{E}[X] = 0 \\cdot (0.1 + 0.3) + 1 \\cdot (0.2 + 0.4) = 0.6$,\n- $\\mathbb{E}[Y] = 0 \\cdot (0.1 + 0.2) + 1 \\cdot (0.3 + 0.4) = 0.7$, and\n- $\\mathbb{E}[XY] = 0 \\cdot 0.1 + 0 \\cdot 0.3 + 1 \\cdot 0.2 + 1 \\cdot 0.4 = 0.6$.\n\nThen $\\text{Cov}(X, Y) = 0.6 - (0.6)(0.7) = 0.6 - 0.42 = 0.18$.\n\n:::\n\n\n## Additional Laws\n\n\n::: {#thm-lotus}\n\n## Law of the Unconscious Statistician\n\nIf $X$ is a random variable and $g$ is a function, then $\\mathbb{E}[g(X)] = \\sum_x g(x) p(x)$ for discrete random variables and $\\mathbb{E}[g(X)] = \\int g(x) f(x)\\, dx$ for continuous random variables.\n:::\n\n\n::: {#thm-total-exp}\n\n## Law of Total Expectation\n\nFor another variable $Y$, $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]]$.\n:::\n\n\n::: {#thm-total-var}\n\n## Law of Total Variance\n\n\n$$\n\\text{Var}(X) = \\mathbb{E}[\\text{Var}(X \\mid Y)] + \\text{Var}(\\mathbb{E}[X \\mid Y])\n$$\n:::\n\n\n\n\n\n::: {#thm-changevar}\n\n## Change of Variables\n\n\nSuppose $X$ is a continuous variable with PDF $f_X(x)$ and $Y = g(X)$ is a strictly monotonic transformation. Then the PDF of $Y$ is $f_Y(y) = f_X(g^{-1}(y)) \\cdot \\left| \\frac{d}{dy} g^{-1}(y) \\right|$.\n:::\n\n\n::: {#exm-change1}\nLet $X \\sim \\text{Exponential}(1)$ and $Y = \\log(X)$. Find the PDF of $Y$.\n\n*Solution.*\n\nNotice that $g^{-1}(y) = e^y$. Thus, $\\frac{d}{dy} g^{-1}(y) = e^y$.\n\n$f_X(x) = e^{-x}$.\n\nThen $f_Y(y) = f_X(e^y) \\cdot e^y = e^{-e^y} \\cdot e^y = e^y \\cdot e^{-e^y}$.\n\n:::\n\n\n## Normal Distribution and `*norm()` in R\n\nLet $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Then:\n\n::: {#def-normal}\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n$$\n:::\n\n- $\\mathbb{E}[X] = \\mu$\n- $\\text{Var}(X) = \\sigma^2$\n\n\n### R Functions for Normal Distribution\n\n| Function    | Description                        |\n|-------------|------------------------------------|\n| `dnorm(x)`  | PDF: $f(x)$                        |\n| `pnorm(x)`  | CDF: $P(X \\le x)$                  |\n| `qnorm(p)`  | Quantile: inverse CDF              |\n| `rnorm(n)`  | Simulate from $\\mathcal{N}(\\mu, \\sigma^2)$ |\n\n\n::: {#exm-rnorm1}\nSimulate 5 draws from $\\mathcal{N}(2, 1)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(5, mean = 2, sd = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.961536 1.351848 2.608346 2.216705 2.561633\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {#exm-rnorm2}\nCompute $P(X \\le 1.96)$ for $X \\sim \\mathcal{N}(0, 1)$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(1.96)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9750021\n```\n\n\n:::\n:::\n\n\n\n\nReturns approximately `0.975`.\n:::\n\n",
    "supporting": [
      "probability_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}