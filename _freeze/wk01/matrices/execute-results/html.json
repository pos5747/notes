{
  "hash": "6cffcecd94a69c0752b0ae574b738071",
  "result": {
    "engine": "knitr",
    "markdown": "# Matrices\n\n## Definitions\n\nA **scalar** is a single number.\n\nA **vector** is a 1-dimensional array of numbers.^[By default, we treat these as $1 \\times n$ *column* vectors (see definition of *matrix* below).]\n\nA **matrix** is a two-dimensional array of numbers.\n\n- $x$ is $n \\times 1$ (column vector).\n- $x'$ is $1 \\times n$ (row vector).\n- $A$ is $n \\times p$ (matrix).\n\nMatrices allow us to efficiently describe and perform complex calculations involving many, many numbers (e.g., data sets).\n\n::: {#exm-matrix1}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(1, 2, 3)           # 3 x 1 (column vector)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1 2 3\n```\n\n\n:::\n\n```{.r .cell-code}\nt(x)                      # 1 x 3 (row vector); t() finds transpose, see below\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n```\n\n\n:::\n\n```{.r .cell-code}\nA <- matrix(1:6, nrow = 2)\nA                         # 2 x 3 matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Transpose\n\nThe **transpose** of a matrix flips its rows and columns. If $A$ is $n \\times p$, then $A'$ is $p \\times n$.\n\nKey identity: $(AB)' = B'A'$\n\n::: {#exm-matrix2}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(1, 2, 3, 4), nrow = 2)\n\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n```\n\n\n:::\n\n```{.r .cell-code}\nt(A)  # transpose\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n```\n\n\n:::\n\n```{.r .cell-code}\nA2 <- matrix(c(1, 2, 3, 4), nrow = 2, byrow = TRUE)  # note the byrow arg\nA2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n```\n\n\n:::\n\n```{.r .cell-code}\nB <- matrix(c(5, 6, 7, 8), nrow = 2)\nAB  <- A %*% B            # matrix multiplication\nt(AB)                     # transpose of product\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   23   34\n[2,]   31   46\n```\n\n\n:::\n\n```{.r .cell-code}\nt(B) %*% t(A)             # product of transposes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   23   34\n[2,]   31   46\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Matrix Multiplication\n\n### Definition and Computation\n\nThe **matrix product** $C = AB$ is defined when the number of columns in $A$ equals the number of rows in $B$. If $A$ is an $n \\times p$ matrix and $B$ is a $p \\times q$ matrix, then the product $C = AB$ is an $n \\times q$ matrix.\n\nThe $(i, j)$-entry of $C$ is computed as the *dot product* of the $i$th row of $A$ and the $j$th column of $B$:\n\n$$\nC_{ij} = \\sum_{k=1}^{p} A_{ik} B_{kj}\n$$\n\nThat is, to compute the entry in the $i$th row and $j$th column of $C$, multiply corresponding elements from the $i$th row of $A$ and the $j$th column of $B$, and then sum the results.\n\n::: {#exm-matrix-mult-check}\n\nSuppose \n$$\nA = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n5 & 6 & 7 & 8 \\\\\n9 & 0 & 1 & 2\n\\end{bmatrix}, \\quad\nB = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n$$.\n\nThe product $C = AB$ is a $3 \\times 2$ matrix. Each entry is computed as $C_{ij} = \\sum_{k=1}^4 A_{ik} B_{kj}$.\n\n$$\nC = AB = \\begin{bmatrix}\n1 \\cdot 1 + 2 \\cdot 0 + 3 \\cdot 1 + 4 \\cdot 0 & 1 \\cdot 0 + 2 \\cdot 1 + 3 \\cdot 0 + 4 \\cdot 1 \\\\\n5 \\cdot 1 + 6 \\cdot 0 + 7 \\cdot 1 + 8 \\cdot 0 & 5 \\cdot 0 + 6 \\cdot 1 + 7 \\cdot 0 + 8 \\cdot 1 \\\\\n9 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 1 + 2 \\cdot 0 & 9 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 2 \\cdot 1\n\\end{bmatrix}\n$$\n\nSimplifying, we have \n\n$$\n= \\begin{bmatrix}\n1 + 0 + 3 + 0 & 0 + 2 + 0 + 4 \\\\\n5 + 0 + 7 + 0 & 0 + 6 + 0 + 8 \\\\\n9 + 0 + 1 + 0 & 0 + 0 + 0 + 2\n\\end{bmatrix}\n= \\begin{bmatrix}\n4 & 6 \\\\\n12 & 14 \\\\\n10 & 2\n\\end{bmatrix}\n$$.\n\nWe can confirm our answer with R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(\n  1, 2, 3, 4,\n  5, 6, 7, 8,\n  9, 0, 1, 2\n), nrow = 3, byrow = TRUE)\n\nB <- matrix(c(\n  1, 0,\n  0, 1,\n  1, 0,\n  0, 1\n), nrow = 4, byrow = TRUE)\n\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    4    6\n[2,]   12   14\n[3,]   10    2\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {#exm-matrix9}\nLet $X$ be a $3 \\times 2$ matrix and $\\beta$ a $2 \\times 1$ vector. Then $X\\beta$ is a $3 \\times 1$ vector. $X'X$ is $2 \\times 2$ and symmetric.\n\nWe have a couple of famililar matrix multiplications from linear regression. $X\\beta$ is especially important to us!.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(1, 1, 1, 2, 3, 4), nrow = 3)\nbeta <- c(0.5, 1)\nX %*% beta                          # linear prediction\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]  2.5\n[2,]  3.5\n[3,]  4.5\n```\n\n\n:::\n\n```{.r .cell-code}\nt(X) %*% X                          # 2 x 2 matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    9\n[2,]    9   29\n```\n\n\n:::\n:::\n\n\n\n:::\n\n### Rules\n\n- **Associative**: $(AB)C = A(BC)$\n- **Distributive**: $A(B + C) = AB + AC$\n- **Not commutative**: $AB \\neq BA$ in general\n\n::: {#exm-matrix5}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(1, 2, 3, 4), 2)         # 2 x 2\nB <- matrix(c(5, 6, 7, 8), 2)         # 2 x 2\nC <- matrix(c(100, 200, 300, 400), 2) # 2 x 2\n\n# same\n(A %*% B) %*% C \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,]  8500 19300\n[2,] 12600 28600\n```\n\n\n:::\n\n```{.r .cell-code}\nA %*% (B %*% C) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]  [,2]\n[1,]  8500 19300\n[2,] 12600 28600\n```\n\n\n:::\n\n```{.r .cell-code}\n# same\nA %*% (B + C) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  723 1531\n[2,] 1034 2246\n```\n\n\n:::\n\n```{.r .cell-code}\n(A %*% B) + (A %*% C) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  723 1531\n[2,] 1034 2246\n```\n\n\n:::\n\n```{.r .cell-code}\n# different\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   23   31\n[2,]   34   46\n```\n\n\n:::\n\n```{.r .cell-code}\nB %*% A \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   19   43\n[2,]   22   50\n```\n\n\n:::\n:::\n\n\n\n:::\n\n## Special Matrices\n\n### Identity Matrix\n\nAn **identity matrix** is a square matrix with 1s on the diagonal and 0s elsewhere. Denoted $I$.\n\n- $AI = A$\n- $IA = A$\n- $I_n$ is $n \\times n$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nI <- diag(3)  # shortcut to make 3x3 identity matrix\nI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n```\n\n\n:::\n\n```{.r .cell-code}\nA <- matrix(1:9, nrow = 3)\nA %*% I                           # same as A\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n```\n\n\n:::\n:::\n\n\n\n\n### Diagonal Matrix\n\nA **diagonal matrix** has nonzero entries only on the diagonal. These matrices are often used for variances or weights.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- c(2, 4, 6)\nD <- diag(d)  # shortcut to make diagonal matrix\n```\n:::\n\n\n\n\n### Symmetric Matrix\n\nA matrix $A$ is **symmetric** if $A' = A$.\n\n- $X'X$ is always symmetric.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- matrix(c(1, 2, 3, 4, 5, 6), nrow = 3)\nt(X) %*% X                        # symmetric\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]   14   32\n[2,]   32   77\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {#exm-matrix1}\n\nConstruct the following matrices:\n\n- Identity matrix $I_2$\n- Diagonal matrix with entries $1, 2, 3$\n- Symmetric matrix $S = \\begin{bmatrix}2 & 1 \\\\ 1 & 3\\end{bmatrix}$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nI2 <- diag(2)\ndiag_mat <- diag(1:3)\nS <- matrix(c(2, 1, 1, 3), 2)\n```\n:::\n\n\n\n\n:::\n\n::: {#exm-matrix2}\n\nLet $\\Sigma = \\begin{bmatrix}4 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 9\\end{bmatrix}$. This is a diagonal covariance matrix.\n\nExtract the standard deviations.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSigma <- diag(c(4, 1, 9))\nsqrt(diag(Sigma))                # std devs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2 1 3\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n\n## Matrix Inverses and Rank\n\n### Inverse\n\nThe **inverse** of a square matrix $A$ is a matrix $A^{-1}$ such that $AA^{-1} = A^{-1}A = I$.\n\n- Not all square matrices have an inverse.\n- Inverse exists only if matrix is **full rank**.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(2, 1, 1, 1), 2)\nsolve(A)                         # A-inverse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1   -1\n[2,]   -1    2\n```\n\n\n:::\n\n```{.r .cell-code}\nA %*% solve(A)                   # should be identity\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n\n\n:::\n:::\n\n\n\n\n## Full Rank\n\nA matrix has **full rank** if its columns are linearly independent.\n\n- $n \\times p$ matrix has full column rank if rank = $p$.\n- A square matrix is invertible if and only if full rank.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- matrix(c(1, 2, 2, 4), 2)\nqr(B)$rank # use QR decomposition to find rank\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\n\n## Why Rank Matters\n\n- If $X$ is not full rank, $X'X$ is not invertible.\n- In regression, full rank $X$ ensures a unique $\\hat\\beta$.\n- If rank is less than the number of columns, this means on variable is perfectly collinear with another.\n\n## Examples\n\n::: {#exm-matrix3}\n\nLet $A = \\begin{bmatrix}4 & 7 \\\\ 2 & 6\\end{bmatrix}$. Compute $A^{-1}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(4, 2, 7, 6), 2)\nsolve(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.6 -0.7\n[2,] -0.2  0.4\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {#exm-matrix4}\n\nLet $C = \\begin{bmatrix}1 & 2 \\\\ 2 & 4\\end{bmatrix}$. Is $C$ invertible?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC <- matrix(c(1, 2, 2, 4), 2)\nqr(C)$rank                      # rank < n\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\nWe can see that the second column is simple 2 times the first column, so the matrix is not full rank and thus not invertible.\n:::\n\n## Matrix Calculus (Introductory)\n\nMatrix calculus helps compute gradients and Hessians of functions involving vectors and matrices.\n\n- Used in optimization, MLE, and Bayesian computation\n- Especially common in linear models\n\n### Gradients\n\nIf $f(x)$ is a **scalar-valued function** of a **vector** $x$, then the **gradient** of $f$ with respect to $x$, denoted $\\frac{\\partial f}{\\partial x}$ or $\\nabla_x f$, is a vector containing the partial derivatives of $f$ with respect to each component of $x$, so that \n\n$$\n\\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}\n$$.\n\nThis vector tells us how the function $f(x)$ changes in each coordinate direction, and it points in the direction of steepest ascent.\n\n::: {#exm-grad1}\n\nLet $f(x) = x_1^2 + 3x_2$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$.\n\nThen the gradient is\n\n$$\n\\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x_1}(x_1^2 + 3x_2) \\\\\n\\frac{\\partial}{\\partial x_2}(x_1^2 + 3x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2x_1 \\\\\n3\n\\end{bmatrix}.\n$$\n\n:::\n\n\n::: {#exm-grad3}\n\nLet $f(x) = x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4}$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}$ and $x_3 > 0$.\n\nThen the gradient is\n\n$$\n\\frac{\\partial f}{\\partial x} =\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial x_1}(x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4}) \\\\\n\\frac{\\partial}{\\partial x_2}(x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4}) \\\\\n\\frac{\\partial}{\\partial x_3}(x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4}) \\\\\n\\frac{\\partial}{\\partial x_4}(x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2x_1 + x_2 \\\\\nx_1 \\\\\n\\frac{1}{x_3} \\\\\ne^{x_4}\n\\end{bmatrix}\n$$.\n\n:::\n\n\n\n### Hessians\n\nIf $f(x)$ is a **scalar-valued function** of a **vector** $x$, then the **Hessian** of $f$ with respect to $x$, denoted $\\frac{\\partial^2 f}{\\partial x \\partial x'}$, is an $n \\times n$ matrix of second-order partial derivatives:\n\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix}.\n$$\n\nThe Hessian describes the curvature of $f(x)$ and is used in second-order optimization methods (like Newtonâ€™s method) and in statistical approximations (e.g., Laplace approximation).\n\n::: {#exm-hess1}\n\nLet $f(x) = x_1^2 + 3x_2$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$.\n\nThen the Hessian is\n\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n\\frac{\\partial^2}{\\partial x_1^2}(x_1^2 + 3x_2) & \\frac{\\partial^2}{\\partial x_1 \\partial x_2}(x_1^2 + 3x_2) \\\\\n\\frac{\\partial^2}{\\partial x_2 \\partial x_1}(x_1^2 + 3x_2) & \\frac{\\partial^2}{\\partial x_2^2}(x_1^2 + 3x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 & 0 \\\\\n0 & 0\n\\end{bmatrix}.\n$$\n\n:::\n\n::: {#exm-hess2}\n\nLet $f(x) = x_1^2 + x_1 x_2 + \\log(x_3) + e^{x_4}$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix}$ and $x_3 > 0$.\n\nThen the Hessian is\n\n$$\n\\frac{\\partial^2 f}{\\partial x \\partial x'} =\n\\begin{bmatrix}\n2 & 1 & 0 & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 0 & -\\frac{1}{x_3^2} & 0 \\\\\n0 & 0 & 0 & e^{x_4}\n\\end{bmatrix}.\n$$\n\n:::\n\n\n## Where These Ideas Arise in Modeling\n\n- **Vectors and matrices**: Represent the outcome variable $y$, a matrix of explanatory variables $X$ (usually including a column of ones in the first column), parameters $\\beta$, or residuals $\\epsilon$.\n- **Matrix multiplication**:\n  - Linear predictor: $X\\beta$\n  - OLS and ML estimate of normal-linear model: $\\hat{\\beta} = (X'X)^{-1}X'y$\n- **Transpose**:\n  - Quadratic loss: $(y - X\\beta)'(y - X\\beta)$ \n  - Covariance formulas: $X'\\Sigma^{-1}X$\n- **Special matrices**:\n  - Identity matrix $I$: appears in prior variances, regularization, ridge regression\n  - Diagonal matrices: independent variances, prior precision matrices\n  - Symmetric matrices: $X'X$, covariance matrices, Hessians\n- **Inverses and rank**:\n  - Invert $X'X$ to find MLE in linear regression\n  - Non-full-rank $X$ causes identifiability issues\n- **Matrix calculus**:\n  - Score function: gradient of log-likelihood and log-posteriors\n  - Hessian matrix: used for Newton-Raphson and Fisher information \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}