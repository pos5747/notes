{
  "hash": "5e9477e9b2ebf13013730a5662cd7ee2",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n# Predictive Distribution\n\nIn Bayesian statistics, a popular tool for model evaluation is the *posterior predictive distribution.* But we might use an analogous approach for models fit with maximum likelihood. \n\nThe predictive distribution is just the distribution given the ML estimates. Using our notation above, the predictive distribution is $f(y; \\hat{\\theta})$. \n\nWe're going to use this predictive distribution to understand and evaluate our model.\n\nIn my view, the predictive distribution is the best way to (1) understand, (2) evaluate, and then (3) improve models.\n\nYou can use the predictive distribution as follows:\n\n1. Fit your model with maximum likelihood.\n1. Simulate a new outcome variable using the estimated model parameters (i.e., $f(y; \\hat{\\theta})$). Perhaps simulate a several for comparison.\n1. Compare the simulated outcome variable(s) to the observed outcome variables. You can use histograms and or plots of the ECDF.\n\nThere are two reasons why an observed data set might not look like the distribution assumed by the model. \n\n1. First, the model might not capture the observed data well. For example, you might use a normal distribution to model a variable that isn't similar to a normal distribution. \n2. Second, the deviations between the observed data and model might be consistent with noise. For example, if you take 25 draws from a normal distribution and draw a histogram of those values, the histogram will not look exactly normal. It will be lumpy, bumpy, and/or skewed, simply due to noise.\n\nWhen we compare the observed data with the predictive distribution, we are keeping both of these deviations in mind. We simulate several fake data sets to understand how the outcome might change from sample to sample if the assumed distrubtion were correct. The we compare the patterns across the simulated fake data sets to the observed data set: are the two easily distiguishable?\n\n## Example: Poisson Distribution\n\nEarlier, we fit a Poisson distribution to data from @Holland2015.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute ml estimates of poisson parameter\nml_est <- mean(y)\nprint(ml_est, digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23.2\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulate from predictive distribution\nn <- length(y)\ny_pred <- rpois(n, lambda = ml_est)\nprint(y_pred[1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 22 22 27 16 21 24 30 25 15 25\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(y[1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  7  0 20  1  1 50  3  0 36 32\n```\n\n\n:::\n:::\n\n\n\n\nSimply printing a few results, we can immediately see a problem with data, when compared with the raw data\n\nTo see it even more clearly, we can create a histogram of the observed and simulated data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(patchwork)\n\n# make plot\ngg1 <- ggplot() + geom_histogram(aes(x = y)) + xlim(min(y), max(y))\ngg2 <- ggplot() + geom_histogram(aes(x = y_pred)) + xlim(min(y), max(y))\ngg1 / gg2 +  plot_layout(axes = \"collect\")  # stitch these together w/ patchwork\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can use plots of the ECDFs rather than histograms.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make plot\ngg1 <- ggplot() + stat_ecdf(aes(x = y)) + xlim(min(y), max(y))\ngg2 <- ggplot() + stat_ecdf(aes(x = y_pred)) + xlim(min(y), max(y))\ngg1 / gg2 +  plot_layout(axes = \"collect\")  # stitch these together w/ patchwork\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\nFor a more accurate and complete comparison, let's simulate five fake data sets. By using five fake data sets, we'll see clearly how much of the variation might be due to noise (across the five fake data sets) and how the observed data set differs from the model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create observed data set\nobserved_data <- tibble(operations = y, type = \"observed\") %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 36\nColumns: 2\n$ operations <dbl> 7, 0, 20, 1, 1, 50, 3, 0, 36, 32, 22, 42, 50, 20, 44, 3, 0,…\n$ type       <chr> \"observed\", \"observed\", \"observed\", \"observed\", \"observed\",…\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulate five fake data sets\nsim_list <- list()\nfor (i in 1:5) {\n  y_pred <- rpois(n, lambda = ml_est)\n  sim_list[[i]] <- tibble(operations = y_pred, \n                          type = paste0(\"simulated #\", i))\n}\n\n# combine the fake and observed data sets\ngg_data <- bind_rows(sim_list) %>%\n  bind_rows(observed_data) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 216\nColumns: 2\n$ operations <dbl> 17, 23, 27, 18, 30, 23, 21, 26, 22, 22, 25, 24, 23, 18, 19,…\n$ type       <chr> \"simulated #1\", \"simulated #1\", \"simulated #1\", \"simulated …\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot the observed and fake data sets\nggplot(gg_data, aes(x = operations)) + \n  geom_histogram() + \n  facet_wrap(vars(type))\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# make plots of ecdf\nggplot(gg_data, aes(x = operations)) + \n  stat_ecdf() + \n  facet_wrap(vars(type))\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n## Example: Beta Distribution\n\nNow let's return to our beta model of batting averages from earlier.\n\n\n\n\n\n\n\n\n\nNow let's simulate five fake data sets from the predictive distribution and compare that to the observed data. In this case, the beta model fits the data pretty well, so let's add a pdf of the fitted model to the plots as well for even more precise comparisons of the real and fitted data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a dataframe of observed data\nobserved_data <- bstats %>%\n  mutate(type = \"observed\") %>%\n  select(-player_id) %>%  # variable not needed, makes things neater later\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 457\nColumns: 2\n$ batting_average <dbl> 0.354, 0.337, 0.331, 0.330, 0.327, 0.319, 0.316, 0.312…\n$ type            <chr> \"observed\", \"observed\", \"observed\", \"observed\", \"obser…\n```\n\n\n:::\n\n```{.r .cell-code}\n# simulate fake data sets\nn <- nrow(bstats)\nsim_list <- list()\nfor (i in 1:5) {\n  y_pred <- rbeta(n, shape1 = theta_hat$est[1], shape2 = theta_hat$est[2])\n  sim_list[[i]] <- tibble(batting_average = y_pred, \n                          type = paste0(\"simulated #\", i))\n}\n\n# combine the fake and observed data sets\ngg_data <- bind_rows(sim_list) %>%\n  bind_rows(observed_data) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,742\nColumns: 2\n$ batting_average <dbl> 0.2319739, 0.2919389, 0.2494256, 0.2562710, 0.2741193,…\n$ type            <chr> \"simulated #1\", \"simulated #1\", \"simulated #1\", \"simul…\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot histograms of real and fake data\nggplot(gg_data, aes(x = batting_average)) + \n  geom_histogram(aes(y = after_stat(density))) + \n  facet_wrap(vars(type)) + \n  stat_function(fun = dbeta, args = list(shape1 = theta_hat$est[1], \n                                         shape2 = theta_hat$est[2]), \n                color = \"red\")\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nOn the whole, we see hear a close correspondence between the observed and simulated data. That suggests that our model is a good description of the data.\n\nIf we use the ECDFs rather than the histograms, we see a similarly well-fitting model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot histograms of real and fake data\nggplot(gg_data, aes(x = batting_average)) + \n  stat_ecdf() + \n  facet_wrap(vars(type)) + \n  stat_function(fun = pbeta, args = list(shape1 = theta_hat$est[1], \n                                         shape2 = theta_hat$est[2]), \n                color = \"red\")\n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\nIt's really hard to tell this apart!\n\nTo make the comparison even more refined, let's put all the curves in the same panel.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# separate the labels of the simulated data into two parts\ngg_data2 <- gg_data |>\n  separate(type, into = c(\"type\", \"version\")) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 2,742\nColumns: 3\n$ batting_average <dbl> 0.2319739, 0.2919389, 0.2494256, 0.2562710, 0.2741193,…\n$ type            <chr> \"simulated\", \"simulated\", \"simulated\", \"simulated\", \"s…\n$ version         <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\",…\n```\n\n\n:::\n\n```{.r .cell-code}\n# plot histograms of real and fake data\nggplot(gg_data2, aes(x = batting_average, color = type, group = version)) + \n  stat_ecdf() \n```\n\n::: {.cell-output-display}\n![](03-predictive-distribution_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nIf we look *really* hard, we can start to see some small differences between these the observed data and the model, but the model is a really good one. With only two parameters, the beta distribution does an excellent job of recreating the distribution of the observed data.",
    "supporting": [
      "03-predictive-distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}