{
  "hash": "f3fd9ca5243dbe60a4a9cc1efbf76149",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n# The Invariance Property\n\nWhen discussing the properties of an ML estimate $\\hat{\\theta}$: we've mentioned two properties. First, $\\hat{\\theta}$ is a consistent estimator. Second, @thm-ml-invariance shows that we can transform $\\hat{\\theta}$ to obtain an ML estimate of the transformation (i.e., $\\hat{\\tau} = \\tau{(\\hat{\\theta})}$). Let's explore this invariance property a bit more.\n\nModel parameters sometimes have a nice interpretation. For example, the parameter $\\pi$ in the Bernoulli model has a nice interpretation--it's a probability or the expected fraction of 1s in the long-run. However, the model parameters might not always have nice interpretation. For example, the shape parameters $\\alpha$ and $\\beta$ of the beta distribution do not have a nice interpretation. Fortunately, it's easy to transform the ML estimates of the model parameters into ML estimates of a quantity of interest.\n\n::: callout-important\n## Invariance Property (@thm-ml-invariance)\n\nSuppose we obtain an ML estimate $\\hat{\\theta}$ of a parameter $\\theta$. But we also (or instead) want to estimate a transformation $\\tau(\\theta)$. The we can estimate $\\tau(\\theta)$ by applying the transformation $\\tau$ to the ML estimate $\\hat{\\theta}$, so that $\\widehat{\\tau(\\theta)} = \\hat{\\tau} = \\tau(\\hat{\\theta})$.\n:::\n\n## Example: Bernoulli Odds\n\nSuppose that we want an ML estimator of the *odds* of getting a top for the toothpaste cap problem. We already used ML to estimate the *probability* $\\pi$ of getting a top and came up with $\\frac{8}{150} \\approx 0.053$. We can directly transform a probability into odds using $\\text{odds} = \\frac{\\pi}{1 - \\pi}$. This has a nice interpretation: odds = 2 means that a top is twice as likely as not; odds = 0.5 means that a top is half as likely as not.\n\nIn our case, we can plug our ML estimate of $\\pi$ into the transformation to obtain the ML estimate of the odds. $$\n\\begin{aligned}\n\\widehat{\\text{odds}} &= \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}} \\\\\n& = \\frac{\\frac{8}{150}}{1 - \\frac{8}{150}} \\\\\n& = \\frac{\\frac{8}{150}}{\\frac{150}{150} - \\frac{8}{150}} \\\\\n& = \\frac{\\frac{8}{150}}{\\frac{142}{150}} \\\\\n& = \\frac{8}{142} \\\\\n& \\approx 0.056\n\\end{aligned}\n$$ This means that tops are about 0.06 times as likelihood as not-tops. Inverted, you're about $\\frac{142}{8} \\approx 18$ times more likely to not get a top than get a top.\n\n## Example: Poisson SD\n\nIn this example, we use real data from @Holland2015, who is interested in the number of enforcement operations across districts in three cities. See `?crdata::holland2015` for the details. We can model the number of enforcement operations in each city as a Poisson distribution and estimate $\\lambda$ in each of the three cities. (We previously found that the sample mean is the ML estimator for the mean parameter $\\lambda$ for the Poisson distribution.)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install package (once per computer)\n# remotes::install_github(\"carlislerainey/crdata\")\n\n# load holland's data (once per session)\nholland2015 <- crdata::holland2015 |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 89\nColumns: 7\n$ city       <chr> \"santiago\", \"santiago\", \"santiago\", \"santiago\", \"santiago\",…\n$ district   <chr> \"Cerrillos\", \"Cerro Navia\", \"Conchali\", \"El Bosque\", \"Estac…\n$ operations <dbl> 0, 0, 0, 0, 12, 0, 0, 0, 1, 1, 0, 10, 1, 5, 0, 0, 0, 4, 4, …\n$ lower      <dbl> 52.2, 69.8, 54.8, 58.4, 43.6, 58.3, 41.0, 38.3, 36.7, 60.1,…\n$ vendors    <dbl> 0.50, 0.60, 5.00, 1.20, 1.00, 0.30, 0.05, 1.25, 2.21, 0.70,…\n$ budget     <dbl> 337.24, 188.87, 210.71, 153.76, 264.43, 430.42, 312.75, 255…\n$ population <dbl> 6.6160, 13.3943, 10.7246, 16.8302, 11.1702, 8.5761, 5.1277,…\n```\n\n\n:::\n\n```{.r .cell-code}\n# compute ml estimate of poisson mean parameter lambda\nholland2015 |>\n  group_by(city) |>\n  summarize(lambda_hat = mean(operations))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 2\n  city     lambda_hat\n  <chr>         <dbl>\n1 bogota         8.89\n2 lima          23.2 \n3 santiago       2.71\n```\n\n\n:::\n:::\n\n\n\n\nThe parameter $\\lambda$ is a nice, interpretable quantity---it's a mean! But we might want also want the SD. For the Poisson distribution, the variance equals the mean, so $\\text{Var}(y) = \\text{E}(y) = \\lambda$. Therefore, the SD is $\\sqrt{\\lambda}$. We don't need to do the hard work of finding a new ML estimator for the SD, we can just use $\\widehat{\\text{SD}} = \\sqrt{\\hat{\\lambda}}$. This is the ML estimate of the SD of the data, and it carries all the properties of ML estimators. \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute compute ml estimate of lambda (mean) and sd\nholland2015 |>\n  group_by(city) |>\n  summarize(lambda_hat = mean(operations), \n            sd_hat = sqrt(lambda_hat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  city     lambda_hat sd_hat\n  <chr>         <dbl>  <dbl>\n1 bogota         8.89   2.98\n2 lima          23.2    4.82\n3 santiago       2.71   1.64\n```\n\n\n:::\n:::\n\n\n\n\n\nWe're using the invariance property to move from the mean to the SD by a simple transformation. Note that we can do this because the Poisson distribution assumes that the variance and SD are a function of the mean. We couldn't similarly infer the variance from the mean in a normal model, for example. But we're also *assuming* a Poisson distribution. A later exercise asks you if these estimates of the SD are close to the actual SD of the data.\n\n## Example: Beta Mean and Variance\n\nNow let's see an example of the beta distribution $Y \\sim \\text{beta}(\\alpha, \\beta)$. The beta distribution does not have parameters that are easily interpretable in terms of mean and variance. Instead, it has two \"shape\" parameters $\\alpha$ and $\\beta$ that are in tension---one pulls the distribution to the left and the other pulls the distribution to the right.\n\nFor an example, let's return to the batting average data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(Lahman)  # data from Lahman's baseball database\n\n# create data frame with batting average\nbstats <- battingStats() |> \n  filter(yearID == 2023, AB > 100) |>  # data from 2023\n  filter(AB >= 100) |>  # players with at least 100 at-bats\n  select(player_id = playerID, batting_average = BA) |>\n  arrange(-batting_average) |>\n  na.omit() |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 457\nColumns: 2\n$ player_id       <chr> \"arraelu01\", \"acunaro01\", \"freemfr01\", \"diazya01\", \"se…\n$ batting_average <dbl> 0.354, 0.337, 0.331, 0.330, 0.327, 0.319, 0.316, 0.312…\n```\n\n\n:::\n\n```{.r .cell-code}\n# create function to estimate beta parameters\nll_fn <- function(theta, y) { \n  alpha <- theta[1] \n  beta <- theta[2] \n  ll <- sum(dbeta(y, shape1 = alpha, shape2 = beta, log = TRUE))\n  return(ll)\n}\nest_beta <- function(y) {\n  est <- optim(par = c(2, 2), fn = ll_fn, y = y,\n               control = list(fnscale = -1),\n               method = \"BFGS\") # for >1d problems\n  if (est$convergence != 0) print(\"Model did not converge!\")\n  res <- list(est = est$par)\n  return(res)\n}\n\n# estimate beta model\ntheta_hat <- est_beta(bstats$batting_average)\ntheta_hat$est\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  37.07655 114.92550\n```\n\n\n:::\n:::\n\n\n\n\nFor the beta distribution, the mean is given by $\\frac{\\alpha}{\\alpha + \\beta}$ and the variance is given by $\\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}$. (See @tbl-beta.)\n\nWe can use the invariance property to obtain ML estimates of the mean, variance, and SD using our ML estimates of $\\alpha$ and $\\beta$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- theta_hat$est[1]\nb <- theta_hat$est[2]\n\na/(a + b)  # mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2439214\n```\n\n\n:::\n\n```{.r .cell-code}\n(a * b)/((a + b)^2 * (a + b + 1))  # var\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001205368\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt((a * b)/((a + b)^2 * (a + b + 1)))  # sd\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03471841\n```\n\n\n:::\n:::\n\n\n\n\nIt's worth noting that these correspond closely, *but not exactly* to the observed mean, variance, and SD.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(bstats$batting_average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2439672\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(bstats$batting_average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001155203\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(bstats$batting_average)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03398828\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}