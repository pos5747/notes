# \{brms\}

```{r}
#| include: false
# load packages
library(tidyverse)
```

In most cases, we do not have to write our own Stan code, because the {brms} package can do that for us. The `brm()` function in the {brms} takes `glm()`-like arguments, and writes accurate and fast Stan code. If `brm()` works for your application, you should much prefer it to writing your on Stan models. It will (probably) be faster and is less like to have errors.

```{r}
# load packages
library(brms)
```

{brms} offers very flexible options for specifying your models and works with {marginaleffects}. We can use it for almost any application, though for simple `lm()` or `glm()` fits, it isn't needed.

See `?brms::brm` for the details. There are numerous tutorials available for specific models.

## The `turnout` example

Here's the syntax for our ususal logistic regression model.

```{r}
#| results: hide

# load only the turnout data frame and hard-code rescaled variables
turnout <- ZeligData::turnout  |>
  mutate(across(age:income, arm::rescale, .names = "rs_{.col}"))

# build model frame and design matrices
f  <- vote ~ rs_age + rs_educate + rs_income + race

# fit model with glm()
fit_glm <- glm(f, data = turnout, family = binomial)

# fit model with brm()
fit_brm_rstan <- brm(f, data = turnout, family = bernoulli, 
           chains = 10, cores = 10)

# fit model with brm() via cmdstan!
fit_brm_cmdstanr <- brm(f, data = turnout, family = bernoulli, 
           chains = 10, cores = 10, 
           backend = "cmdstanr")
```
`glm()` and `brm()` obtain the same estimates.^[The two are not mathematically equal, but they are very close in practice. Outside of cases where maximum likelihood fails, it is rare that a Bayesian estimate or interval differs from the ML estimate or interval.]

```{r}
#| code-fold: true
library(modelsummary)

normalize_terms <- function(x) {
  x <- sub("^b_", "", x)  # brms to glm names
  x <- ifelse(x == "(Intercept)", "Intercept", x)  # unify intercept
  x
}

modelsummary(
  list("glm()" = fit_glm, "brm(); rstan" = fit_brm_rstan, "brm(); cmdstanr" = fit_brm_cmdstanr),
  coef_rename = normalize_terms, 
  gof_map = NULL
)
```

And {brms} plays very nicely with {marginaleffects}.

```{r}
library(marginaleffects)

# compute first different
comparisons(fit_brm_cmdstanr, variables = list(rs_age = c(-0.5, 0.5)), 
            newdata = datagrid(grid_type = "mean_or_mode"))


# compute and plot expected values (i.e., predicted probabilities)
p <- predictions(fit_brm_cmdstanr, variables = list(rs_age = seq(-1, 1, by = 0.1)), 
            newdata = datagrid(grid_type = "mean_or_mode"))
ggplot(p, aes(x = rs_age, y = estimate, ymin = conf.low, ymax = conf.high)) + 
  geom_ribbon(fill = "grey") + 
  geom_line()
```

## The flexibility of MCMC and `brm()`

### Modeling "smooth" relationships

To illustrate the power of MCMC and `brm()`, we can imagine fitting a smooth curve for age. The details are not important for now, but this isn't an easy task in our usual framework. But it's straightfoward with Bayesian MCMC via `brm()`.

```{r}
#| results: hide
# smooth relationship between age and voting--notice the s()
f  <- vote ~ s(rs_age) + rs_educate + rs_income + race
smooth_fit <- brm(f, data = turnout, family = bernoulli, 
           chains = 10, cores = 10, 
           backend = "cmdstanr")
```

```{r}
# compute and plot expected values (i.e., predicted probabilities)
p <- predictions(smooth_fit, variables = list(rs_age = seq(-1, 1, by = 0.1)), 
            newdata = datagrid(grid_type = "mean_or_mode"))
ggplot(p, aes(x = rs_age, y = estimate, ymin = conf.low, ymax = conf.high)) + 
  geom_ribbon(fill = "grey") + 
  geom_line()
```

### Awkwardly scaled variables

Also, `brm()` has no trouble with the variables on their original scales. The original scales poses a insurmountable hurdle for our `metrop()` sampler.

```{r}
#| results: hide
# smooth relationship between age and voting--notice the s()
f  <- vote ~ s(age) + educate + income + race
smooth_fit <- brm(f, data = turnout, family = bernoulli, 
           chains = 10, cores = 10, 
           backend = "cmdstanr")
```

```{r}
# compute and plot expected values (i.e., predicted probabilities)
p <- predictions(smooth_fit, variables = list(age = seq(18, 90, by = 1)), 
            newdata = datagrid(grid_type = "mean_or_mode"))
ggplot(p, aes(x = age, y = estimate, ymin = conf.low, ymax = conf.high)) + 
  geom_ribbon(fill = "grey") + 
  geom_line()
```